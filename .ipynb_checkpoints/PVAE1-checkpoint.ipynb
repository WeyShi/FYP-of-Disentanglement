{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162079 20259 20261\n",
      "torch.Size([128, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from disentanglement_lib.data.ground_truth import dsprites\n",
    "import os\n",
    "import torch\n",
    "from tensorflow import gfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "dsprites.DSprites\n",
    "IMAGE_PATH = 'img_align_celeba/'\n",
    "image_size = 64\n",
    "# SAMPLE_PATH = '../'\n",
    "DSPRITES_PATH = \"/home/ISO/Pruned_VAE/data/dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"\n",
    "with gfile.Open(DSPRITES_PATH, \"rb\") as data_file:\n",
    "    # Data was saved originally using python2, so we need to set the encoding.\n",
    "    data = np.load(data_file, encoding=\"latin1\", allow_pickle=True)\n",
    "# if not os.path.exists(SAMPLE_PATH):\n",
    "#     os.makedirs(SAMPLE_PATH)\n",
    "    \n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    #transforms.Scale(image_size),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop((image_size,image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "data_loader = ImageFolder(IMAGE_PATH, transform)\n",
    "\n",
    "\n",
    "#data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "valid_loader, train_loader, test_loader = get_celeba_dataloader(data_loader, \n",
    "                                                                batch_size=128)\n",
    "test_batch = iter(test_loader)\n",
    "test_batch = next(test_batch)\n",
    "new_labels =torch.tensor(test_batch[1])\n",
    "print(torch.tensor(test_batch[0]).shape)\n",
    "#latent_dist = model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "limit_a, limit_b, epsilon = -.5, 1.5, 1e-6\n",
    "eps = np.linspace(0.1,0.9,20)\n",
    "def sigmoid(x):\n",
    "    y = 1./(1.+np.exp(-x))\n",
    "    return y\n",
    "def quantile_concrete(x,temperature,qz_loga):\n",
    "        \n",
    "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
    "        y = sigmoid((np.log(x) - np.log(1 - x) + qz_loga) / temperature)\n",
    "        return y * (limit_b - limit_a) + limit_a\n",
    "\n",
    "z = quantile_concrete(eps,1/20,2)\n",
    "z[z>=1]=1\n",
    "z[z<=0]=0\n",
    "print(z)\n",
    "plt.plot(eps,z)\n",
    "droprate_init = 0.2\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.zeros(4), requires_grad=False).data.normal_(math.log(1 - droprate_init) - math.log(droprate_init), 1e-2)\n",
    "z=(x+y).detach()\n",
    "print(y)\n",
    "#z.is_leaf \n",
    "a=torch.ones((64,32))\n",
    "b=torch.ones((1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737280, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(data[\"imgs\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "\n",
    "#####train\n",
    "train_x = torch.tensor(data[\"imgs\"][:649600,:,:]) # a list of numpy arrays\n",
    "train_y = torch.zeros((data[\"imgs\"][:649600,:,:].shape[0],1)) # another list of numpy arrays (targets)\n",
    "train_set = utils.TensorDataset(train_x,train_y) # create your datset\n",
    "train_loader = utils.DataLoader(train_set, batch_size=64) # create your dataloader\n",
    "\n",
    "#####test\n",
    "test_x = torch.tensor(data[\"imgs\"][649600:704000,:,:]) # a list of numpy arrays\n",
    "test_y = torch.zeros((data[\"imgs\"][649600:704000,:,:].shape[0],1)) # another list of numpy arrays (targets)\n",
    "test_set = utils.TensorDataset(test_x,test_y) # create your datset\n",
    "test_loader = utils.DataLoader(test_set, batch_size=64) # create your dataloader\n",
    "\n",
    "#####valid\n",
    "valid_x = torch.tensor(data[\"imgs\"][700000:,:,:]) # a list of numpy arrays\n",
    "valid_y = torch.zeros((data[\"imgs\"][700000:,:,:].shape[0],1)) # another list of numpy arrays (targets)\n",
    "valid_set = utils.TensorDataset(valid_x,valid_y) # create your datset\n",
    "valid_loader = utils.DataLoader(valid_set, batch_size=64) # create your dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (img_to_features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (features_to_hidden): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_latent): Sequential(\n",
      "    (0): L0Pair(256 -> 2*10, droprate_init=0.2, lamba=0.1, temperature=0.05, weight_decay=0.001, local_rep=False)\n",
      "  )\n",
      "  (fc_alphas): ModuleList()\n",
      "  (latent_to_features): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (features_to_img): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader \n",
    "from torchvision import transforms \n",
    "from torchvision.datasets import ImageFolder \n",
    "from torch.utils.data import DataLoader \n",
    "import os \n",
    "import torch\n",
    "from jointvae.models_f import VAE\n",
    "from jointvae.training import Trainer\n",
    "from torch import optim\n",
    "from viz.visualize_c import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#valid_loader, train_loader, test_loader = get_mnist_dataloaders(batch_size=64)\n",
    "\n",
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions  7-14\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "n_cont = 10\n",
    "disc = []\n",
    "n_disc = len(disc)\n",
    "latent_spec = {'cont': n_cont,\n",
    "               'disc': disc}\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = VAE(latent_spec=latent_spec, img_size=(1, 64, 64)).cuda()\n",
    "# model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32)).cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "lr=0.00005\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "gamma=1.0\n",
    "cont_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "\n",
    "lambda_d = 2\n",
    "lambda_od = 10*lambda_d\n",
    "lambda_dis = 20*lambda_d \n",
    "path=\"Evaluations/\".format(n_cont,gamma,lambda_d)\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity,lambda_d = lambda_d,\n",
    "                  lambda_od = lambda_od, lambda_dis = lambda_dis )\n",
    "# Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from jointvae.training import Trainer\n",
    "\n",
    "\n",
    "trainer._train_epoch(train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/649600\tLoss: 143.619\tL0 Loss: 0.529\n",
      "3200/649600\tLoss: 140.775\tL0 Loss: 0.529\n",
      "6400/649600\tLoss: 134.487\tL0 Loss: 0.529\n",
      "9600/649600\tLoss: 128.890\tL0 Loss: 0.529\n",
      "12800/649600\tLoss: 122.201\tL0 Loss: 0.529\n",
      "16000/649600\tLoss: 119.447\tL0 Loss: 0.529\n",
      "19200/649600\tLoss: 120.974\tL0 Loss: 0.529\n",
      "22400/649600\tLoss: 119.902\tL0 Loss: 0.529\n",
      "25600/649600\tLoss: 119.810\tL0 Loss: 0.529\n",
      "28800/649600\tLoss: 119.188\tL0 Loss: 0.529\n",
      "32000/649600\tLoss: 119.087\tL0 Loss: 0.529\n",
      "35200/649600\tLoss: 118.923\tL0 Loss: 0.529\n",
      "38400/649600\tLoss: 116.630\tL0 Loss: 0.529\n",
      "41600/649600\tLoss: 122.281\tL0 Loss: 0.529\n",
      "44800/649600\tLoss: 138.281\tL0 Loss: 0.529\n",
      "48000/649600\tLoss: 120.899\tL0 Loss: 0.529\n",
      "51200/649600\tLoss: 114.021\tL0 Loss: 0.530\n",
      "54400/649600\tLoss: 106.756\tL0 Loss: 0.530\n",
      "57600/649600\tLoss: 102.205\tL0 Loss: 0.530\n",
      "60800/649600\tLoss: 97.640\tL0 Loss: 0.530\n",
      "64000/649600\tLoss: 95.880\tL0 Loss: 0.530\n",
      "67200/649600\tLoss: 89.821\tL0 Loss: 0.530\n",
      "70400/649600\tLoss: 85.901\tL0 Loss: 0.530\n",
      "73600/649600\tLoss: 87.777\tL0 Loss: 0.530\n",
      "76800/649600\tLoss: 83.880\tL0 Loss: 0.530\n",
      "80000/649600\tLoss: 81.644\tL0 Loss: 0.530\n",
      "83200/649600\tLoss: 85.167\tL0 Loss: 0.530\n",
      "86400/649600\tLoss: 91.482\tL0 Loss: 0.530\n",
      "89600/649600\tLoss: 88.625\tL0 Loss: 0.530\n",
      "92800/649600\tLoss: 91.720\tL0 Loss: 0.530\n",
      "96000/649600\tLoss: 84.006\tL0 Loss: 0.530\n",
      "99200/649600\tLoss: 83.793\tL0 Loss: 0.530\n",
      "102400/649600\tLoss: 89.271\tL0 Loss: 0.530\n",
      "105600/649600\tLoss: 84.847\tL0 Loss: 0.530\n",
      "108800/649600\tLoss: 81.398\tL0 Loss: 0.530\n",
      "112000/649600\tLoss: 80.858\tL0 Loss: 0.530\n",
      "115200/649600\tLoss: 83.532\tL0 Loss: 0.530\n",
      "118400/649600\tLoss: 77.613\tL0 Loss: 0.530\n",
      "121600/649600\tLoss: 78.312\tL0 Loss: 0.530\n",
      "124800/649600\tLoss: 86.681\tL0 Loss: 0.530\n",
      "128000/649600\tLoss: 88.728\tL0 Loss: 0.530\n",
      "131200/649600\tLoss: 92.880\tL0 Loss: 0.530\n",
      "134400/649600\tLoss: 91.877\tL0 Loss: 0.530\n",
      "137600/649600\tLoss: 89.062\tL0 Loss: 0.530\n",
      "140800/649600\tLoss: 88.893\tL0 Loss: 0.530\n",
      "144000/649600\tLoss: 94.251\tL0 Loss: 0.531\n",
      "147200/649600\tLoss: 89.767\tL0 Loss: 0.531\n",
      "150400/649600\tLoss: 97.215\tL0 Loss: 0.531\n",
      "153600/649600\tLoss: 91.538\tL0 Loss: 0.531\n",
      "156800/649600\tLoss: 82.299\tL0 Loss: 0.531\n",
      "160000/649600\tLoss: 85.198\tL0 Loss: 0.531\n",
      "163200/649600\tLoss: 82.990\tL0 Loss: 0.531\n",
      "166400/649600\tLoss: 98.778\tL0 Loss: 0.531\n",
      "169600/649600\tLoss: 110.011\tL0 Loss: 0.531\n",
      "172800/649600\tLoss: 106.166\tL0 Loss: 0.531\n",
      "176000/649600\tLoss: 97.486\tL0 Loss: 0.531\n",
      "179200/649600\tLoss: 99.710\tL0 Loss: 0.531\n",
      "182400/649600\tLoss: 92.336\tL0 Loss: 0.531\n",
      "185600/649600\tLoss: 95.062\tL0 Loss: 0.531\n",
      "188800/649600\tLoss: 107.823\tL0 Loss: 0.531\n",
      "192000/649600\tLoss: 104.385\tL0 Loss: 0.531\n",
      "195200/649600\tLoss: 106.008\tL0 Loss: 0.531\n",
      "198400/649600\tLoss: 99.281\tL0 Loss: 0.531\n",
      "201600/649600\tLoss: 105.497\tL0 Loss: 0.531\n",
      "204800/649600\tLoss: 95.866\tL0 Loss: 0.531\n",
      "208000/649600\tLoss: 124.518\tL0 Loss: 0.531\n",
      "211200/649600\tLoss: 103.008\tL0 Loss: 0.531\n",
      "214400/649600\tLoss: 109.249\tL0 Loss: 0.531\n",
      "217600/649600\tLoss: 107.326\tL0 Loss: 0.531\n",
      "220800/649600\tLoss: 104.781\tL0 Loss: 0.531\n",
      "224000/649600\tLoss: 110.055\tL0 Loss: 0.531\n",
      "227200/649600\tLoss: 108.232\tL0 Loss: 0.531\n",
      "230400/649600\tLoss: 114.223\tL0 Loss: 0.531\n",
      "233600/649600\tLoss: 109.062\tL0 Loss: 0.531\n",
      "236800/649600\tLoss: 102.547\tL0 Loss: 0.531\n",
      "240000/649600\tLoss: 108.577\tL0 Loss: 0.531\n",
      "243200/649600\tLoss: 108.211\tL0 Loss: 0.531\n",
      "246400/649600\tLoss: 118.086\tL0 Loss: 0.531\n",
      "249600/649600\tLoss: 93.665\tL0 Loss: 0.531\n",
      "252800/649600\tLoss: 83.309\tL0 Loss: 0.531\n",
      "256000/649600\tLoss: 78.681\tL0 Loss: 0.531\n",
      "259200/649600\tLoss: 76.199\tL0 Loss: 0.531\n",
      "262400/649600\tLoss: 70.424\tL0 Loss: 0.531\n",
      "265600/649600\tLoss: 65.934\tL0 Loss: 0.531\n",
      "268800/649600\tLoss: 64.103\tL0 Loss: 0.531\n",
      "272000/649600\tLoss: 61.786\tL0 Loss: 0.531\n",
      "275200/649600\tLoss: 60.595\tL0 Loss: 0.531\n",
      "278400/649600\tLoss: 58.485\tL0 Loss: 0.531\n",
      "281600/649600\tLoss: 57.280\tL0 Loss: 0.531\n",
      "284800/649600\tLoss: 59.329\tL0 Loss: 0.532\n",
      "288000/649600\tLoss: 67.607\tL0 Loss: 0.532\n",
      "291200/649600\tLoss: 67.985\tL0 Loss: 0.532\n",
      "294400/649600\tLoss: 67.014\tL0 Loss: 0.532\n",
      "297600/649600\tLoss: 66.730\tL0 Loss: 0.532\n",
      "300800/649600\tLoss: 68.039\tL0 Loss: 0.532\n",
      "304000/649600\tLoss: 67.855\tL0 Loss: 0.532\n",
      "307200/649600\tLoss: 67.626\tL0 Loss: 0.532\n",
      "310400/649600\tLoss: 64.864\tL0 Loss: 0.532\n",
      "313600/649600\tLoss: 62.999\tL0 Loss: 0.532\n",
      "316800/649600\tLoss: 62.725\tL0 Loss: 0.532\n",
      "320000/649600\tLoss: 63.759\tL0 Loss: 0.532\n",
      "323200/649600\tLoss: 63.388\tL0 Loss: 0.532\n",
      "326400/649600\tLoss: 61.823\tL0 Loss: 0.532\n",
      "329600/649600\tLoss: 72.253\tL0 Loss: 0.532\n",
      "332800/649600\tLoss: 69.305\tL0 Loss: 0.532\n",
      "336000/649600\tLoss: 71.319\tL0 Loss: 0.532\n",
      "339200/649600\tLoss: 74.361\tL0 Loss: 0.532\n",
      "342400/649600\tLoss: 74.640\tL0 Loss: 0.532\n",
      "345600/649600\tLoss: 77.317\tL0 Loss: 0.532\n",
      "348800/649600\tLoss: 72.660\tL0 Loss: 0.532\n",
      "352000/649600\tLoss: 71.031\tL0 Loss: 0.532\n",
      "355200/649600\tLoss: 73.715\tL0 Loss: 0.532\n",
      "358400/649600\tLoss: 73.284\tL0 Loss: 0.532\n",
      "361600/649600\tLoss: 73.565\tL0 Loss: 0.532\n",
      "364800/649600\tLoss: 70.535\tL0 Loss: 0.532\n",
      "368000/649600\tLoss: 75.359\tL0 Loss: 0.532\n",
      "371200/649600\tLoss: 82.882\tL0 Loss: 0.532\n",
      "374400/649600\tLoss: 84.985\tL0 Loss: 0.532\n",
      "377600/649600\tLoss: 85.701\tL0 Loss: 0.532\n",
      "380800/649600\tLoss: 79.332\tL0 Loss: 0.532\n",
      "384000/649600\tLoss: 84.047\tL0 Loss: 0.532\n",
      "387200/649600\tLoss: 80.523\tL0 Loss: 0.532\n",
      "390400/649600\tLoss: 79.906\tL0 Loss: 0.532\n",
      "393600/649600\tLoss: 90.030\tL0 Loss: 0.532\n",
      "396800/649600\tLoss: 91.399\tL0 Loss: 0.532\n",
      "400000/649600\tLoss: 86.133\tL0 Loss: 0.532\n",
      "403200/649600\tLoss: 84.270\tL0 Loss: 0.532\n",
      "406400/649600\tLoss: 81.244\tL0 Loss: 0.532\n",
      "409600/649600\tLoss: 82.851\tL0 Loss: 0.532\n",
      "412800/649600\tLoss: 91.934\tL0 Loss: 0.532\n",
      "416000/649600\tLoss: 102.699\tL0 Loss: 0.532\n",
      "419200/649600\tLoss: 106.397\tL0 Loss: 0.532\n",
      "422400/649600\tLoss: 99.458\tL0 Loss: 0.532\n",
      "425600/649600\tLoss: 103.114\tL0 Loss: 0.532\n",
      "428800/649600\tLoss: 104.509\tL0 Loss: 0.532\n",
      "432000/649600\tLoss: 92.705\tL0 Loss: 0.532\n",
      "435200/649600\tLoss: 95.812\tL0 Loss: 0.532\n",
      "438400/649600\tLoss: 98.570\tL0 Loss: 0.532\n",
      "441600/649600\tLoss: 95.808\tL0 Loss: 0.532\n",
      "444800/649600\tLoss: 105.418\tL0 Loss: 0.533\n",
      "448000/649600\tLoss: 116.618\tL0 Loss: 0.533\n",
      "451200/649600\tLoss: 106.920\tL0 Loss: 0.533\n",
      "454400/649600\tLoss: 105.493\tL0 Loss: 0.533\n",
      "457600/649600\tLoss: 111.310\tL0 Loss: 0.533\n",
      "460800/649600\tLoss: 108.382\tL0 Loss: 0.533\n",
      "464000/649600\tLoss: 111.647\tL0 Loss: 0.533\n",
      "467200/649600\tLoss: 114.564\tL0 Loss: 0.533\n",
      "470400/649600\tLoss: 110.056\tL0 Loss: 0.533\n",
      "473600/649600\tLoss: 102.745\tL0 Loss: 0.533\n",
      "476800/649600\tLoss: 108.702\tL0 Loss: 0.533\n",
      "480000/649600\tLoss: 115.264\tL0 Loss: 0.533\n",
      "483200/649600\tLoss: 108.044\tL0 Loss: 0.533\n",
      "486400/649600\tLoss: 116.261\tL0 Loss: 0.533\n",
      "489600/649600\tLoss: 122.552\tL0 Loss: 0.533\n",
      "492800/649600\tLoss: 110.317\tL0 Loss: 0.533\n",
      "496000/649600\tLoss: 79.746\tL0 Loss: 0.533\n",
      "499200/649600\tLoss: 77.995\tL0 Loss: 0.533\n",
      "502400/649600\tLoss: 76.944\tL0 Loss: 0.533\n",
      "505600/649600\tLoss: 76.496\tL0 Loss: 0.533\n",
      "508800/649600\tLoss: 76.008\tL0 Loss: 0.533\n",
      "512000/649600\tLoss: 74.898\tL0 Loss: 0.533\n",
      "515200/649600\tLoss: 74.833\tL0 Loss: 0.533\n",
      "518400/649600\tLoss: 73.208\tL0 Loss: 0.533\n",
      "521600/649600\tLoss: 71.815\tL0 Loss: 0.533\n",
      "524800/649600\tLoss: 70.306\tL0 Loss: 0.533\n",
      "528000/649600\tLoss: 69.400\tL0 Loss: 0.533\n",
      "531200/649600\tLoss: 67.017\tL0 Loss: 0.533\n",
      "534400/649600\tLoss: 66.964\tL0 Loss: 0.533\n",
      "537600/649600\tLoss: 66.725\tL0 Loss: 0.533\n",
      "540800/649600\tLoss: 65.323\tL0 Loss: 0.533\n",
      "544000/649600\tLoss: 62.375\tL0 Loss: 0.533\n",
      "547200/649600\tLoss: 59.343\tL0 Loss: 0.533\n",
      "550400/649600\tLoss: 63.009\tL0 Loss: 0.533\n",
      "553600/649600\tLoss: 57.688\tL0 Loss: 0.533\n",
      "556800/649600\tLoss: 57.020\tL0 Loss: 0.533\n",
      "560000/649600\tLoss: 56.085\tL0 Loss: 0.533\n",
      "563200/649600\tLoss: 55.806\tL0 Loss: 0.533\n",
      "566400/649600\tLoss: 56.148\tL0 Loss: 0.533\n",
      "569600/649600\tLoss: 55.241\tL0 Loss: 0.533\n",
      "572800/649600\tLoss: 53.198\tL0 Loss: 0.533\n",
      "576000/649600\tLoss: 63.401\tL0 Loss: 0.533\n",
      "579200/649600\tLoss: 62.916\tL0 Loss: 0.533\n",
      "582400/649600\tLoss: 62.554\tL0 Loss: 0.533\n",
      "585600/649600\tLoss: 61.097\tL0 Loss: 0.533\n",
      "588800/649600\tLoss: 63.462\tL0 Loss: 0.533\n",
      "592000/649600\tLoss: 61.874\tL0 Loss: 0.533\n",
      "595200/649600\tLoss: 61.682\tL0 Loss: 0.533\n",
      "598400/649600\tLoss: 63.013\tL0 Loss: 0.533\n",
      "601600/649600\tLoss: 62.843\tL0 Loss: 0.533\n",
      "604800/649600\tLoss: 63.423\tL0 Loss: 0.533\n",
      "608000/649600\tLoss: 64.556\tL0 Loss: 0.533\n",
      "611200/649600\tLoss: 66.069\tL0 Loss: 0.533\n",
      "614400/649600\tLoss: 60.527\tL0 Loss: 0.533\n",
      "617600/649600\tLoss: 71.623\tL0 Loss: 0.533\n",
      "620800/649600\tLoss: 72.140\tL0 Loss: 0.533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624000/649600\tLoss: 70.047\tL0 Loss: 0.533\n",
      "627200/649600\tLoss: 68.564\tL0 Loss: 0.533\n",
      "630400/649600\tLoss: 68.256\tL0 Loss: 0.533\n",
      "633600/649600\tLoss: 68.913\tL0 Loss: 0.533\n",
      "636800/649600\tLoss: 67.916\tL0 Loss: 0.534\n",
      "640000/649600\tLoss: 68.675\tL0 Loss: 0.534\n",
      "643200/649600\tLoss: 71.615\tL0 Loss: 0.534\n",
      "646400/649600\tLoss: 68.723\tL0 Loss: 0.534\n",
      "Valid Loss: 144.206, Recon Error: 0.016\n",
      "144.20617746448025\n",
      "Epoch: 1 Average loss: 86.47 Valid loss: 144.20617746448025\tRecon Error:0.016\n",
      "0/649600\tLoss: 68.984\tL0 Loss: 0.534\n",
      "3200/649600\tLoss: 57.928\tL0 Loss: 0.534\n",
      "6400/649600\tLoss: 52.503\tL0 Loss: 0.534\n",
      "9600/649600\tLoss: 52.852\tL0 Loss: 0.534\n",
      "12800/649600\tLoss: 54.162\tL0 Loss: 0.534\n",
      "16000/649600\tLoss: 54.569\tL0 Loss: 0.534\n",
      "19200/649600\tLoss: 53.103\tL0 Loss: 0.534\n",
      "22400/649600\tLoss: 52.248\tL0 Loss: 0.534\n",
      "25600/649600\tLoss: 51.965\tL0 Loss: 0.534\n",
      "28800/649600\tLoss: 53.166\tL0 Loss: 0.534\n",
      "32000/649600\tLoss: 53.937\tL0 Loss: 0.534\n",
      "35200/649600\tLoss: 50.606\tL0 Loss: 0.534\n",
      "38400/649600\tLoss: 51.110\tL0 Loss: 0.534\n",
      "41600/649600\tLoss: 55.380\tL0 Loss: 0.534\n",
      "44800/649600\tLoss: 63.813\tL0 Loss: 0.534\n",
      "48000/649600\tLoss: 58.312\tL0 Loss: 0.534\n",
      "51200/649600\tLoss: 62.825\tL0 Loss: 0.534\n",
      "54400/649600\tLoss: 62.432\tL0 Loss: 0.534\n",
      "57600/649600\tLoss: 58.863\tL0 Loss: 0.534\n",
      "60800/649600\tLoss: 54.822\tL0 Loss: 0.534\n",
      "64000/649600\tLoss: 56.664\tL0 Loss: 0.534\n",
      "67200/649600\tLoss: 58.646\tL0 Loss: 0.534\n",
      "70400/649600\tLoss: 59.462\tL0 Loss: 0.534\n",
      "73600/649600\tLoss: 57.550\tL0 Loss: 0.534\n",
      "76800/649600\tLoss: 62.492\tL0 Loss: 0.534\n",
      "80000/649600\tLoss: 58.571\tL0 Loss: 0.534\n",
      "83200/649600\tLoss: 68.412\tL0 Loss: 0.534\n",
      "86400/649600\tLoss: 68.396\tL0 Loss: 0.534\n",
      "89600/649600\tLoss: 65.389\tL0 Loss: 0.534\n",
      "92800/649600\tLoss: 70.656\tL0 Loss: 0.534\n",
      "96000/649600\tLoss: 66.001\tL0 Loss: 0.534\n",
      "99200/649600\tLoss: 64.207\tL0 Loss: 0.534\n",
      "102400/649600\tLoss: 67.660\tL0 Loss: 0.534\n",
      "105600/649600\tLoss: 68.914\tL0 Loss: 0.534\n",
      "108800/649600\tLoss: 67.932\tL0 Loss: 0.534\n",
      "112000/649600\tLoss: 68.258\tL0 Loss: 0.534\n",
      "115200/649600\tLoss: 66.931\tL0 Loss: 0.534\n",
      "118400/649600\tLoss: 66.870\tL0 Loss: 0.534\n",
      "121600/649600\tLoss: 65.574\tL0 Loss: 0.534\n",
      "124800/649600\tLoss: 74.323\tL0 Loss: 0.534\n",
      "128000/649600\tLoss: 77.038\tL0 Loss: 0.534\n",
      "131200/649600\tLoss: 76.085\tL0 Loss: 0.534\n",
      "134400/649600\tLoss: 76.660\tL0 Loss: 0.535\n",
      "137600/649600\tLoss: 80.113\tL0 Loss: 0.535\n",
      "140800/649600\tLoss: 75.214\tL0 Loss: 0.535\n",
      "144000/649600\tLoss: 79.063\tL0 Loss: 0.535\n",
      "147200/649600\tLoss: 74.042\tL0 Loss: 0.535\n",
      "150400/649600\tLoss: 83.786\tL0 Loss: 0.535\n",
      "153600/649600\tLoss: 77.145\tL0 Loss: 0.535\n",
      "156800/649600\tLoss: 73.052\tL0 Loss: 0.535\n",
      "160000/649600\tLoss: 76.053\tL0 Loss: 0.535\n",
      "163200/649600\tLoss: 82.098\tL0 Loss: 0.535\n",
      "166400/649600\tLoss: 89.210\tL0 Loss: 0.535\n",
      "169600/649600\tLoss: 90.687\tL0 Loss: 0.535\n",
      "172800/649600\tLoss: 86.644\tL0 Loss: 0.535\n",
      "176000/649600\tLoss: 86.043\tL0 Loss: 0.535\n",
      "179200/649600\tLoss: 90.367\tL0 Loss: 0.535\n",
      "182400/649600\tLoss: 89.502\tL0 Loss: 0.535\n",
      "185600/649600\tLoss: 84.351\tL0 Loss: 0.535\n",
      "188800/649600\tLoss: 86.163\tL0 Loss: 0.535\n",
      "192000/649600\tLoss: 85.882\tL0 Loss: 0.535\n",
      "195200/649600\tLoss: 88.195\tL0 Loss: 0.535\n",
      "198400/649600\tLoss: 98.252\tL0 Loss: 0.535\n",
      "201600/649600\tLoss: 90.329\tL0 Loss: 0.535\n",
      "204800/649600\tLoss: 88.955\tL0 Loss: 0.535\n",
      "208000/649600\tLoss: 103.442\tL0 Loss: 0.535\n",
      "211200/649600\tLoss: 99.095\tL0 Loss: 0.535\n",
      "214400/649600\tLoss: 104.056\tL0 Loss: 0.535\n",
      "217600/649600\tLoss: 99.535\tL0 Loss: 0.535\n",
      "220800/649600\tLoss: 97.421\tL0 Loss: 0.535\n",
      "224000/649600\tLoss: 100.296\tL0 Loss: 0.535\n",
      "227200/649600\tLoss: 95.437\tL0 Loss: 0.535\n",
      "230400/649600\tLoss: 97.102\tL0 Loss: 0.535\n",
      "233600/649600\tLoss: 102.784\tL0 Loss: 0.535\n",
      "236800/649600\tLoss: 102.672\tL0 Loss: 0.535\n",
      "240000/649600\tLoss: 120.916\tL0 Loss: 0.535\n",
      "243200/649600\tLoss: 111.817\tL0 Loss: 0.535\n",
      "246400/649600\tLoss: 114.795\tL0 Loss: 0.535\n",
      "249600/649600\tLoss: 95.155\tL0 Loss: 0.535\n",
      "252800/649600\tLoss: 90.490\tL0 Loss: 0.535\n",
      "256000/649600\tLoss: 86.597\tL0 Loss: 0.535\n",
      "259200/649600\tLoss: 81.912\tL0 Loss: 0.535\n",
      "262400/649600\tLoss: 76.012\tL0 Loss: 0.535\n",
      "265600/649600\tLoss: 71.700\tL0 Loss: 0.535\n",
      "268800/649600\tLoss: 68.098\tL0 Loss: 0.535\n",
      "272000/649600\tLoss: 66.125\tL0 Loss: 0.535\n",
      "275200/649600\tLoss: 62.341\tL0 Loss: 0.535\n",
      "278400/649600\tLoss: 60.143\tL0 Loss: 0.535\n",
      "281600/649600\tLoss: 58.957\tL0 Loss: 0.535\n",
      "284800/649600\tLoss: 57.602\tL0 Loss: 0.535\n",
      "288000/649600\tLoss: 61.958\tL0 Loss: 0.535\n",
      "291200/649600\tLoss: 62.143\tL0 Loss: 0.535\n",
      "294400/649600\tLoss: 62.582\tL0 Loss: 0.535\n",
      "297600/649600\tLoss: 61.315\tL0 Loss: 0.535\n",
      "300800/649600\tLoss: 61.038\tL0 Loss: 0.535\n",
      "304000/649600\tLoss: 63.504\tL0 Loss: 0.536\n",
      "307200/649600\tLoss: 59.851\tL0 Loss: 0.536\n",
      "310400/649600\tLoss: 59.499\tL0 Loss: 0.536\n",
      "313600/649600\tLoss: 59.514\tL0 Loss: 0.536\n",
      "316800/649600\tLoss: 60.132\tL0 Loss: 0.536\n",
      "320000/649600\tLoss: 60.022\tL0 Loss: 0.536\n",
      "323200/649600\tLoss: 62.371\tL0 Loss: 0.536\n",
      "326400/649600\tLoss: 64.216\tL0 Loss: 0.536\n",
      "329600/649600\tLoss: 67.444\tL0 Loss: 0.536\n",
      "332800/649600\tLoss: 70.959\tL0 Loss: 0.536\n",
      "336000/649600\tLoss: 68.765\tL0 Loss: 0.536\n",
      "339200/649600\tLoss: 65.609\tL0 Loss: 0.536\n",
      "342400/649600\tLoss: 70.072\tL0 Loss: 0.536\n",
      "345600/649600\tLoss: 70.852\tL0 Loss: 0.536\n",
      "348800/649600\tLoss: 71.088\tL0 Loss: 0.536\n",
      "352000/649600\tLoss: 68.611\tL0 Loss: 0.536\n",
      "355200/649600\tLoss: 68.897\tL0 Loss: 0.536\n",
      "358400/649600\tLoss: 70.826\tL0 Loss: 0.536\n",
      "361600/649600\tLoss: 69.065\tL0 Loss: 0.536\n",
      "364800/649600\tLoss: 69.860\tL0 Loss: 0.536\n",
      "368000/649600\tLoss: 70.826\tL0 Loss: 0.536\n",
      "371200/649600\tLoss: 78.279\tL0 Loss: 0.536\n",
      "374400/649600\tLoss: 76.112\tL0 Loss: 0.536\n",
      "377600/649600\tLoss: 80.452\tL0 Loss: 0.536\n",
      "380800/649600\tLoss: 77.439\tL0 Loss: 0.536\n",
      "384000/649600\tLoss: 77.945\tL0 Loss: 0.536\n",
      "387200/649600\tLoss: 78.518\tL0 Loss: 0.536\n",
      "390400/649600\tLoss: 76.299\tL0 Loss: 0.536\n",
      "393600/649600\tLoss: 79.385\tL0 Loss: 0.536\n",
      "396800/649600\tLoss: 80.571\tL0 Loss: 0.536\n",
      "400000/649600\tLoss: 82.250\tL0 Loss: 0.536\n",
      "403200/649600\tLoss: 82.307\tL0 Loss: 0.536\n",
      "406400/649600\tLoss: 80.831\tL0 Loss: 0.536\n",
      "409600/649600\tLoss: 77.214\tL0 Loss: 0.536\n",
      "412800/649600\tLoss: 89.498\tL0 Loss: 0.536\n",
      "416000/649600\tLoss: 91.471\tL0 Loss: 0.536\n",
      "419200/649600\tLoss: 91.786\tL0 Loss: 0.536\n",
      "422400/649600\tLoss: 96.720\tL0 Loss: 0.536\n",
      "425600/649600\tLoss: 98.670\tL0 Loss: 0.536\n",
      "428800/649600\tLoss: 99.105\tL0 Loss: 0.536\n",
      "432000/649600\tLoss: 89.539\tL0 Loss: 0.536\n",
      "435200/649600\tLoss: 88.830\tL0 Loss: 0.536\n",
      "438400/649600\tLoss: 92.576\tL0 Loss: 0.536\n",
      "441600/649600\tLoss: 94.913\tL0 Loss: 0.536\n",
      "444800/649600\tLoss: 101.569\tL0 Loss: 0.536\n",
      "448000/649600\tLoss: 97.754\tL0 Loss: 0.536\n",
      "451200/649600\tLoss: 97.199\tL0 Loss: 0.536\n",
      "454400/649600\tLoss: 104.935\tL0 Loss: 0.536\n",
      "457600/649600\tLoss: 116.044\tL0 Loss: 0.536\n",
      "460800/649600\tLoss: 110.218\tL0 Loss: 0.537\n",
      "464000/649600\tLoss: 106.618\tL0 Loss: 0.537\n",
      "467200/649600\tLoss: 111.144\tL0 Loss: 0.537\n",
      "470400/649600\tLoss: 107.268\tL0 Loss: 0.537\n",
      "473600/649600\tLoss: 105.154\tL0 Loss: 0.537\n",
      "476800/649600\tLoss: 110.482\tL0 Loss: 0.537\n",
      "480000/649600\tLoss: 104.833\tL0 Loss: 0.537\n",
      "483200/649600\tLoss: 112.172\tL0 Loss: 0.537\n",
      "486400/649600\tLoss: 109.564\tL0 Loss: 0.537\n",
      "489600/649600\tLoss: 103.707\tL0 Loss: 0.537\n",
      "492800/649600\tLoss: 104.814\tL0 Loss: 0.537\n",
      "496000/649600\tLoss: 78.663\tL0 Loss: 0.537\n",
      "499200/649600\tLoss: 75.350\tL0 Loss: 0.537\n",
      "502400/649600\tLoss: 71.416\tL0 Loss: 0.537\n",
      "505600/649600\tLoss: 65.178\tL0 Loss: 0.537\n",
      "508800/649600\tLoss: 58.304\tL0 Loss: 0.537\n",
      "512000/649600\tLoss: 55.502\tL0 Loss: 0.537\n",
      "515200/649600\tLoss: 50.599\tL0 Loss: 0.537\n",
      "518400/649600\tLoss: 51.931\tL0 Loss: 0.537\n",
      "521600/649600\tLoss: 49.654\tL0 Loss: 0.537\n",
      "524800/649600\tLoss: 49.361\tL0 Loss: 0.537\n",
      "528000/649600\tLoss: 47.090\tL0 Loss: 0.537\n",
      "531200/649600\tLoss: 48.720\tL0 Loss: 0.537\n",
      "534400/649600\tLoss: 52.982\tL0 Loss: 0.537\n",
      "537600/649600\tLoss: 53.048\tL0 Loss: 0.537\n",
      "540800/649600\tLoss: 53.725\tL0 Loss: 0.537\n",
      "544000/649600\tLoss: 54.905\tL0 Loss: 0.537\n",
      "547200/649600\tLoss: 52.536\tL0 Loss: 0.537\n",
      "550400/649600\tLoss: 52.711\tL0 Loss: 0.537\n",
      "553600/649600\tLoss: 51.494\tL0 Loss: 0.537\n",
      "556800/649600\tLoss: 51.965\tL0 Loss: 0.537\n",
      "560000/649600\tLoss: 50.880\tL0 Loss: 0.537\n",
      "563200/649600\tLoss: 54.159\tL0 Loss: 0.537\n",
      "566400/649600\tLoss: 51.407\tL0 Loss: 0.537\n",
      "569600/649600\tLoss: 50.782\tL0 Loss: 0.537\n",
      "572800/649600\tLoss: 51.377\tL0 Loss: 0.537\n",
      "576000/649600\tLoss: 57.956\tL0 Loss: 0.537\n",
      "579200/649600\tLoss: 59.038\tL0 Loss: 0.537\n",
      "582400/649600\tLoss: 58.097\tL0 Loss: 0.537\n",
      "585600/649600\tLoss: 57.415\tL0 Loss: 0.537\n",
      "588800/649600\tLoss: 58.470\tL0 Loss: 0.537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592000/649600\tLoss: 58.423\tL0 Loss: 0.537\n",
      "595200/649600\tLoss: 56.373\tL0 Loss: 0.537\n",
      "598400/649600\tLoss: 58.101\tL0 Loss: 0.537\n",
      "601600/649600\tLoss: 58.111\tL0 Loss: 0.537\n",
      "604800/649600\tLoss: 57.580\tL0 Loss: 0.537\n",
      "608000/649600\tLoss: 59.451\tL0 Loss: 0.537\n",
      "611200/649600\tLoss: 59.065\tL0 Loss: 0.537\n",
      "614400/649600\tLoss: 57.034\tL0 Loss: 0.537\n",
      "617600/649600\tLoss: 70.470\tL0 Loss: 0.537\n",
      "620800/649600\tLoss: 71.096\tL0 Loss: 0.537\n",
      "624000/649600\tLoss: 67.659\tL0 Loss: 0.537\n",
      "627200/649600\tLoss: 70.554\tL0 Loss: 0.537\n",
      "630400/649600\tLoss: 67.409\tL0 Loss: 0.537\n",
      "633600/649600\tLoss: 67.367\tL0 Loss: 0.537\n",
      "636800/649600\tLoss: 64.723\tL0 Loss: 0.537\n",
      "640000/649600\tLoss: 64.515\tL0 Loss: 0.537\n",
      "643200/649600\tLoss: 63.473\tL0 Loss: 0.537\n",
      "646400/649600\tLoss: 64.788\tL0 Loss: 0.537\n",
      "Valid Loss: 136.766, Recon Error: 0.017\n",
      "136.76647044945744\n",
      "Epoch: 2 Average loss: 73.43 Valid loss: 136.76647044945744\tRecon Error:0.017\n",
      "0/649600\tLoss: 68.723\tL0 Loss: 0.537\n",
      "3200/649600\tLoss: 55.908\tL0 Loss: 0.537\n",
      "6400/649600\tLoss: 53.611\tL0 Loss: 0.538\n",
      "9600/649600\tLoss: 49.134\tL0 Loss: 0.538\n",
      "12800/649600\tLoss: 50.377\tL0 Loss: 0.538\n",
      "16000/649600\tLoss: 50.616\tL0 Loss: 0.538\n",
      "19200/649600\tLoss: 48.716\tL0 Loss: 0.538\n",
      "22400/649600\tLoss: 48.946\tL0 Loss: 0.538\n",
      "25600/649600\tLoss: 49.595\tL0 Loss: 0.538\n",
      "28800/649600\tLoss: 48.984\tL0 Loss: 0.538\n",
      "32000/649600\tLoss: 51.580\tL0 Loss: 0.538\n",
      "35200/649600\tLoss: 48.734\tL0 Loss: 0.538\n",
      "38400/649600\tLoss: 51.065\tL0 Loss: 0.538\n",
      "41600/649600\tLoss: 52.816\tL0 Loss: 0.538\n",
      "44800/649600\tLoss: 58.580\tL0 Loss: 0.538\n",
      "48000/649600\tLoss: 58.087\tL0 Loss: 0.538\n",
      "51200/649600\tLoss: 61.967\tL0 Loss: 0.538\n",
      "54400/649600\tLoss: 55.839\tL0 Loss: 0.538\n",
      "57600/649600\tLoss: 57.305\tL0 Loss: 0.538\n",
      "60800/649600\tLoss: 57.303\tL0 Loss: 0.538\n",
      "64000/649600\tLoss: 54.517\tL0 Loss: 0.538\n",
      "67200/649600\tLoss: 57.478\tL0 Loss: 0.538\n",
      "70400/649600\tLoss: 58.025\tL0 Loss: 0.538\n",
      "73600/649600\tLoss: 54.957\tL0 Loss: 0.538\n",
      "76800/649600\tLoss: 55.954\tL0 Loss: 0.538\n",
      "80000/649600\tLoss: 58.545\tL0 Loss: 0.538\n",
      "83200/649600\tLoss: 63.474\tL0 Loss: 0.538\n",
      "86400/649600\tLoss: 65.246\tL0 Loss: 0.538\n",
      "89600/649600\tLoss: 63.963\tL0 Loss: 0.538\n",
      "92800/649600\tLoss: 61.689\tL0 Loss: 0.538\n",
      "96000/649600\tLoss: 62.115\tL0 Loss: 0.538\n",
      "99200/649600\tLoss: 67.882\tL0 Loss: 0.538\n",
      "102400/649600\tLoss: 66.002\tL0 Loss: 0.538\n",
      "105600/649600\tLoss: 61.107\tL0 Loss: 0.538\n",
      "108800/649600\tLoss: 63.586\tL0 Loss: 0.538\n",
      "112000/649600\tLoss: 67.574\tL0 Loss: 0.538\n",
      "115200/649600\tLoss: 68.571\tL0 Loss: 0.538\n",
      "118400/649600\tLoss: 65.388\tL0 Loss: 0.538\n",
      "121600/649600\tLoss: 65.405\tL0 Loss: 0.538\n",
      "124800/649600\tLoss: 71.301\tL0 Loss: 0.538\n",
      "128000/649600\tLoss: 72.271\tL0 Loss: 0.538\n",
      "131200/649600\tLoss: 69.954\tL0 Loss: 0.538\n",
      "134400/649600\tLoss: 70.682\tL0 Loss: 0.538\n",
      "137600/649600\tLoss: 74.589\tL0 Loss: 0.538\n",
      "140800/649600\tLoss: 69.989\tL0 Loss: 0.538\n",
      "144000/649600\tLoss: 72.903\tL0 Loss: 0.538\n",
      "147200/649600\tLoss: 68.511\tL0 Loss: 0.538\n",
      "150400/649600\tLoss: 68.968\tL0 Loss: 0.538\n",
      "153600/649600\tLoss: 69.213\tL0 Loss: 0.538\n",
      "156800/649600\tLoss: 70.954\tL0 Loss: 0.538\n",
      "160000/649600\tLoss: 70.538\tL0 Loss: 0.538\n",
      "163200/649600\tLoss: 75.888\tL0 Loss: 0.538\n",
      "166400/649600\tLoss: 80.624\tL0 Loss: 0.539\n",
      "169600/649600\tLoss: 84.800\tL0 Loss: 0.539\n",
      "172800/649600\tLoss: 85.759\tL0 Loss: 0.539\n",
      "176000/649600\tLoss: 100.693\tL0 Loss: 0.539\n",
      "179200/649600\tLoss: 99.355\tL0 Loss: 0.539\n",
      "182400/649600\tLoss: 90.136\tL0 Loss: 0.539\n",
      "185600/649600\tLoss: 88.008\tL0 Loss: 0.539\n",
      "188800/649600\tLoss: 93.388\tL0 Loss: 0.539\n",
      "192000/649600\tLoss: 89.044\tL0 Loss: 0.539\n",
      "195200/649600\tLoss: 88.486\tL0 Loss: 0.539\n",
      "198400/649600\tLoss: 89.758\tL0 Loss: 0.539\n",
      "201600/649600\tLoss: 85.236\tL0 Loss: 0.539\n",
      "204800/649600\tLoss: 85.327\tL0 Loss: 0.539\n",
      "208000/649600\tLoss: 100.743\tL0 Loss: 0.539\n",
      "211200/649600\tLoss: 99.678\tL0 Loss: 0.539\n",
      "214400/649600\tLoss: 96.260\tL0 Loss: 0.539\n",
      "217600/649600\tLoss: 92.449\tL0 Loss: 0.539\n",
      "220800/649600\tLoss: 93.636\tL0 Loss: 0.539\n",
      "224000/649600\tLoss: 90.453\tL0 Loss: 0.539\n",
      "227200/649600\tLoss: 92.016\tL0 Loss: 0.539\n",
      "230400/649600\tLoss: 92.131\tL0 Loss: 0.539\n",
      "233600/649600\tLoss: 92.955\tL0 Loss: 0.539\n",
      "236800/649600\tLoss: 93.349\tL0 Loss: 0.539\n",
      "240000/649600\tLoss: 85.424\tL0 Loss: 0.539\n",
      "243200/649600\tLoss: 101.330\tL0 Loss: 0.539\n",
      "246400/649600\tLoss: 115.051\tL0 Loss: 0.539\n",
      "249600/649600\tLoss: 92.178\tL0 Loss: 0.539\n",
      "252800/649600\tLoss: 80.944\tL0 Loss: 0.539\n",
      "256000/649600\tLoss: 70.895\tL0 Loss: 0.539\n",
      "259200/649600\tLoss: 64.722\tL0 Loss: 0.539\n",
      "262400/649600\tLoss: 59.854\tL0 Loss: 0.539\n",
      "265600/649600\tLoss: 57.351\tL0 Loss: 0.539\n",
      "268800/649600\tLoss: 55.054\tL0 Loss: 0.539\n",
      "272000/649600\tLoss: 54.653\tL0 Loss: 0.539\n",
      "275200/649600\tLoss: 51.790\tL0 Loss: 0.539\n",
      "278400/649600\tLoss: 51.740\tL0 Loss: 0.539\n",
      "281600/649600\tLoss: 51.627\tL0 Loss: 0.539\n",
      "284800/649600\tLoss: 51.442\tL0 Loss: 0.539\n",
      "288000/649600\tLoss: 58.378\tL0 Loss: 0.539\n",
      "291200/649600\tLoss: 56.779\tL0 Loss: 0.539\n",
      "294400/649600\tLoss: 59.358\tL0 Loss: 0.539\n",
      "297600/649600\tLoss: 60.173\tL0 Loss: 0.539\n",
      "300800/649600\tLoss: 57.756\tL0 Loss: 0.539\n",
      "304000/649600\tLoss: 60.116\tL0 Loss: 0.539\n",
      "307200/649600\tLoss: 57.914\tL0 Loss: 0.539\n",
      "310400/649600\tLoss: 58.130\tL0 Loss: 0.539\n",
      "313600/649600\tLoss: 58.711\tL0 Loss: 0.539\n",
      "316800/649600\tLoss: 58.081\tL0 Loss: 0.539\n",
      "320000/649600\tLoss: 59.971\tL0 Loss: 0.539\n",
      "323200/649600\tLoss: 59.584\tL0 Loss: 0.539\n",
      "326400/649600\tLoss: 59.484\tL0 Loss: 0.539\n",
      "329600/649600\tLoss: 65.013\tL0 Loss: 0.539\n",
      "332800/649600\tLoss: 63.684\tL0 Loss: 0.539\n",
      "336000/649600\tLoss: 73.812\tL0 Loss: 0.539\n",
      "339200/649600\tLoss: 68.245\tL0 Loss: 0.539\n",
      "342400/649600\tLoss: 68.283\tL0 Loss: 0.539\n",
      "345600/649600\tLoss: 69.460\tL0 Loss: 0.539\n",
      "348800/649600\tLoss: 67.132\tL0 Loss: 0.540\n",
      "352000/649600\tLoss: 66.354\tL0 Loss: 0.540\n",
      "355200/649600\tLoss: 67.856\tL0 Loss: 0.540\n",
      "358400/649600\tLoss: 65.616\tL0 Loss: 0.540\n",
      "361600/649600\tLoss: 66.043\tL0 Loss: 0.540\n",
      "364800/649600\tLoss: 66.153\tL0 Loss: 0.540\n",
      "368000/649600\tLoss: 68.324\tL0 Loss: 0.540\n",
      "371200/649600\tLoss: 74.324\tL0 Loss: 0.540\n",
      "374400/649600\tLoss: 75.357\tL0 Loss: 0.540\n",
      "377600/649600\tLoss: 81.234\tL0 Loss: 0.540\n",
      "380800/649600\tLoss: 76.990\tL0 Loss: 0.540\n",
      "384000/649600\tLoss: 78.725\tL0 Loss: 0.540\n",
      "387200/649600\tLoss: 77.253\tL0 Loss: 0.540\n",
      "390400/649600\tLoss: 74.022\tL0 Loss: 0.540\n",
      "393600/649600\tLoss: 81.306\tL0 Loss: 0.540\n",
      "396800/649600\tLoss: 76.944\tL0 Loss: 0.540\n",
      "400000/649600\tLoss: 78.602\tL0 Loss: 0.540\n",
      "403200/649600\tLoss: 78.516\tL0 Loss: 0.540\n",
      "406400/649600\tLoss: 75.407\tL0 Loss: 0.540\n",
      "409600/649600\tLoss: 80.407\tL0 Loss: 0.540\n",
      "412800/649600\tLoss: 87.869\tL0 Loss: 0.540\n",
      "416000/649600\tLoss: 88.942\tL0 Loss: 0.540\n",
      "419200/649600\tLoss: 90.803\tL0 Loss: 0.540\n",
      "422400/649600\tLoss: 88.240\tL0 Loss: 0.540\n",
      "425600/649600\tLoss: 90.867\tL0 Loss: 0.540\n",
      "428800/649600\tLoss: 95.735\tL0 Loss: 0.540\n",
      "432000/649600\tLoss: 85.650\tL0 Loss: 0.540\n",
      "435200/649600\tLoss: 88.987\tL0 Loss: 0.540\n",
      "438400/649600\tLoss: 89.003\tL0 Loss: 0.540\n",
      "441600/649600\tLoss: 90.809\tL0 Loss: 0.540\n",
      "444800/649600\tLoss: 88.137\tL0 Loss: 0.540\n",
      "448000/649600\tLoss: 92.455\tL0 Loss: 0.540\n",
      "451200/649600\tLoss: 92.167\tL0 Loss: 0.540\n",
      "454400/649600\tLoss: 94.843\tL0 Loss: 0.540\n",
      "457600/649600\tLoss: 104.522\tL0 Loss: 0.540\n",
      "460800/649600\tLoss: 104.488\tL0 Loss: 0.540\n",
      "464000/649600\tLoss: 97.341\tL0 Loss: 0.540\n",
      "467200/649600\tLoss: 106.917\tL0 Loss: 0.540\n",
      "470400/649600\tLoss: 93.859\tL0 Loss: 0.540\n",
      "473600/649600\tLoss: 94.080\tL0 Loss: 0.540\n",
      "476800/649600\tLoss: 105.242\tL0 Loss: 0.540\n",
      "480000/649600\tLoss: 107.010\tL0 Loss: 0.540\n",
      "483200/649600\tLoss: 104.981\tL0 Loss: 0.540\n",
      "486400/649600\tLoss: 112.123\tL0 Loss: 0.540\n",
      "489600/649600\tLoss: 111.333\tL0 Loss: 0.540\n",
      "492800/649600\tLoss: 104.152\tL0 Loss: 0.540\n",
      "496000/649600\tLoss: 76.651\tL0 Loss: 0.540\n",
      "499200/649600\tLoss: 71.456\tL0 Loss: 0.540\n",
      "502400/649600\tLoss: 63.022\tL0 Loss: 0.540\n",
      "505600/649600\tLoss: 57.948\tL0 Loss: 0.540\n",
      "508800/649600\tLoss: 53.257\tL0 Loss: 0.540\n",
      "512000/649600\tLoss: 50.001\tL0 Loss: 0.540\n",
      "515200/649600\tLoss: 48.657\tL0 Loss: 0.540\n",
      "518400/649600\tLoss: 46.967\tL0 Loss: 0.540\n",
      "521600/649600\tLoss: 46.917\tL0 Loss: 0.540\n",
      "524800/649600\tLoss: 48.972\tL0 Loss: 0.540\n",
      "528000/649600\tLoss: 47.421\tL0 Loss: 0.540\n",
      "531200/649600\tLoss: 45.124\tL0 Loss: 0.540\n",
      "534400/649600\tLoss: 50.127\tL0 Loss: 0.540\n",
      "537600/649600\tLoss: 51.357\tL0 Loss: 0.540\n",
      "540800/649600\tLoss: 50.642\tL0 Loss: 0.540\n",
      "544000/649600\tLoss: 49.247\tL0 Loss: 0.540\n",
      "547200/649600\tLoss: 49.251\tL0 Loss: 0.540\n",
      "550400/649600\tLoss: 49.489\tL0 Loss: 0.540\n",
      "553600/649600\tLoss: 49.945\tL0 Loss: 0.540\n",
      "556800/649600\tLoss: 50.814\tL0 Loss: 0.541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560000/649600\tLoss: 48.678\tL0 Loss: 0.541\n",
      "563200/649600\tLoss: 48.830\tL0 Loss: 0.541\n",
      "566400/649600\tLoss: 50.982\tL0 Loss: 0.541\n",
      "569600/649600\tLoss: 49.396\tL0 Loss: 0.541\n",
      "572800/649600\tLoss: 49.630\tL0 Loss: 0.541\n",
      "576000/649600\tLoss: 57.828\tL0 Loss: 0.541\n",
      "579200/649600\tLoss: 54.638\tL0 Loss: 0.541\n",
      "582400/649600\tLoss: 55.402\tL0 Loss: 0.541\n",
      "585600/649600\tLoss: 57.411\tL0 Loss: 0.541\n",
      "588800/649600\tLoss: 57.285\tL0 Loss: 0.541\n",
      "592000/649600\tLoss: 54.564\tL0 Loss: 0.541\n",
      "595200/649600\tLoss: 55.920\tL0 Loss: 0.541\n",
      "598400/649600\tLoss: 55.800\tL0 Loss: 0.541\n",
      "601600/649600\tLoss: 55.184\tL0 Loss: 0.541\n",
      "604800/649600\tLoss: 53.330\tL0 Loss: 0.541\n",
      "608000/649600\tLoss: 56.185\tL0 Loss: 0.541\n",
      "611200/649600\tLoss: 57.101\tL0 Loss: 0.541\n",
      "614400/649600\tLoss: 55.801\tL0 Loss: 0.541\n",
      "617600/649600\tLoss: 68.326\tL0 Loss: 0.541\n",
      "620800/649600\tLoss: 64.410\tL0 Loss: 0.541\n",
      "624000/649600\tLoss: 66.417\tL0 Loss: 0.541\n",
      "627200/649600\tLoss: 65.687\tL0 Loss: 0.541\n",
      "630400/649600\tLoss: 64.728\tL0 Loss: 0.541\n",
      "633600/649600\tLoss: 67.391\tL0 Loss: 0.541\n",
      "636800/649600\tLoss: 65.997\tL0 Loss: 0.541\n",
      "640000/649600\tLoss: 64.673\tL0 Loss: 0.541\n",
      "643200/649600\tLoss: 60.344\tL0 Loss: 0.541\n",
      "646400/649600\tLoss: 62.331\tL0 Loss: 0.541\n",
      "Valid Loss: 134.455, Recon Error: 0.017\n",
      "134.45465222680957\n",
      "Epoch: 3 Average loss: 69.80 Valid loss: 134.45465222680957\tRecon Error:0.017\n",
      "0/649600\tLoss: 75.226\tL0 Loss: 0.541\n",
      "3200/649600\tLoss: 53.564\tL0 Loss: 0.541\n",
      "6400/649600\tLoss: 48.982\tL0 Loss: 0.541\n",
      "9600/649600\tLoss: 46.592\tL0 Loss: 0.541\n",
      "12800/649600\tLoss: 46.925\tL0 Loss: 0.541\n",
      "16000/649600\tLoss: 47.595\tL0 Loss: 0.541\n",
      "19200/649600\tLoss: 48.244\tL0 Loss: 0.541\n",
      "22400/649600\tLoss: 48.919\tL0 Loss: 0.541\n",
      "25600/649600\tLoss: 47.767\tL0 Loss: 0.541\n",
      "28800/649600\tLoss: 49.450\tL0 Loss: 0.541\n",
      "32000/649600\tLoss: 48.305\tL0 Loss: 0.541\n",
      "35200/649600\tLoss: 47.704\tL0 Loss: 0.541\n",
      "38400/649600\tLoss: 47.676\tL0 Loss: 0.541\n",
      "41600/649600\tLoss: 50.501\tL0 Loss: 0.541\n",
      "44800/649600\tLoss: 57.484\tL0 Loss: 0.541\n",
      "48000/649600\tLoss: 52.744\tL0 Loss: 0.541\n",
      "51200/649600\tLoss: 55.405\tL0 Loss: 0.541\n",
      "54400/649600\tLoss: 50.555\tL0 Loss: 0.541\n",
      "57600/649600\tLoss: 53.593\tL0 Loss: 0.541\n",
      "60800/649600\tLoss: 52.756\tL0 Loss: 0.541\n",
      "64000/649600\tLoss: 52.542\tL0 Loss: 0.541\n",
      "67200/649600\tLoss: 51.547\tL0 Loss: 0.541\n",
      "70400/649600\tLoss: 53.203\tL0 Loss: 0.541\n",
      "73600/649600\tLoss: 55.266\tL0 Loss: 0.541\n",
      "76800/649600\tLoss: 55.665\tL0 Loss: 0.541\n",
      "80000/649600\tLoss: 54.948\tL0 Loss: 0.542\n",
      "83200/649600\tLoss: 57.696\tL0 Loss: 0.542\n",
      "86400/649600\tLoss: 64.334\tL0 Loss: 0.542\n",
      "89600/649600\tLoss: 61.655\tL0 Loss: 0.542\n",
      "92800/649600\tLoss: 64.485\tL0 Loss: 0.542\n",
      "96000/649600\tLoss: 60.410\tL0 Loss: 0.542\n",
      "99200/649600\tLoss: 63.144\tL0 Loss: 0.542\n",
      "102400/649600\tLoss: 61.319\tL0 Loss: 0.542\n",
      "105600/649600\tLoss: 59.429\tL0 Loss: 0.542\n",
      "108800/649600\tLoss: 58.823\tL0 Loss: 0.542\n",
      "112000/649600\tLoss: 61.935\tL0 Loss: 0.542\n",
      "115200/649600\tLoss: 62.621\tL0 Loss: 0.542\n",
      "118400/649600\tLoss: 59.499\tL0 Loss: 0.542\n",
      "121600/649600\tLoss: 64.334\tL0 Loss: 0.542\n",
      "124800/649600\tLoss: 76.958\tL0 Loss: 0.542\n",
      "128000/649600\tLoss: 75.573\tL0 Loss: 0.542\n",
      "131200/649600\tLoss: 68.006\tL0 Loss: 0.542\n",
      "134400/649600\tLoss: 68.359\tL0 Loss: 0.542\n",
      "137600/649600\tLoss: 69.032\tL0 Loss: 0.542\n",
      "140800/649600\tLoss: 71.658\tL0 Loss: 0.542\n",
      "144000/649600\tLoss: 74.907\tL0 Loss: 0.542\n",
      "147200/649600\tLoss: 70.356\tL0 Loss: 0.542\n",
      "150400/649600\tLoss: 70.352\tL0 Loss: 0.542\n",
      "153600/649600\tLoss: 71.222\tL0 Loss: 0.542\n",
      "156800/649600\tLoss: 69.527\tL0 Loss: 0.542\n",
      "160000/649600\tLoss: 66.408\tL0 Loss: 0.542\n",
      "163200/649600\tLoss: 69.006\tL0 Loss: 0.542\n",
      "166400/649600\tLoss: 79.337\tL0 Loss: 0.542\n",
      "169600/649600\tLoss: 75.652\tL0 Loss: 0.542\n",
      "172800/649600\tLoss: 80.150\tL0 Loss: 0.542\n",
      "176000/649600\tLoss: 75.015\tL0 Loss: 0.542\n",
      "179200/649600\tLoss: 90.192\tL0 Loss: 0.542\n",
      "182400/649600\tLoss: 78.229\tL0 Loss: 0.542\n",
      "185600/649600\tLoss: 84.660\tL0 Loss: 0.542\n",
      "188800/649600\tLoss: 78.553\tL0 Loss: 0.542\n",
      "192000/649600\tLoss: 78.459\tL0 Loss: 0.542\n",
      "195200/649600\tLoss: 79.960\tL0 Loss: 0.542\n",
      "198400/649600\tLoss: 81.671\tL0 Loss: 0.542\n",
      "201600/649600\tLoss: 86.993\tL0 Loss: 0.542\n",
      "204800/649600\tLoss: 81.217\tL0 Loss: 0.542\n",
      "208000/649600\tLoss: 93.462\tL0 Loss: 0.542\n",
      "211200/649600\tLoss: 91.653\tL0 Loss: 0.542\n",
      "214400/649600\tLoss: 96.862\tL0 Loss: 0.542\n",
      "217600/649600\tLoss: 93.567\tL0 Loss: 0.542\n",
      "220800/649600\tLoss: 93.298\tL0 Loss: 0.542\n",
      "224000/649600\tLoss: 87.871\tL0 Loss: 0.543\n",
      "227200/649600\tLoss: 85.615\tL0 Loss: 0.543\n",
      "230400/649600\tLoss: 86.397\tL0 Loss: 0.543\n",
      "233600/649600\tLoss: 90.504\tL0 Loss: 0.543\n",
      "236800/649600\tLoss: 85.530\tL0 Loss: 0.543\n",
      "240000/649600\tLoss: 90.654\tL0 Loss: 0.543\n",
      "243200/649600\tLoss: 92.360\tL0 Loss: 0.543\n",
      "246400/649600\tLoss: 110.880\tL0 Loss: 0.543\n",
      "249600/649600\tLoss: 96.163\tL0 Loss: 0.543\n",
      "252800/649600\tLoss: 89.684\tL0 Loss: 0.543\n",
      "256000/649600\tLoss: 80.500\tL0 Loss: 0.543\n",
      "259200/649600\tLoss: 68.809\tL0 Loss: 0.543\n",
      "262400/649600\tLoss: 57.800\tL0 Loss: 0.543\n",
      "265600/649600\tLoss: 54.520\tL0 Loss: 0.543\n",
      "268800/649600\tLoss: 53.309\tL0 Loss: 0.543\n",
      "272000/649600\tLoss: 52.084\tL0 Loss: 0.543\n",
      "275200/649600\tLoss: 52.165\tL0 Loss: 0.543\n",
      "278400/649600\tLoss: 51.032\tL0 Loss: 0.543\n",
      "281600/649600\tLoss: 50.488\tL0 Loss: 0.543\n",
      "284800/649600\tLoss: 49.360\tL0 Loss: 0.543\n",
      "288000/649600\tLoss: 53.649\tL0 Loss: 0.543\n",
      "291200/649600\tLoss: 56.764\tL0 Loss: 0.543\n",
      "294400/649600\tLoss: 55.272\tL0 Loss: 0.543\n",
      "297600/649600\tLoss: 55.911\tL0 Loss: 0.543\n",
      "300800/649600\tLoss: 57.133\tL0 Loss: 0.543\n",
      "304000/649600\tLoss: 56.296\tL0 Loss: 0.543\n",
      "307200/649600\tLoss: 58.143\tL0 Loss: 0.543\n",
      "310400/649600\tLoss: 56.242\tL0 Loss: 0.543\n",
      "313600/649600\tLoss: 57.868\tL0 Loss: 0.543\n",
      "316800/649600\tLoss: 57.166\tL0 Loss: 0.543\n",
      "320000/649600\tLoss: 56.034\tL0 Loss: 0.543\n",
      "323200/649600\tLoss: 57.716\tL0 Loss: 0.543\n",
      "326400/649600\tLoss: 57.376\tL0 Loss: 0.543\n",
      "329600/649600\tLoss: 64.169\tL0 Loss: 0.543\n",
      "332800/649600\tLoss: 63.266\tL0 Loss: 0.543\n",
      "336000/649600\tLoss: 66.061\tL0 Loss: 0.543\n",
      "339200/649600\tLoss: 68.060\tL0 Loss: 0.543\n",
      "342400/649600\tLoss: 66.209\tL0 Loss: 0.543\n",
      "345600/649600\tLoss: 64.819\tL0 Loss: 0.543\n",
      "348800/649600\tLoss: 65.135\tL0 Loss: 0.543\n",
      "352000/649600\tLoss: 64.028\tL0 Loss: 0.543\n",
      "355200/649600\tLoss: 65.488\tL0 Loss: 0.543\n",
      "358400/649600\tLoss: 66.576\tL0 Loss: 0.543\n",
      "361600/649600\tLoss: 63.166\tL0 Loss: 0.543\n",
      "364800/649600\tLoss: 66.387\tL0 Loss: 0.543\n",
      "368000/649600\tLoss: 65.583\tL0 Loss: 0.543\n",
      "371200/649600\tLoss: 68.232\tL0 Loss: 0.543\n",
      "374400/649600\tLoss: 72.476\tL0 Loss: 0.543\n",
      "377600/649600\tLoss: 73.804\tL0 Loss: 0.543\n",
      "380800/649600\tLoss: 76.675\tL0 Loss: 0.543\n",
      "384000/649600\tLoss: 79.399\tL0 Loss: 0.543\n",
      "387200/649600\tLoss: 74.354\tL0 Loss: 0.543\n",
      "390400/649600\tLoss: 72.567\tL0 Loss: 0.543\n",
      "393600/649600\tLoss: 70.833\tL0 Loss: 0.543\n",
      "396800/649600\tLoss: 77.149\tL0 Loss: 0.543\n",
      "400000/649600\tLoss: 73.782\tL0 Loss: 0.543\n",
      "403200/649600\tLoss: 75.318\tL0 Loss: 0.543\n",
      "406400/649600\tLoss: 74.298\tL0 Loss: 0.543\n",
      "409600/649600\tLoss: 73.262\tL0 Loss: 0.543\n",
      "412800/649600\tLoss: 77.901\tL0 Loss: 0.543\n",
      "416000/649600\tLoss: 86.074\tL0 Loss: 0.543\n",
      "419200/649600\tLoss: 87.582\tL0 Loss: 0.543\n",
      "422400/649600\tLoss: 88.282\tL0 Loss: 0.543\n",
      "425600/649600\tLoss: 88.232\tL0 Loss: 0.543\n",
      "428800/649600\tLoss: 87.167\tL0 Loss: 0.543\n",
      "432000/649600\tLoss: 76.083\tL0 Loss: 0.543\n",
      "435200/649600\tLoss: 85.561\tL0 Loss: 0.543\n",
      "438400/649600\tLoss: 88.024\tL0 Loss: 0.543\n",
      "441600/649600\tLoss: 83.377\tL0 Loss: 0.544\n",
      "444800/649600\tLoss: 85.615\tL0 Loss: 0.544\n",
      "448000/649600\tLoss: 86.525\tL0 Loss: 0.544\n",
      "451200/649600\tLoss: 81.042\tL0 Loss: 0.544\n",
      "454400/649600\tLoss: 84.351\tL0 Loss: 0.544\n",
      "457600/649600\tLoss: 97.939\tL0 Loss: 0.544\n",
      "460800/649600\tLoss: 106.755\tL0 Loss: 0.544\n",
      "464000/649600\tLoss: 104.106\tL0 Loss: 0.544\n",
      "467200/649600\tLoss: 104.486\tL0 Loss: 0.544\n",
      "470400/649600\tLoss: 100.108\tL0 Loss: 0.544\n",
      "473600/649600\tLoss: 89.929\tL0 Loss: 0.544\n",
      "476800/649600\tLoss: 96.548\tL0 Loss: 0.544\n",
      "480000/649600\tLoss: 101.471\tL0 Loss: 0.544\n",
      "483200/649600\tLoss: 98.454\tL0 Loss: 0.544\n",
      "486400/649600\tLoss: 100.452\tL0 Loss: 0.544\n",
      "489600/649600\tLoss: 100.516\tL0 Loss: 0.544\n",
      "492800/649600\tLoss: 97.025\tL0 Loss: 0.544\n",
      "496000/649600\tLoss: 69.189\tL0 Loss: 0.544\n",
      "499200/649600\tLoss: 55.107\tL0 Loss: 0.544\n",
      "502400/649600\tLoss: 49.568\tL0 Loss: 0.544\n",
      "505600/649600\tLoss: 48.460\tL0 Loss: 0.544\n",
      "508800/649600\tLoss: 46.053\tL0 Loss: 0.544\n",
      "512000/649600\tLoss: 44.692\tL0 Loss: 0.544\n",
      "515200/649600\tLoss: 43.908\tL0 Loss: 0.544\n",
      "518400/649600\tLoss: 45.511\tL0 Loss: 0.544\n",
      "521600/649600\tLoss: 43.596\tL0 Loss: 0.544\n",
      "524800/649600\tLoss: 44.940\tL0 Loss: 0.544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528000/649600\tLoss: 43.544\tL0 Loss: 0.544\n",
      "531200/649600\tLoss: 42.838\tL0 Loss: 0.544\n",
      "534400/649600\tLoss: 47.526\tL0 Loss: 0.544\n",
      "537600/649600\tLoss: 48.269\tL0 Loss: 0.544\n",
      "540800/649600\tLoss: 48.678\tL0 Loss: 0.544\n",
      "544000/649600\tLoss: 47.904\tL0 Loss: 0.544\n",
      "547200/649600\tLoss: 48.548\tL0 Loss: 0.544\n",
      "550400/649600\tLoss: 47.919\tL0 Loss: 0.544\n",
      "553600/649600\tLoss: 48.367\tL0 Loss: 0.544\n",
      "556800/649600\tLoss: 46.844\tL0 Loss: 0.544\n",
      "560000/649600\tLoss: 46.478\tL0 Loss: 0.544\n",
      "563200/649600\tLoss: 47.948\tL0 Loss: 0.544\n",
      "566400/649600\tLoss: 47.268\tL0 Loss: 0.544\n",
      "569600/649600\tLoss: 48.644\tL0 Loss: 0.544\n",
      "572800/649600\tLoss: 49.039\tL0 Loss: 0.544\n",
      "576000/649600\tLoss: 56.515\tL0 Loss: 0.544\n",
      "579200/649600\tLoss: 53.064\tL0 Loss: 0.544\n",
      "582400/649600\tLoss: 53.539\tL0 Loss: 0.544\n",
      "585600/649600\tLoss: 53.232\tL0 Loss: 0.544\n",
      "588800/649600\tLoss: 52.303\tL0 Loss: 0.544\n",
      "592000/649600\tLoss: 53.951\tL0 Loss: 0.544\n",
      "595200/649600\tLoss: 53.439\tL0 Loss: 0.544\n",
      "598400/649600\tLoss: 54.568\tL0 Loss: 0.544\n",
      "601600/649600\tLoss: 53.360\tL0 Loss: 0.544\n",
      "604800/649600\tLoss: 53.189\tL0 Loss: 0.544\n",
      "608000/649600\tLoss: 53.993\tL0 Loss: 0.544\n",
      "611200/649600\tLoss: 54.207\tL0 Loss: 0.544\n",
      "614400/649600\tLoss: 54.158\tL0 Loss: 0.544\n",
      "617600/649600\tLoss: 59.521\tL0 Loss: 0.544\n",
      "620800/649600\tLoss: 59.916\tL0 Loss: 0.544\n",
      "624000/649600\tLoss: 62.461\tL0 Loss: 0.544\n",
      "627200/649600\tLoss: 61.049\tL0 Loss: 0.544\n",
      "630400/649600\tLoss: 61.431\tL0 Loss: 0.545\n",
      "633600/649600\tLoss: 59.925\tL0 Loss: 0.545\n",
      "636800/649600\tLoss: 58.792\tL0 Loss: 0.545\n",
      "640000/649600\tLoss: 60.260\tL0 Loss: 0.545\n",
      "643200/649600\tLoss: 61.232\tL0 Loss: 0.545\n",
      "646400/649600\tLoss: 59.151\tL0 Loss: 0.545\n",
      "Valid Loss: 146.201, Recon Error: 0.023\n",
      "146.20116599918842\n",
      "Epoch: 4 Average loss: 66.50 Valid loss: 146.20116599918842\tRecon Error:0.023\n",
      "0/649600\tLoss: 61.444\tL0 Loss: 0.545\n",
      "3200/649600\tLoss: 51.486\tL0 Loss: 0.545\n",
      "6400/649600\tLoss: 48.008\tL0 Loss: 0.545\n",
      "9600/649600\tLoss: 45.713\tL0 Loss: 0.545\n",
      "12800/649600\tLoss: 46.742\tL0 Loss: 0.545\n",
      "16000/649600\tLoss: 44.583\tL0 Loss: 0.545\n",
      "19200/649600\tLoss: 45.875\tL0 Loss: 0.545\n",
      "22400/649600\tLoss: 45.218\tL0 Loss: 0.545\n",
      "25600/649600\tLoss: 44.832\tL0 Loss: 0.545\n",
      "28800/649600\tLoss: 44.078\tL0 Loss: 0.545\n",
      "32000/649600\tLoss: 45.157\tL0 Loss: 0.545\n",
      "35200/649600\tLoss: 44.241\tL0 Loss: 0.545\n",
      "38400/649600\tLoss: 45.277\tL0 Loss: 0.545\n",
      "41600/649600\tLoss: 48.535\tL0 Loss: 0.545\n",
      "44800/649600\tLoss: 56.628\tL0 Loss: 0.545\n",
      "48000/649600\tLoss: 49.111\tL0 Loss: 0.545\n",
      "51200/649600\tLoss: 50.774\tL0 Loss: 0.545\n",
      "54400/649600\tLoss: 54.083\tL0 Loss: 0.545\n",
      "57600/649600\tLoss: 50.217\tL0 Loss: 0.545\n",
      "60800/649600\tLoss: 50.364\tL0 Loss: 0.545\n",
      "64000/649600\tLoss: 49.036\tL0 Loss: 0.545\n",
      "67200/649600\tLoss: 51.013\tL0 Loss: 0.545\n",
      "70400/649600\tLoss: 49.499\tL0 Loss: 0.545\n",
      "73600/649600\tLoss: 49.862\tL0 Loss: 0.545\n",
      "76800/649600\tLoss: 50.380\tL0 Loss: 0.545\n",
      "80000/649600\tLoss: 50.278\tL0 Loss: 0.545\n",
      "83200/649600\tLoss: 59.743\tL0 Loss: 0.545\n",
      "86400/649600\tLoss: 57.430\tL0 Loss: 0.545\n",
      "89600/649600\tLoss: 58.011\tL0 Loss: 0.545\n",
      "92800/649600\tLoss: 57.441\tL0 Loss: 0.545\n",
      "96000/649600\tLoss: 56.534\tL0 Loss: 0.545\n",
      "99200/649600\tLoss: 58.323\tL0 Loss: 0.545\n",
      "102400/649600\tLoss: 57.576\tL0 Loss: 0.545\n",
      "105600/649600\tLoss: 57.669\tL0 Loss: 0.545\n",
      "108800/649600\tLoss: 59.578\tL0 Loss: 0.545\n",
      "112000/649600\tLoss: 63.280\tL0 Loss: 0.545\n",
      "115200/649600\tLoss: 60.009\tL0 Loss: 0.545\n",
      "118400/649600\tLoss: 58.300\tL0 Loss: 0.545\n",
      "121600/649600\tLoss: 57.456\tL0 Loss: 0.545\n",
      "124800/649600\tLoss: 67.296\tL0 Loss: 0.545\n",
      "128000/649600\tLoss: 67.296\tL0 Loss: 0.545\n",
      "131200/649600\tLoss: 70.045\tL0 Loss: 0.545\n",
      "134400/649600\tLoss: 66.568\tL0 Loss: 0.545\n",
      "137600/649600\tLoss: 65.433\tL0 Loss: 0.545\n",
      "140800/649600\tLoss: 66.245\tL0 Loss: 0.546\n",
      "144000/649600\tLoss: 63.816\tL0 Loss: 0.546\n",
      "147200/649600\tLoss: 65.106\tL0 Loss: 0.546\n",
      "150400/649600\tLoss: 64.299\tL0 Loss: 0.546\n",
      "153600/649600\tLoss: 63.885\tL0 Loss: 0.546\n",
      "156800/649600\tLoss: 65.221\tL0 Loss: 0.546\n",
      "160000/649600\tLoss: 65.105\tL0 Loss: 0.546\n",
      "163200/649600\tLoss: 64.385\tL0 Loss: 0.546\n",
      "166400/649600\tLoss: 72.692\tL0 Loss: 0.546\n",
      "169600/649600\tLoss: 78.491\tL0 Loss: 0.546\n",
      "172800/649600\tLoss: 74.936\tL0 Loss: 0.546\n",
      "176000/649600\tLoss: 82.174\tL0 Loss: 0.546\n",
      "179200/649600\tLoss: 79.899\tL0 Loss: 0.546\n",
      "182400/649600\tLoss: 74.334\tL0 Loss: 0.546\n",
      "185600/649600\tLoss: 73.138\tL0 Loss: 0.546\n",
      "188800/649600\tLoss: 76.449\tL0 Loss: 0.546\n",
      "192000/649600\tLoss: 77.712\tL0 Loss: 0.546\n",
      "195200/649600\tLoss: 72.547\tL0 Loss: 0.546\n",
      "198400/649600\tLoss: 74.981\tL0 Loss: 0.546\n",
      "201600/649600\tLoss: 75.703\tL0 Loss: 0.546\n",
      "204800/649600\tLoss: 76.547\tL0 Loss: 0.546\n",
      "208000/649600\tLoss: 87.558\tL0 Loss: 0.546\n",
      "211200/649600\tLoss: 85.094\tL0 Loss: 0.546\n",
      "214400/649600\tLoss: 81.616\tL0 Loss: 0.546\n",
      "217600/649600\tLoss: 85.125\tL0 Loss: 0.546\n",
      "220800/649600\tLoss: 86.413\tL0 Loss: 0.546\n",
      "224000/649600\tLoss: 83.045\tL0 Loss: 0.546\n",
      "227200/649600\tLoss: 85.154\tL0 Loss: 0.546\n",
      "230400/649600\tLoss: 80.829\tL0 Loss: 0.546\n",
      "233600/649600\tLoss: 84.848\tL0 Loss: 0.546\n",
      "236800/649600\tLoss: 82.296\tL0 Loss: 0.546\n",
      "240000/649600\tLoss: 82.605\tL0 Loss: 0.546\n",
      "243200/649600\tLoss: 86.637\tL0 Loss: 0.546\n",
      "246400/649600\tLoss: 101.665\tL0 Loss: 0.546\n",
      "249600/649600\tLoss: 86.839\tL0 Loss: 0.546\n",
      "252800/649600\tLoss: 60.668\tL0 Loss: 0.546\n",
      "256000/649600\tLoss: 51.563\tL0 Loss: 0.546\n",
      "259200/649600\tLoss: 50.059\tL0 Loss: 0.546\n",
      "262400/649600\tLoss: 50.196\tL0 Loss: 0.546\n",
      "265600/649600\tLoss: 49.352\tL0 Loss: 0.546\n",
      "268800/649600\tLoss: 48.470\tL0 Loss: 0.546\n",
      "272000/649600\tLoss: 47.895\tL0 Loss: 0.546\n",
      "275200/649600\tLoss: 48.008\tL0 Loss: 0.546\n",
      "278400/649600\tLoss: 49.166\tL0 Loss: 0.546\n",
      "281600/649600\tLoss: 47.855\tL0 Loss: 0.546\n",
      "284800/649600\tLoss: 48.179\tL0 Loss: 0.546\n",
      "288000/649600\tLoss: 52.375\tL0 Loss: 0.546\n",
      "291200/649600\tLoss: 54.259\tL0 Loss: 0.546\n",
      "294400/649600\tLoss: 52.692\tL0 Loss: 0.546\n",
      "297600/649600\tLoss: 52.527\tL0 Loss: 0.546\n",
      "300800/649600\tLoss: 54.182\tL0 Loss: 0.546\n",
      "304000/649600\tLoss: 54.782\tL0 Loss: 0.546\n",
      "307200/649600\tLoss: 54.375\tL0 Loss: 0.546\n",
      "310400/649600\tLoss: 52.743\tL0 Loss: 0.546\n",
      "313600/649600\tLoss: 52.341\tL0 Loss: 0.546\n",
      "316800/649600\tLoss: 52.591\tL0 Loss: 0.546\n",
      "320000/649600\tLoss: 54.514\tL0 Loss: 0.546\n",
      "323200/649600\tLoss: 54.973\tL0 Loss: 0.546\n",
      "326400/649600\tLoss: 53.630\tL0 Loss: 0.546\n",
      "329600/649600\tLoss: 59.657\tL0 Loss: 0.546\n",
      "332800/649600\tLoss: 59.380\tL0 Loss: 0.547\n",
      "336000/649600\tLoss: 62.199\tL0 Loss: 0.547\n",
      "339200/649600\tLoss: 62.919\tL0 Loss: 0.547\n",
      "342400/649600\tLoss: 65.612\tL0 Loss: 0.547\n",
      "345600/649600\tLoss: 62.984\tL0 Loss: 0.547\n",
      "348800/649600\tLoss: 62.138\tL0 Loss: 0.547\n",
      "352000/649600\tLoss: 61.216\tL0 Loss: 0.547\n",
      "355200/649600\tLoss: 61.316\tL0 Loss: 0.547\n",
      "358400/649600\tLoss: 63.344\tL0 Loss: 0.547\n",
      "361600/649600\tLoss: 62.582\tL0 Loss: 0.547\n",
      "364800/649600\tLoss: 61.891\tL0 Loss: 0.547\n",
      "368000/649600\tLoss: 62.913\tL0 Loss: 0.547\n",
      "371200/649600\tLoss: 66.685\tL0 Loss: 0.547\n",
      "374400/649600\tLoss: 67.515\tL0 Loss: 0.547\n",
      "377600/649600\tLoss: 72.231\tL0 Loss: 0.547\n",
      "380800/649600\tLoss: 72.305\tL0 Loss: 0.547\n",
      "384000/649600\tLoss: 73.555\tL0 Loss: 0.547\n",
      "387200/649600\tLoss: 72.010\tL0 Loss: 0.547\n",
      "390400/649600\tLoss: 71.770\tL0 Loss: 0.547\n",
      "393600/649600\tLoss: 70.125\tL0 Loss: 0.547\n",
      "396800/649600\tLoss: 77.832\tL0 Loss: 0.547\n",
      "400000/649600\tLoss: 73.425\tL0 Loss: 0.547\n",
      "403200/649600\tLoss: 71.349\tL0 Loss: 0.547\n",
      "406400/649600\tLoss: 72.553\tL0 Loss: 0.547\n",
      "409600/649600\tLoss: 75.270\tL0 Loss: 0.547\n",
      "412800/649600\tLoss: 76.222\tL0 Loss: 0.547\n",
      "416000/649600\tLoss: 79.776\tL0 Loss: 0.547\n",
      "419200/649600\tLoss: 82.432\tL0 Loss: 0.547\n",
      "422400/649600\tLoss: 80.223\tL0 Loss: 0.547\n",
      "425600/649600\tLoss: 84.605\tL0 Loss: 0.547\n",
      "428800/649600\tLoss: 81.350\tL0 Loss: 0.547\n",
      "432000/649600\tLoss: 78.289\tL0 Loss: 0.547\n",
      "435200/649600\tLoss: 81.390\tL0 Loss: 0.547\n",
      "438400/649600\tLoss: 85.614\tL0 Loss: 0.547\n",
      "441600/649600\tLoss: 83.338\tL0 Loss: 0.547\n",
      "444800/649600\tLoss: 83.785\tL0 Loss: 0.547\n",
      "448000/649600\tLoss: 81.396\tL0 Loss: 0.547\n",
      "451200/649600\tLoss: 81.918\tL0 Loss: 0.547\n",
      "454400/649600\tLoss: 86.729\tL0 Loss: 0.547\n",
      "457600/649600\tLoss: 93.941\tL0 Loss: 0.547\n",
      "460800/649600\tLoss: 97.023\tL0 Loss: 0.547\n",
      "464000/649600\tLoss: 97.969\tL0 Loss: 0.547\n",
      "467200/649600\tLoss: 104.856\tL0 Loss: 0.547\n",
      "470400/649600\tLoss: 94.867\tL0 Loss: 0.547\n",
      "473600/649600\tLoss: 90.978\tL0 Loss: 0.547\n",
      "476800/649600\tLoss: 102.898\tL0 Loss: 0.547\n",
      "480000/649600\tLoss: 99.757\tL0 Loss: 0.547\n",
      "483200/649600\tLoss: 93.764\tL0 Loss: 0.547\n",
      "486400/649600\tLoss: 93.438\tL0 Loss: 0.547\n",
      "489600/649600\tLoss: 96.621\tL0 Loss: 0.547\n",
      "492800/649600\tLoss: 101.269\tL0 Loss: 0.547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496000/649600\tLoss: 76.887\tL0 Loss: 0.547\n",
      "499200/649600\tLoss: 70.523\tL0 Loss: 0.547\n",
      "502400/649600\tLoss: 61.608\tL0 Loss: 0.547\n",
      "505600/649600\tLoss: 52.681\tL0 Loss: 0.547\n",
      "508800/649600\tLoss: 48.320\tL0 Loss: 0.547\n",
      "512000/649600\tLoss: 45.493\tL0 Loss: 0.547\n",
      "515200/649600\tLoss: 44.787\tL0 Loss: 0.547\n",
      "518400/649600\tLoss: 43.662\tL0 Loss: 0.547\n",
      "521600/649600\tLoss: 43.709\tL0 Loss: 0.547\n",
      "524800/649600\tLoss: 42.550\tL0 Loss: 0.547\n",
      "528000/649600\tLoss: 42.586\tL0 Loss: 0.547\n",
      "531200/649600\tLoss: 42.043\tL0 Loss: 0.547\n",
      "534400/649600\tLoss: 46.255\tL0 Loss: 0.547\n",
      "537600/649600\tLoss: 46.789\tL0 Loss: 0.547\n",
      "540800/649600\tLoss: 46.731\tL0 Loss: 0.547\n",
      "544000/649600\tLoss: 47.243\tL0 Loss: 0.547\n",
      "547200/649600\tLoss: 48.165\tL0 Loss: 0.547\n",
      "550400/649600\tLoss: 46.255\tL0 Loss: 0.548\n",
      "553600/649600\tLoss: 45.636\tL0 Loss: 0.548\n",
      "556800/649600\tLoss: 44.667\tL0 Loss: 0.548\n",
      "560000/649600\tLoss: 46.547\tL0 Loss: 0.548\n",
      "563200/649600\tLoss: 46.427\tL0 Loss: 0.548\n",
      "566400/649600\tLoss: 47.229\tL0 Loss: 0.548\n",
      "569600/649600\tLoss: 46.478\tL0 Loss: 0.548\n",
      "572800/649600\tLoss: 45.882\tL0 Loss: 0.548\n",
      "576000/649600\tLoss: 54.298\tL0 Loss: 0.548\n",
      "579200/649600\tLoss: 52.380\tL0 Loss: 0.548\n",
      "582400/649600\tLoss: 50.915\tL0 Loss: 0.548\n",
      "585600/649600\tLoss: 50.969\tL0 Loss: 0.548\n",
      "588800/649600\tLoss: 52.893\tL0 Loss: 0.548\n",
      "592000/649600\tLoss: 52.327\tL0 Loss: 0.548\n",
      "595200/649600\tLoss: 50.242\tL0 Loss: 0.548\n",
      "598400/649600\tLoss: 52.730\tL0 Loss: 0.548\n",
      "601600/649600\tLoss: 50.396\tL0 Loss: 0.548\n",
      "604800/649600\tLoss: 50.460\tL0 Loss: 0.548\n",
      "608000/649600\tLoss: 51.596\tL0 Loss: 0.548\n",
      "611200/649600\tLoss: 52.589\tL0 Loss: 0.548\n",
      "614400/649600\tLoss: 51.157\tL0 Loss: 0.548\n",
      "617600/649600\tLoss: 59.538\tL0 Loss: 0.548\n",
      "620800/649600\tLoss: 59.584\tL0 Loss: 0.548\n",
      "624000/649600\tLoss: 61.286\tL0 Loss: 0.548\n",
      "627200/649600\tLoss: 60.512\tL0 Loss: 0.548\n",
      "630400/649600\tLoss: 61.225\tL0 Loss: 0.548\n",
      "633600/649600\tLoss: 57.345\tL0 Loss: 0.548\n",
      "636800/649600\tLoss: 57.656\tL0 Loss: 0.548\n",
      "640000/649600\tLoss: 59.108\tL0 Loss: 0.548\n",
      "643200/649600\tLoss: 59.282\tL0 Loss: 0.548\n",
      "646400/649600\tLoss: 59.854\tL0 Loss: 0.548\n",
      "Valid Loss: 136.754, Recon Error: 0.018\n",
      "136.75372344551937\n",
      "Epoch: 5 Average loss: 63.40 Valid loss: 136.75372344551937\tRecon Error:0.018\n",
      "0/649600\tLoss: 61.625\tL0 Loss: 0.548\n",
      "3200/649600\tLoss: 51.646\tL0 Loss: 0.548\n",
      "6400/649600\tLoss: 43.331\tL0 Loss: 0.548\n",
      "9600/649600\tLoss: 44.613\tL0 Loss: 0.548\n",
      "12800/649600\tLoss: 42.084\tL0 Loss: 0.548\n",
      "16000/649600\tLoss: 43.681\tL0 Loss: 0.548\n",
      "19200/649600\tLoss: 44.344\tL0 Loss: 0.548\n",
      "22400/649600\tLoss: 43.989\tL0 Loss: 0.548\n",
      "25600/649600\tLoss: 43.114\tL0 Loss: 0.548\n",
      "28800/649600\tLoss: 43.073\tL0 Loss: 0.548\n",
      "32000/649600\tLoss: 42.818\tL0 Loss: 0.548\n",
      "35200/649600\tLoss: 44.587\tL0 Loss: 0.548\n",
      "38400/649600\tLoss: 42.459\tL0 Loss: 0.548\n",
      "41600/649600\tLoss: 46.547\tL0 Loss: 0.548\n",
      "44800/649600\tLoss: 52.219\tL0 Loss: 0.548\n",
      "48000/649600\tLoss: 50.682\tL0 Loss: 0.548\n",
      "51200/649600\tLoss: 48.632\tL0 Loss: 0.548\n",
      "54400/649600\tLoss: 48.793\tL0 Loss: 0.548\n",
      "57600/649600\tLoss: 50.200\tL0 Loss: 0.548\n",
      "60800/649600\tLoss: 47.204\tL0 Loss: 0.548\n",
      "64000/649600\tLoss: 48.526\tL0 Loss: 0.548\n",
      "67200/649600\tLoss: 49.107\tL0 Loss: 0.548\n",
      "70400/649600\tLoss: 50.083\tL0 Loss: 0.548\n",
      "73600/649600\tLoss: 47.450\tL0 Loss: 0.548\n",
      "76800/649600\tLoss: 49.805\tL0 Loss: 0.548\n",
      "80000/649600\tLoss: 49.066\tL0 Loss: 0.548\n",
      "83200/649600\tLoss: 55.117\tL0 Loss: 0.548\n",
      "86400/649600\tLoss: 59.066\tL0 Loss: 0.548\n",
      "89600/649600\tLoss: 56.535\tL0 Loss: 0.548\n",
      "92800/649600\tLoss: 55.487\tL0 Loss: 0.549\n",
      "96000/649600\tLoss: 56.677\tL0 Loss: 0.549\n",
      "99200/649600\tLoss: 55.449\tL0 Loss: 0.549\n",
      "102400/649600\tLoss: 58.147\tL0 Loss: 0.549\n",
      "105600/649600\tLoss: 54.741\tL0 Loss: 0.549\n",
      "108800/649600\tLoss: 55.078\tL0 Loss: 0.549\n",
      "112000/649600\tLoss: 61.277\tL0 Loss: 0.549\n",
      "115200/649600\tLoss: 56.769\tL0 Loss: 0.549\n",
      "118400/649600\tLoss: 58.620\tL0 Loss: 0.549\n",
      "121600/649600\tLoss: 58.245\tL0 Loss: 0.549\n",
      "124800/649600\tLoss: 62.740\tL0 Loss: 0.549\n",
      "128000/649600\tLoss: 63.196\tL0 Loss: 0.549\n",
      "131200/649600\tLoss: 62.415\tL0 Loss: 0.549\n",
      "134400/649600\tLoss: 66.526\tL0 Loss: 0.549\n",
      "137600/649600\tLoss: 64.758\tL0 Loss: 0.549\n",
      "140800/649600\tLoss: 64.560\tL0 Loss: 0.549\n",
      "144000/649600\tLoss: 63.220\tL0 Loss: 0.549\n",
      "147200/649600\tLoss: 65.151\tL0 Loss: 0.549\n",
      "150400/649600\tLoss: 65.774\tL0 Loss: 0.549\n",
      "153600/649600\tLoss: 62.846\tL0 Loss: 0.549\n",
      "156800/649600\tLoss: 64.392\tL0 Loss: 0.549\n",
      "160000/649600\tLoss: 63.808\tL0 Loss: 0.549\n",
      "163200/649600\tLoss: 64.983\tL0 Loss: 0.549\n",
      "166400/649600\tLoss: 71.958\tL0 Loss: 0.549\n",
      "169600/649600\tLoss: 69.563\tL0 Loss: 0.549\n",
      "172800/649600\tLoss: 78.065\tL0 Loss: 0.549\n",
      "176000/649600\tLoss: 72.818\tL0 Loss: 0.549\n",
      "179200/649600\tLoss: 69.927\tL0 Loss: 0.549\n",
      "182400/649600\tLoss: 72.945\tL0 Loss: 0.549\n",
      "185600/649600\tLoss: 76.678\tL0 Loss: 0.549\n",
      "188800/649600\tLoss: 70.753\tL0 Loss: 0.549\n",
      "192000/649600\tLoss: 72.769\tL0 Loss: 0.549\n",
      "195200/649600\tLoss: 70.477\tL0 Loss: 0.549\n",
      "198400/649600\tLoss: 69.443\tL0 Loss: 0.549\n",
      "201600/649600\tLoss: 79.400\tL0 Loss: 0.549\n",
      "204800/649600\tLoss: 72.846\tL0 Loss: 0.549\n",
      "208000/649600\tLoss: 86.352\tL0 Loss: 0.549\n",
      "211200/649600\tLoss: 82.534\tL0 Loss: 0.549\n",
      "214400/649600\tLoss: 80.889\tL0 Loss: 0.549\n",
      "217600/649600\tLoss: 79.101\tL0 Loss: 0.549\n",
      "220800/649600\tLoss: 81.517\tL0 Loss: 0.549\n",
      "224000/649600\tLoss: 79.412\tL0 Loss: 0.549\n",
      "227200/649600\tLoss: 78.612\tL0 Loss: 0.549\n",
      "230400/649600\tLoss: 80.173\tL0 Loss: 0.549\n",
      "233600/649600\tLoss: 81.597\tL0 Loss: 0.549\n",
      "236800/649600\tLoss: 78.677\tL0 Loss: 0.549\n",
      "240000/649600\tLoss: 79.927\tL0 Loss: 0.549\n",
      "243200/649600\tLoss: 83.105\tL0 Loss: 0.549\n",
      "246400/649600\tLoss: 100.793\tL0 Loss: 0.549\n",
      "249600/649600\tLoss: 85.404\tL0 Loss: 0.549\n",
      "252800/649600\tLoss: 61.935\tL0 Loss: 0.549\n",
      "256000/649600\tLoss: 53.310\tL0 Loss: 0.549\n",
      "259200/649600\tLoss: 50.325\tL0 Loss: 0.549\n",
      "262400/649600\tLoss: 48.012\tL0 Loss: 0.550\n",
      "265600/649600\tLoss: 47.081\tL0 Loss: 0.550\n",
      "268800/649600\tLoss: 47.595\tL0 Loss: 0.550\n",
      "272000/649600\tLoss: 45.088\tL0 Loss: 0.550\n",
      "275200/649600\tLoss: 48.645\tL0 Loss: 0.550\n",
      "278400/649600\tLoss: 46.397\tL0 Loss: 0.550\n",
      "281600/649600\tLoss: 47.140\tL0 Loss: 0.550\n",
      "284800/649600\tLoss: 46.345\tL0 Loss: 0.550\n",
      "288000/649600\tLoss: 50.146\tL0 Loss: 0.550\n",
      "291200/649600\tLoss: 51.420\tL0 Loss: 0.550\n",
      "294400/649600\tLoss: 53.121\tL0 Loss: 0.550\n",
      "297600/649600\tLoss: 54.842\tL0 Loss: 0.550\n",
      "300800/649600\tLoss: 55.126\tL0 Loss: 0.550\n",
      "304000/649600\tLoss: 54.105\tL0 Loss: 0.550\n",
      "307200/649600\tLoss: 51.793\tL0 Loss: 0.550\n",
      "310400/649600\tLoss: 51.290\tL0 Loss: 0.550\n",
      "313600/649600\tLoss: 54.606\tL0 Loss: 0.550\n",
      "316800/649600\tLoss: 55.904\tL0 Loss: 0.550\n",
      "320000/649600\tLoss: 55.362\tL0 Loss: 0.550\n",
      "323200/649600\tLoss: 55.848\tL0 Loss: 0.550\n",
      "326400/649600\tLoss: 54.072\tL0 Loss: 0.550\n",
      "329600/649600\tLoss: 57.842\tL0 Loss: 0.550\n",
      "332800/649600\tLoss: 58.752\tL0 Loss: 0.550\n",
      "336000/649600\tLoss: 62.014\tL0 Loss: 0.550\n",
      "339200/649600\tLoss: 64.011\tL0 Loss: 0.550\n",
      "342400/649600\tLoss: 64.442\tL0 Loss: 0.550\n",
      "345600/649600\tLoss: 65.554\tL0 Loss: 0.550\n",
      "348800/649600\tLoss: 61.599\tL0 Loss: 0.550\n",
      "352000/649600\tLoss: 60.720\tL0 Loss: 0.550\n",
      "355200/649600\tLoss: 61.647\tL0 Loss: 0.550\n",
      "358400/649600\tLoss: 62.514\tL0 Loss: 0.550\n",
      "361600/649600\tLoss: 61.574\tL0 Loss: 0.550\n",
      "364800/649600\tLoss: 61.160\tL0 Loss: 0.550\n",
      "368000/649600\tLoss: 60.726\tL0 Loss: 0.550\n",
      "371200/649600\tLoss: 67.121\tL0 Loss: 0.550\n",
      "374400/649600\tLoss: 68.020\tL0 Loss: 0.550\n",
      "377600/649600\tLoss: 73.299\tL0 Loss: 0.550\n",
      "380800/649600\tLoss: 69.038\tL0 Loss: 0.550\n",
      "384000/649600\tLoss: 72.362\tL0 Loss: 0.550\n",
      "387200/649600\tLoss: 70.776\tL0 Loss: 0.550\n",
      "390400/649600\tLoss: 70.942\tL0 Loss: 0.550\n",
      "393600/649600\tLoss: 69.433\tL0 Loss: 0.550\n",
      "396800/649600\tLoss: 72.605\tL0 Loss: 0.550\n",
      "400000/649600\tLoss: 70.330\tL0 Loss: 0.550\n",
      "403200/649600\tLoss: 73.265\tL0 Loss: 0.550\n",
      "406400/649600\tLoss: 72.707\tL0 Loss: 0.550\n",
      "409600/649600\tLoss: 70.159\tL0 Loss: 0.550\n",
      "412800/649600\tLoss: 77.540\tL0 Loss: 0.550\n",
      "416000/649600\tLoss: 80.573\tL0 Loss: 0.550\n",
      "419200/649600\tLoss: 80.629\tL0 Loss: 0.550\n",
      "422400/649600\tLoss: 79.935\tL0 Loss: 0.550\n",
      "425600/649600\tLoss: 82.022\tL0 Loss: 0.550\n",
      "428800/649600\tLoss: 81.420\tL0 Loss: 0.550\n",
      "432000/649600\tLoss: 80.518\tL0 Loss: 0.550\n",
      "435200/649600\tLoss: 81.182\tL0 Loss: 0.550\n",
      "438400/649600\tLoss: 83.493\tL0 Loss: 0.550\n",
      "441600/649600\tLoss: 80.722\tL0 Loss: 0.550\n",
      "444800/649600\tLoss: 80.373\tL0 Loss: 0.550\n",
      "448000/649600\tLoss: 81.930\tL0 Loss: 0.550\n",
      "451200/649600\tLoss: 80.810\tL0 Loss: 0.550\n",
      "454400/649600\tLoss: 83.891\tL0 Loss: 0.550\n",
      "457600/649600\tLoss: 95.301\tL0 Loss: 0.550\n",
      "460800/649600\tLoss: 93.165\tL0 Loss: 0.550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464000/649600\tLoss: 89.796\tL0 Loss: 0.550\n",
      "467200/649600\tLoss: 96.774\tL0 Loss: 0.550\n",
      "470400/649600\tLoss: 91.069\tL0 Loss: 0.550\n",
      "473600/649600\tLoss: 83.234\tL0 Loss: 0.551\n",
      "476800/649600\tLoss: 95.331\tL0 Loss: 0.551\n",
      "480000/649600\tLoss: 92.417\tL0 Loss: 0.551\n",
      "483200/649600\tLoss: 90.669\tL0 Loss: 0.551\n",
      "486400/649600\tLoss: 95.008\tL0 Loss: 0.551\n",
      "489600/649600\tLoss: 93.724\tL0 Loss: 0.551\n",
      "492800/649600\tLoss: 92.733\tL0 Loss: 0.551\n",
      "496000/649600\tLoss: 74.320\tL0 Loss: 0.551\n",
      "499200/649600\tLoss: 66.190\tL0 Loss: 0.551\n",
      "502400/649600\tLoss: 57.520\tL0 Loss: 0.551\n",
      "505600/649600\tLoss: 49.881\tL0 Loss: 0.551\n",
      "508800/649600\tLoss: 46.674\tL0 Loss: 0.551\n",
      "512000/649600\tLoss: 45.236\tL0 Loss: 0.551\n",
      "515200/649600\tLoss: 42.943\tL0 Loss: 0.551\n",
      "518400/649600\tLoss: 42.380\tL0 Loss: 0.551\n",
      "521600/649600\tLoss: 42.393\tL0 Loss: 0.551\n",
      "524800/649600\tLoss: 43.182\tL0 Loss: 0.551\n",
      "528000/649600\tLoss: 41.716\tL0 Loss: 0.551\n",
      "531200/649600\tLoss: 41.319\tL0 Loss: 0.551\n",
      "534400/649600\tLoss: 45.644\tL0 Loss: 0.551\n",
      "537600/649600\tLoss: 46.724\tL0 Loss: 0.551\n",
      "540800/649600\tLoss: 46.115\tL0 Loss: 0.551\n",
      "544000/649600\tLoss: 46.245\tL0 Loss: 0.551\n",
      "547200/649600\tLoss: 45.222\tL0 Loss: 0.551\n",
      "550400/649600\tLoss: 45.257\tL0 Loss: 0.551\n",
      "553600/649600\tLoss: 44.735\tL0 Loss: 0.551\n",
      "556800/649600\tLoss: 43.921\tL0 Loss: 0.551\n",
      "560000/649600\tLoss: 46.223\tL0 Loss: 0.551\n",
      "563200/649600\tLoss: 45.901\tL0 Loss: 0.551\n",
      "566400/649600\tLoss: 45.110\tL0 Loss: 0.551\n",
      "569600/649600\tLoss: 44.297\tL0 Loss: 0.551\n",
      "572800/649600\tLoss: 44.278\tL0 Loss: 0.551\n",
      "576000/649600\tLoss: 52.302\tL0 Loss: 0.551\n",
      "579200/649600\tLoss: 49.618\tL0 Loss: 0.551\n",
      "582400/649600\tLoss: 51.478\tL0 Loss: 0.551\n",
      "585600/649600\tLoss: 51.107\tL0 Loss: 0.551\n",
      "588800/649600\tLoss: 52.096\tL0 Loss: 0.551\n",
      "592000/649600\tLoss: 51.128\tL0 Loss: 0.551\n",
      "595200/649600\tLoss: 50.696\tL0 Loss: 0.551\n",
      "598400/649600\tLoss: 48.739\tL0 Loss: 0.551\n",
      "601600/649600\tLoss: 51.429\tL0 Loss: 0.551\n",
      "604800/649600\tLoss: 51.558\tL0 Loss: 0.551\n",
      "608000/649600\tLoss: 50.282\tL0 Loss: 0.551\n",
      "611200/649600\tLoss: 50.024\tL0 Loss: 0.551\n",
      "614400/649600\tLoss: 52.597\tL0 Loss: 0.551\n",
      "617600/649600\tLoss: 57.143\tL0 Loss: 0.551\n",
      "620800/649600\tLoss: 57.087\tL0 Loss: 0.551\n",
      "624000/649600\tLoss: 58.589\tL0 Loss: 0.551\n",
      "627200/649600\tLoss: 58.139\tL0 Loss: 0.551\n",
      "630400/649600\tLoss: 58.441\tL0 Loss: 0.551\n",
      "633600/649600\tLoss: 58.367\tL0 Loss: 0.551\n",
      "636800/649600\tLoss: 57.687\tL0 Loss: 0.551\n",
      "640000/649600\tLoss: 56.630\tL0 Loss: 0.551\n",
      "643200/649600\tLoss: 60.362\tL0 Loss: 0.551\n",
      "646400/649600\tLoss: 58.525\tL0 Loss: 0.551\n",
      "Valid Loss: 140.008, Recon Error: 0.018\n",
      "140.00754679292186\n",
      "Epoch: 6 Average loss: 61.78 Valid loss: 140.00754679292186\tRecon Error:0.018\n",
      "0/649600\tLoss: 56.184\tL0 Loss: 0.551\n",
      "3200/649600\tLoss: 49.547\tL0 Loss: 0.551\n",
      "6400/649600\tLoss: 44.945\tL0 Loss: 0.551\n",
      "9600/649600\tLoss: 42.751\tL0 Loss: 0.551\n",
      "12800/649600\tLoss: 41.979\tL0 Loss: 0.551\n",
      "16000/649600\tLoss: 43.672\tL0 Loss: 0.551\n",
      "19200/649600\tLoss: 42.286\tL0 Loss: 0.551\n",
      "22400/649600\tLoss: 41.480\tL0 Loss: 0.551\n",
      "25600/649600\tLoss: 43.092\tL0 Loss: 0.551\n",
      "28800/649600\tLoss: 42.586\tL0 Loss: 0.551\n",
      "32000/649600\tLoss: 43.109\tL0 Loss: 0.551\n",
      "35200/649600\tLoss: 43.715\tL0 Loss: 0.551\n",
      "38400/649600\tLoss: 42.061\tL0 Loss: 0.551\n",
      "41600/649600\tLoss: 45.411\tL0 Loss: 0.551\n",
      "44800/649600\tLoss: 51.237\tL0 Loss: 0.551\n",
      "48000/649600\tLoss: 48.199\tL0 Loss: 0.551\n",
      "51200/649600\tLoss: 47.044\tL0 Loss: 0.552\n",
      "54400/649600\tLoss: 47.418\tL0 Loss: 0.552\n",
      "57600/649600\tLoss: 48.333\tL0 Loss: 0.552\n",
      "60800/649600\tLoss: 50.176\tL0 Loss: 0.552\n",
      "64000/649600\tLoss: 48.411\tL0 Loss: 0.552\n",
      "67200/649600\tLoss: 51.972\tL0 Loss: 0.552\n",
      "70400/649600\tLoss: 47.849\tL0 Loss: 0.552\n",
      "73600/649600\tLoss: 50.145\tL0 Loss: 0.552\n",
      "76800/649600\tLoss: 50.756\tL0 Loss: 0.552\n",
      "80000/649600\tLoss: 49.260\tL0 Loss: 0.552\n",
      "83200/649600\tLoss: 53.311\tL0 Loss: 0.552\n",
      "86400/649600\tLoss: 56.277\tL0 Loss: 0.552\n",
      "89600/649600\tLoss: 56.864\tL0 Loss: 0.552\n",
      "92800/649600\tLoss: 57.448\tL0 Loss: 0.552\n",
      "96000/649600\tLoss: 55.251\tL0 Loss: 0.552\n",
      "99200/649600\tLoss: 55.188\tL0 Loss: 0.552\n",
      "102400/649600\tLoss: 55.018\tL0 Loss: 0.552\n",
      "105600/649600\tLoss: 55.228\tL0 Loss: 0.552\n",
      "108800/649600\tLoss: 53.008\tL0 Loss: 0.552\n",
      "112000/649600\tLoss: 52.911\tL0 Loss: 0.552\n",
      "115200/649600\tLoss: 52.960\tL0 Loss: 0.552\n",
      "118400/649600\tLoss: 54.796\tL0 Loss: 0.552\n",
      "121600/649600\tLoss: 52.247\tL0 Loss: 0.552\n",
      "124800/649600\tLoss: 63.357\tL0 Loss: 0.552\n",
      "128000/649600\tLoss: 61.162\tL0 Loss: 0.552\n",
      "131200/649600\tLoss: 65.596\tL0 Loss: 0.552\n",
      "134400/649600\tLoss: 62.773\tL0 Loss: 0.552\n",
      "137600/649600\tLoss: 62.045\tL0 Loss: 0.552\n",
      "140800/649600\tLoss: 61.451\tL0 Loss: 0.552\n",
      "144000/649600\tLoss: 70.737\tL0 Loss: 0.552\n",
      "147200/649600\tLoss: 60.945\tL0 Loss: 0.552\n",
      "150400/649600\tLoss: 62.172\tL0 Loss: 0.552\n",
      "153600/649600\tLoss: 61.456\tL0 Loss: 0.552\n",
      "156800/649600\tLoss: 60.111\tL0 Loss: 0.552\n",
      "160000/649600\tLoss: 63.097\tL0 Loss: 0.552\n",
      "163200/649600\tLoss: 63.173\tL0 Loss: 0.552\n",
      "166400/649600\tLoss: 71.150\tL0 Loss: 0.552\n",
      "169600/649600\tLoss: 69.754\tL0 Loss: 0.552\n",
      "172800/649600\tLoss: 74.701\tL0 Loss: 0.552\n",
      "176000/649600\tLoss: 72.806\tL0 Loss: 0.552\n",
      "179200/649600\tLoss: 69.800\tL0 Loss: 0.552\n",
      "182400/649600\tLoss: 70.028\tL0 Loss: 0.552\n",
      "185600/649600\tLoss: 70.255\tL0 Loss: 0.552\n",
      "188800/649600\tLoss: 71.331\tL0 Loss: 0.552\n",
      "192000/649600\tLoss: 69.306\tL0 Loss: 0.552\n",
      "195200/649600\tLoss: 69.243\tL0 Loss: 0.552\n",
      "198400/649600\tLoss: 70.686\tL0 Loss: 0.552\n",
      "201600/649600\tLoss: 70.413\tL0 Loss: 0.552\n",
      "204800/649600\tLoss: 70.635\tL0 Loss: 0.552\n",
      "208000/649600\tLoss: 82.317\tL0 Loss: 0.552\n",
      "211200/649600\tLoss: 81.151\tL0 Loss: 0.552\n",
      "214400/649600\tLoss: 82.174\tL0 Loss: 0.552\n",
      "217600/649600\tLoss: 79.758\tL0 Loss: 0.552\n",
      "220800/649600\tLoss: 79.932\tL0 Loss: 0.552\n",
      "224000/649600\tLoss: 77.423\tL0 Loss: 0.552\n",
      "227200/649600\tLoss: 85.420\tL0 Loss: 0.552\n",
      "230400/649600\tLoss: 81.891\tL0 Loss: 0.553\n",
      "233600/649600\tLoss: 80.697\tL0 Loss: 0.553\n",
      "236800/649600\tLoss: 78.323\tL0 Loss: 0.553\n",
      "240000/649600\tLoss: 80.636\tL0 Loss: 0.553\n",
      "243200/649600\tLoss: 78.501\tL0 Loss: 0.553\n",
      "246400/649600\tLoss: 94.167\tL0 Loss: 0.553\n",
      "249600/649600\tLoss: 84.968\tL0 Loss: 0.553\n",
      "252800/649600\tLoss: 59.434\tL0 Loss: 0.553\n",
      "256000/649600\tLoss: 50.393\tL0 Loss: 0.553\n",
      "259200/649600\tLoss: 48.540\tL0 Loss: 0.553\n",
      "262400/649600\tLoss: 46.866\tL0 Loss: 0.553\n",
      "265600/649600\tLoss: 46.945\tL0 Loss: 0.553\n",
      "268800/649600\tLoss: 45.884\tL0 Loss: 0.553\n",
      "272000/649600\tLoss: 46.206\tL0 Loss: 0.553\n",
      "275200/649600\tLoss: 44.569\tL0 Loss: 0.553\n",
      "278400/649600\tLoss: 45.786\tL0 Loss: 0.553\n",
      "281600/649600\tLoss: 44.730\tL0 Loss: 0.553\n",
      "284800/649600\tLoss: 45.607\tL0 Loss: 0.553\n",
      "288000/649600\tLoss: 51.175\tL0 Loss: 0.553\n",
      "291200/649600\tLoss: 50.795\tL0 Loss: 0.553\n",
      "294400/649600\tLoss: 54.907\tL0 Loss: 0.553\n",
      "297600/649600\tLoss: 52.967\tL0 Loss: 0.553\n",
      "300800/649600\tLoss: 52.609\tL0 Loss: 0.553\n",
      "304000/649600\tLoss: 51.742\tL0 Loss: 0.553\n",
      "307200/649600\tLoss: 52.664\tL0 Loss: 0.553\n",
      "310400/649600\tLoss: 51.191\tL0 Loss: 0.553\n",
      "313600/649600\tLoss: 51.101\tL0 Loss: 0.553\n",
      "316800/649600\tLoss: 52.874\tL0 Loss: 0.553\n",
      "320000/649600\tLoss: 52.523\tL0 Loss: 0.553\n",
      "323200/649600\tLoss: 52.719\tL0 Loss: 0.553\n",
      "326400/649600\tLoss: 52.791\tL0 Loss: 0.553\n",
      "329600/649600\tLoss: 58.503\tL0 Loss: 0.553\n",
      "332800/649600\tLoss: 57.963\tL0 Loss: 0.553\n",
      "336000/649600\tLoss: 60.143\tL0 Loss: 0.553\n",
      "339200/649600\tLoss: 61.834\tL0 Loss: 0.553\n",
      "342400/649600\tLoss: 62.559\tL0 Loss: 0.553\n",
      "345600/649600\tLoss: 61.581\tL0 Loss: 0.553\n",
      "348800/649600\tLoss: 61.444\tL0 Loss: 0.553\n",
      "352000/649600\tLoss: 59.889\tL0 Loss: 0.553\n",
      "355200/649600\tLoss: 62.037\tL0 Loss: 0.553\n",
      "358400/649600\tLoss: 59.846\tL0 Loss: 0.553\n",
      "361600/649600\tLoss: 60.937\tL0 Loss: 0.553\n",
      "364800/649600\tLoss: 60.368\tL0 Loss: 0.553\n",
      "368000/649600\tLoss: 59.301\tL0 Loss: 0.553\n",
      "371200/649600\tLoss: 64.910\tL0 Loss: 0.553\n",
      "374400/649600\tLoss: 66.277\tL0 Loss: 0.553\n",
      "377600/649600\tLoss: 68.856\tL0 Loss: 0.553\n",
      "380800/649600\tLoss: 67.480\tL0 Loss: 0.553\n",
      "384000/649600\tLoss: 72.257\tL0 Loss: 0.553\n",
      "387200/649600\tLoss: 70.825\tL0 Loss: 0.553\n",
      "390400/649600\tLoss: 65.935\tL0 Loss: 0.553\n",
      "393600/649600\tLoss: 66.935\tL0 Loss: 0.553\n",
      "396800/649600\tLoss: 72.687\tL0 Loss: 0.553\n",
      "400000/649600\tLoss: 69.174\tL0 Loss: 0.553\n",
      "403200/649600\tLoss: 67.540\tL0 Loss: 0.553\n",
      "406400/649600\tLoss: 71.470\tL0 Loss: 0.553\n",
      "409600/649600\tLoss: 68.533\tL0 Loss: 0.553\n",
      "412800/649600\tLoss: 72.370\tL0 Loss: 0.553\n",
      "416000/649600\tLoss: 81.771\tL0 Loss: 0.553\n",
      "419200/649600\tLoss: 83.097\tL0 Loss: 0.553\n",
      "422400/649600\tLoss: 76.009\tL0 Loss: 0.553\n",
      "425600/649600\tLoss: 84.048\tL0 Loss: 0.553\n",
      "428800/649600\tLoss: 79.550\tL0 Loss: 0.553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432000/649600\tLoss: 74.024\tL0 Loss: 0.553\n",
      "435200/649600\tLoss: 81.829\tL0 Loss: 0.553\n",
      "438400/649600\tLoss: 83.108\tL0 Loss: 0.553\n",
      "441600/649600\tLoss: 75.733\tL0 Loss: 0.553\n",
      "444800/649600\tLoss: 79.122\tL0 Loss: 0.553\n",
      "448000/649600\tLoss: 83.307\tL0 Loss: 0.553\n",
      "451200/649600\tLoss: 76.482\tL0 Loss: 0.553\n",
      "454400/649600\tLoss: 78.123\tL0 Loss: 0.553\n",
      "457600/649600\tLoss: 96.763\tL0 Loss: 0.554\n",
      "460800/649600\tLoss: 87.470\tL0 Loss: 0.554\n",
      "464000/649600\tLoss: 86.640\tL0 Loss: 0.554\n",
      "467200/649600\tLoss: 96.927\tL0 Loss: 0.554\n",
      "470400/649600\tLoss: 87.816\tL0 Loss: 0.554\n",
      "473600/649600\tLoss: 79.769\tL0 Loss: 0.554\n",
      "476800/649600\tLoss: 96.888\tL0 Loss: 0.554\n",
      "480000/649600\tLoss: 97.395\tL0 Loss: 0.554\n",
      "483200/649600\tLoss: 85.250\tL0 Loss: 0.554\n",
      "486400/649600\tLoss: 92.563\tL0 Loss: 0.554\n",
      "489600/649600\tLoss: 91.847\tL0 Loss: 0.554\n",
      "492800/649600\tLoss: 87.556\tL0 Loss: 0.554\n",
      "496000/649600\tLoss: 69.280\tL0 Loss: 0.554\n",
      "499200/649600\tLoss: 53.744\tL0 Loss: 0.554\n",
      "502400/649600\tLoss: 47.038\tL0 Loss: 0.554\n",
      "505600/649600\tLoss: 44.893\tL0 Loss: 0.554\n",
      "508800/649600\tLoss: 41.934\tL0 Loss: 0.554\n",
      "512000/649600\tLoss: 42.540\tL0 Loss: 0.554\n",
      "515200/649600\tLoss: 40.353\tL0 Loss: 0.554\n",
      "518400/649600\tLoss: 41.686\tL0 Loss: 0.554\n",
      "521600/649600\tLoss: 41.822\tL0 Loss: 0.554\n",
      "524800/649600\tLoss: 41.984\tL0 Loss: 0.554\n",
      "528000/649600\tLoss: 40.354\tL0 Loss: 0.554\n",
      "531200/649600\tLoss: 39.097\tL0 Loss: 0.554\n",
      "534400/649600\tLoss: 44.964\tL0 Loss: 0.554\n",
      "537600/649600\tLoss: 46.755\tL0 Loss: 0.554\n",
      "540800/649600\tLoss: 44.052\tL0 Loss: 0.554\n",
      "544000/649600\tLoss: 44.716\tL0 Loss: 0.554\n",
      "547200/649600\tLoss: 46.285\tL0 Loss: 0.554\n",
      "550400/649600\tLoss: 44.300\tL0 Loss: 0.554\n",
      "553600/649600\tLoss: 43.694\tL0 Loss: 0.554\n",
      "556800/649600\tLoss: 43.647\tL0 Loss: 0.554\n",
      "560000/649600\tLoss: 43.677\tL0 Loss: 0.554\n",
      "563200/649600\tLoss: 43.594\tL0 Loss: 0.554\n",
      "566400/649600\tLoss: 45.682\tL0 Loss: 0.554\n",
      "569600/649600\tLoss: 44.529\tL0 Loss: 0.554\n",
      "572800/649600\tLoss: 44.637\tL0 Loss: 0.554\n",
      "576000/649600\tLoss: 50.252\tL0 Loss: 0.554\n",
      "579200/649600\tLoss: 51.760\tL0 Loss: 0.554\n",
      "582400/649600\tLoss: 50.384\tL0 Loss: 0.554\n",
      "585600/649600\tLoss: 49.951\tL0 Loss: 0.554\n",
      "588800/649600\tLoss: 50.693\tL0 Loss: 0.554\n",
      "592000/649600\tLoss: 49.731\tL0 Loss: 0.554\n",
      "595200/649600\tLoss: 50.317\tL0 Loss: 0.554\n",
      "598400/649600\tLoss: 48.737\tL0 Loss: 0.554\n",
      "601600/649600\tLoss: 50.361\tL0 Loss: 0.554\n",
      "604800/649600\tLoss: 49.245\tL0 Loss: 0.554\n",
      "608000/649600\tLoss: 50.404\tL0 Loss: 0.554\n",
      "611200/649600\tLoss: 50.844\tL0 Loss: 0.554\n",
      "614400/649600\tLoss: 49.386\tL0 Loss: 0.554\n",
      "617600/649600\tLoss: 57.015\tL0 Loss: 0.554\n",
      "620800/649600\tLoss: 55.827\tL0 Loss: 0.554\n",
      "624000/649600\tLoss: 58.602\tL0 Loss: 0.554\n",
      "627200/649600\tLoss: 59.937\tL0 Loss: 0.554\n",
      "630400/649600\tLoss: 58.067\tL0 Loss: 0.554\n",
      "633600/649600\tLoss: 56.724\tL0 Loss: 0.554\n",
      "636800/649600\tLoss: 56.470\tL0 Loss: 0.554\n",
      "640000/649600\tLoss: 57.027\tL0 Loss: 0.554\n",
      "643200/649600\tLoss: 57.820\tL0 Loss: 0.554\n",
      "646400/649600\tLoss: 58.063\tL0 Loss: 0.554\n",
      "Valid Loss: 141.392, Recon Error: 0.020\n",
      "141.39224900103352\n",
      "Epoch: 7 Average loss: 60.33 Valid loss: 141.39224900103352\tRecon Error:0.020\n",
      "0/649600\tLoss: 63.846\tL0 Loss: 0.554\n",
      "3200/649600\tLoss: 47.609\tL0 Loss: 0.554\n",
      "6400/649600\tLoss: 44.176\tL0 Loss: 0.554\n",
      "9600/649600\tLoss: 43.533\tL0 Loss: 0.554\n",
      "12800/649600\tLoss: 41.782\tL0 Loss: 0.554\n",
      "16000/649600\tLoss: 43.607\tL0 Loss: 0.554\n",
      "19200/649600\tLoss: 42.370\tL0 Loss: 0.554\n",
      "22400/649600\tLoss: 40.944\tL0 Loss: 0.554\n",
      "25600/649600\tLoss: 40.543\tL0 Loss: 0.554\n",
      "28800/649600\tLoss: 42.638\tL0 Loss: 0.554\n",
      "32000/649600\tLoss: 42.572\tL0 Loss: 0.554\n",
      "35200/649600\tLoss: 44.090\tL0 Loss: 0.554\n",
      "38400/649600\tLoss: 41.256\tL0 Loss: 0.555\n",
      "41600/649600\tLoss: 44.922\tL0 Loss: 0.555\n",
      "44800/649600\tLoss: 47.232\tL0 Loss: 0.555\n",
      "48000/649600\tLoss: 47.639\tL0 Loss: 0.555\n",
      "51200/649600\tLoss: 46.940\tL0 Loss: 0.555\n",
      "54400/649600\tLoss: 46.852\tL0 Loss: 0.555\n",
      "57600/649600\tLoss: 48.687\tL0 Loss: 0.555\n",
      "60800/649600\tLoss: 50.693\tL0 Loss: 0.555\n",
      "64000/649600\tLoss: 48.698\tL0 Loss: 0.555\n",
      "67200/649600\tLoss: 49.131\tL0 Loss: 0.555\n",
      "70400/649600\tLoss: 48.909\tL0 Loss: 0.555\n",
      "73600/649600\tLoss: 49.839\tL0 Loss: 0.555\n",
      "76800/649600\tLoss: 48.534\tL0 Loss: 0.555\n",
      "80000/649600\tLoss: 48.090\tL0 Loss: 0.555\n",
      "83200/649600\tLoss: 52.614\tL0 Loss: 0.555\n",
      "86400/649600\tLoss: 53.980\tL0 Loss: 0.555\n",
      "89600/649600\tLoss: 54.777\tL0 Loss: 0.555\n",
      "92800/649600\tLoss: 58.283\tL0 Loss: 0.555\n",
      "96000/649600\tLoss: 51.598\tL0 Loss: 0.555\n",
      "99200/649600\tLoss: 55.577\tL0 Loss: 0.555\n",
      "102400/649600\tLoss: 53.604\tL0 Loss: 0.555\n",
      "105600/649600\tLoss: 55.466\tL0 Loss: 0.555\n",
      "108800/649600\tLoss: 53.575\tL0 Loss: 0.555\n",
      "112000/649600\tLoss: 53.644\tL0 Loss: 0.555\n",
      "115200/649600\tLoss: 53.196\tL0 Loss: 0.555\n",
      "118400/649600\tLoss: 52.254\tL0 Loss: 0.555\n",
      "121600/649600\tLoss: 53.726\tL0 Loss: 0.555\n",
      "124800/649600\tLoss: 63.143\tL0 Loss: 0.555\n",
      "128000/649600\tLoss: 62.946\tL0 Loss: 0.555\n",
      "131200/649600\tLoss: 63.970\tL0 Loss: 0.555\n",
      "134400/649600\tLoss: 61.635\tL0 Loss: 0.555\n",
      "137600/649600\tLoss: 61.801\tL0 Loss: 0.555\n",
      "140800/649600\tLoss: 61.753\tL0 Loss: 0.555\n",
      "144000/649600\tLoss: 61.238\tL0 Loss: 0.555\n",
      "147200/649600\tLoss: 60.306\tL0 Loss: 0.555\n",
      "150400/649600\tLoss: 61.398\tL0 Loss: 0.555\n",
      "153600/649600\tLoss: 61.968\tL0 Loss: 0.555\n",
      "156800/649600\tLoss: 62.803\tL0 Loss: 0.555\n",
      "160000/649600\tLoss: 61.481\tL0 Loss: 0.555\n",
      "163200/649600\tLoss: 61.875\tL0 Loss: 0.555\n",
      "166400/649600\tLoss: 71.480\tL0 Loss: 0.555\n",
      "169600/649600\tLoss: 68.980\tL0 Loss: 0.555\n",
      "172800/649600\tLoss: 68.864\tL0 Loss: 0.555\n",
      "176000/649600\tLoss: 69.329\tL0 Loss: 0.555\n",
      "179200/649600\tLoss: 69.100\tL0 Loss: 0.555\n",
      "182400/649600\tLoss: 69.149\tL0 Loss: 0.555\n",
      "185600/649600\tLoss: 68.264\tL0 Loss: 0.555\n",
      "188800/649600\tLoss: 69.863\tL0 Loss: 0.555\n",
      "192000/649600\tLoss: 73.405\tL0 Loss: 0.555\n",
      "195200/649600\tLoss: 69.124\tL0 Loss: 0.555\n",
      "198400/649600\tLoss: 68.423\tL0 Loss: 0.555\n",
      "201600/649600\tLoss: 70.822\tL0 Loss: 0.555\n",
      "204800/649600\tLoss: 71.934\tL0 Loss: 0.555\n",
      "208000/649600\tLoss: 80.633\tL0 Loss: 0.556\n",
      "211200/649600\tLoss: 81.630\tL0 Loss: 0.556\n",
      "214400/649600\tLoss: 82.400\tL0 Loss: 0.556\n",
      "217600/649600\tLoss: 77.026\tL0 Loss: 0.556\n",
      "220800/649600\tLoss: 80.357\tL0 Loss: 0.556\n",
      "224000/649600\tLoss: 80.530\tL0 Loss: 0.556\n",
      "227200/649600\tLoss: 77.558\tL0 Loss: 0.556\n",
      "230400/649600\tLoss: 77.846\tL0 Loss: 0.556\n",
      "233600/649600\tLoss: 82.263\tL0 Loss: 0.556\n",
      "236800/649600\tLoss: 74.087\tL0 Loss: 0.556\n",
      "240000/649600\tLoss: 82.577\tL0 Loss: 0.556\n",
      "243200/649600\tLoss: 81.411\tL0 Loss: 0.556\n",
      "246400/649600\tLoss: 91.282\tL0 Loss: 0.556\n",
      "249600/649600\tLoss: 87.714\tL0 Loss: 0.556\n",
      "252800/649600\tLoss: 60.137\tL0 Loss: 0.556\n",
      "256000/649600\tLoss: 51.491\tL0 Loss: 0.556\n",
      "259200/649600\tLoss: 48.500\tL0 Loss: 0.556\n",
      "262400/649600\tLoss: 46.960\tL0 Loss: 0.556\n",
      "265600/649600\tLoss: 45.738\tL0 Loss: 0.556\n",
      "268800/649600\tLoss: 46.812\tL0 Loss: 0.556\n",
      "272000/649600\tLoss: 45.806\tL0 Loss: 0.556\n",
      "275200/649600\tLoss: 44.814\tL0 Loss: 0.556\n",
      "278400/649600\tLoss: 43.977\tL0 Loss: 0.556\n",
      "281600/649600\tLoss: 45.781\tL0 Loss: 0.556\n",
      "284800/649600\tLoss: 45.126\tL0 Loss: 0.556\n",
      "288000/649600\tLoss: 49.535\tL0 Loss: 0.556\n",
      "291200/649600\tLoss: 50.734\tL0 Loss: 0.556\n",
      "294400/649600\tLoss: 51.881\tL0 Loss: 0.556\n",
      "297600/649600\tLoss: 52.010\tL0 Loss: 0.556\n",
      "300800/649600\tLoss: 51.765\tL0 Loss: 0.556\n",
      "304000/649600\tLoss: 53.270\tL0 Loss: 0.556\n",
      "307200/649600\tLoss: 51.360\tL0 Loss: 0.556\n",
      "310400/649600\tLoss: 50.314\tL0 Loss: 0.556\n",
      "313600/649600\tLoss: 51.190\tL0 Loss: 0.556\n",
      "316800/649600\tLoss: 51.269\tL0 Loss: 0.556\n",
      "320000/649600\tLoss: 52.390\tL0 Loss: 0.556\n",
      "323200/649600\tLoss: 51.592\tL0 Loss: 0.556\n",
      "326400/649600\tLoss: 51.819\tL0 Loss: 0.556\n",
      "329600/649600\tLoss: 56.612\tL0 Loss: 0.556\n",
      "332800/649600\tLoss: 56.689\tL0 Loss: 0.556\n",
      "336000/649600\tLoss: 58.810\tL0 Loss: 0.556\n",
      "339200/649600\tLoss: 61.546\tL0 Loss: 0.556\n",
      "342400/649600\tLoss: 60.840\tL0 Loss: 0.556\n",
      "345600/649600\tLoss: 60.615\tL0 Loss: 0.556\n",
      "348800/649600\tLoss: 57.810\tL0 Loss: 0.556\n",
      "352000/649600\tLoss: 58.734\tL0 Loss: 0.556\n",
      "355200/649600\tLoss: 60.217\tL0 Loss: 0.556\n",
      "358400/649600\tLoss: 61.375\tL0 Loss: 0.556\n",
      "361600/649600\tLoss: 60.123\tL0 Loss: 0.556\n",
      "364800/649600\tLoss: 59.071\tL0 Loss: 0.556\n",
      "368000/649600\tLoss: 58.881\tL0 Loss: 0.556\n",
      "371200/649600\tLoss: 65.073\tL0 Loss: 0.556\n",
      "374400/649600\tLoss: 68.359\tL0 Loss: 0.556\n",
      "377600/649600\tLoss: 68.145\tL0 Loss: 0.556\n",
      "380800/649600\tLoss: 67.140\tL0 Loss: 0.556\n",
      "384000/649600\tLoss: 70.101\tL0 Loss: 0.556\n",
      "387200/649600\tLoss: 68.735\tL0 Loss: 0.556\n",
      "390400/649600\tLoss: 65.531\tL0 Loss: 0.556\n",
      "393600/649600\tLoss: 67.627\tL0 Loss: 0.556\n",
      "396800/649600\tLoss: 72.106\tL0 Loss: 0.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000/649600\tLoss: 67.855\tL0 Loss: 0.556\n",
      "403200/649600\tLoss: 65.249\tL0 Loss: 0.556\n",
      "406400/649600\tLoss: 68.465\tL0 Loss: 0.556\n",
      "409600/649600\tLoss: 66.874\tL0 Loss: 0.556\n",
      "412800/649600\tLoss: 72.409\tL0 Loss: 0.556\n",
      "416000/649600\tLoss: 76.778\tL0 Loss: 0.556\n",
      "419200/649600\tLoss: 77.655\tL0 Loss: 0.556\n",
      "422400/649600\tLoss: 74.755\tL0 Loss: 0.556\n",
      "425600/649600\tLoss: 79.815\tL0 Loss: 0.556\n",
      "428800/649600\tLoss: 78.509\tL0 Loss: 0.556\n",
      "432000/649600\tLoss: 69.111\tL0 Loss: 0.556\n",
      "435200/649600\tLoss: 77.025\tL0 Loss: 0.556\n",
      "438400/649600\tLoss: 80.355\tL0 Loss: 0.556\n",
      "441600/649600\tLoss: 71.113\tL0 Loss: 0.556\n",
      "444800/649600\tLoss: 73.942\tL0 Loss: 0.556\n",
      "448000/649600\tLoss: 79.236\tL0 Loss: 0.556\n",
      "451200/649600\tLoss: 71.083\tL0 Loss: 0.556\n",
      "454400/649600\tLoss: 80.783\tL0 Loss: 0.557\n",
      "457600/649600\tLoss: 92.582\tL0 Loss: 0.557\n",
      "460800/649600\tLoss: 87.796\tL0 Loss: 0.557\n",
      "464000/649600\tLoss: 80.954\tL0 Loss: 0.557\n",
      "467200/649600\tLoss: 87.982\tL0 Loss: 0.557\n",
      "470400/649600\tLoss: 80.636\tL0 Loss: 0.557\n",
      "473600/649600\tLoss: 73.623\tL0 Loss: 0.557\n",
      "476800/649600\tLoss: 91.648\tL0 Loss: 0.557\n",
      "480000/649600\tLoss: 96.340\tL0 Loss: 0.557\n",
      "483200/649600\tLoss: 85.144\tL0 Loss: 0.557\n",
      "486400/649600\tLoss: 91.323\tL0 Loss: 0.557\n",
      "489600/649600\tLoss: 87.142\tL0 Loss: 0.557\n",
      "492800/649600\tLoss: 89.629\tL0 Loss: 0.557\n",
      "496000/649600\tLoss: 68.997\tL0 Loss: 0.557\n",
      "499200/649600\tLoss: 61.607\tL0 Loss: 0.557\n",
      "502400/649600\tLoss: 53.935\tL0 Loss: 0.557\n",
      "505600/649600\tLoss: 49.632\tL0 Loss: 0.557\n",
      "508800/649600\tLoss: 46.431\tL0 Loss: 0.557\n",
      "512000/649600\tLoss: 43.777\tL0 Loss: 0.557\n",
      "515200/649600\tLoss: 41.712\tL0 Loss: 0.557\n",
      "518400/649600\tLoss: 41.795\tL0 Loss: 0.557\n",
      "521600/649600\tLoss: 41.735\tL0 Loss: 0.557\n",
      "524800/649600\tLoss: 40.562\tL0 Loss: 0.557\n",
      "528000/649600\tLoss: 39.863\tL0 Loss: 0.557\n",
      "531200/649600\tLoss: 38.868\tL0 Loss: 0.557\n",
      "534400/649600\tLoss: 44.319\tL0 Loss: 0.557\n",
      "537600/649600\tLoss: 44.424\tL0 Loss: 0.557\n",
      "540800/649600\tLoss: 45.474\tL0 Loss: 0.557\n",
      "544000/649600\tLoss: 45.474\tL0 Loss: 0.557\n",
      "547200/649600\tLoss: 44.713\tL0 Loss: 0.557\n",
      "550400/649600\tLoss: 43.690\tL0 Loss: 0.557\n",
      "553600/649600\tLoss: 43.169\tL0 Loss: 0.557\n",
      "556800/649600\tLoss: 42.694\tL0 Loss: 0.557\n",
      "560000/649600\tLoss: 43.744\tL0 Loss: 0.557\n",
      "563200/649600\tLoss: 45.241\tL0 Loss: 0.557\n",
      "566400/649600\tLoss: 44.004\tL0 Loss: 0.557\n",
      "569600/649600\tLoss: 43.318\tL0 Loss: 0.557\n",
      "572800/649600\tLoss: 43.248\tL0 Loss: 0.557\n",
      "576000/649600\tLoss: 49.003\tL0 Loss: 0.557\n",
      "579200/649600\tLoss: 49.419\tL0 Loss: 0.557\n",
      "582400/649600\tLoss: 49.434\tL0 Loss: 0.557\n",
      "585600/649600\tLoss: 50.332\tL0 Loss: 0.557\n",
      "588800/649600\tLoss: 50.091\tL0 Loss: 0.557\n",
      "592000/649600\tLoss: 49.894\tL0 Loss: 0.557\n",
      "595200/649600\tLoss: 49.089\tL0 Loss: 0.557\n",
      "598400/649600\tLoss: 49.382\tL0 Loss: 0.557\n",
      "601600/649600\tLoss: 50.950\tL0 Loss: 0.557\n",
      "604800/649600\tLoss: 51.535\tL0 Loss: 0.557\n",
      "608000/649600\tLoss: 49.256\tL0 Loss: 0.557\n",
      "611200/649600\tLoss: 50.310\tL0 Loss: 0.557\n",
      "614400/649600\tLoss: 50.279\tL0 Loss: 0.557\n",
      "617600/649600\tLoss: 56.022\tL0 Loss: 0.557\n",
      "620800/649600\tLoss: 56.088\tL0 Loss: 0.557\n",
      "624000/649600\tLoss: 56.705\tL0 Loss: 0.557\n",
      "627200/649600\tLoss: 59.111\tL0 Loss: 0.557\n",
      "630400/649600\tLoss: 57.267\tL0 Loss: 0.557\n",
      "633600/649600\tLoss: 56.954\tL0 Loss: 0.557\n",
      "636800/649600\tLoss: 56.775\tL0 Loss: 0.557\n",
      "640000/649600\tLoss: 55.323\tL0 Loss: 0.557\n",
      "643200/649600\tLoss: 56.131\tL0 Loss: 0.557\n",
      "646400/649600\tLoss: 57.470\tL0 Loss: 0.557\n",
      "Valid Loss: 141.931, Recon Error: 0.018\n",
      "141.93055354751402\n",
      "Epoch: 8 Average loss: 59.44 Valid loss: 141.93055354751402\tRecon Error:0.018\n",
      "0/649600\tLoss: 70.028\tL0 Loss: 0.557\n",
      "3200/649600\tLoss: 47.403\tL0 Loss: 0.557\n",
      "6400/649600\tLoss: 42.517\tL0 Loss: 0.557\n",
      "9600/649600\tLoss: 41.129\tL0 Loss: 0.557\n",
      "12800/649600\tLoss: 41.287\tL0 Loss: 0.557\n",
      "16000/649600\tLoss: 40.752\tL0 Loss: 0.557\n",
      "19200/649600\tLoss: 40.659\tL0 Loss: 0.557\n",
      "22400/649600\tLoss: 40.709\tL0 Loss: 0.557\n",
      "25600/649600\tLoss: 40.339\tL0 Loss: 0.557\n",
      "28800/649600\tLoss: 41.611\tL0 Loss: 0.558\n",
      "32000/649600\tLoss: 41.362\tL0 Loss: 0.558\n",
      "35200/649600\tLoss: 40.614\tL0 Loss: 0.558\n",
      "38400/649600\tLoss: 40.602\tL0 Loss: 0.558\n",
      "41600/649600\tLoss: 43.222\tL0 Loss: 0.558\n",
      "44800/649600\tLoss: 47.745\tL0 Loss: 0.558\n",
      "48000/649600\tLoss: 46.416\tL0 Loss: 0.558\n",
      "51200/649600\tLoss: 46.179\tL0 Loss: 0.558\n",
      "54400/649600\tLoss: 49.355\tL0 Loss: 0.558\n",
      "57600/649600\tLoss: 46.482\tL0 Loss: 0.558\n",
      "60800/649600\tLoss: 44.766\tL0 Loss: 0.558\n",
      "64000/649600\tLoss: 46.995\tL0 Loss: 0.558\n",
      "67200/649600\tLoss: 48.191\tL0 Loss: 0.558\n",
      "70400/649600\tLoss: 46.783\tL0 Loss: 0.558\n",
      "73600/649600\tLoss: 45.856\tL0 Loss: 0.558\n",
      "76800/649600\tLoss: 46.156\tL0 Loss: 0.558\n",
      "80000/649600\tLoss: 45.412\tL0 Loss: 0.558\n",
      "83200/649600\tLoss: 50.848\tL0 Loss: 0.558\n",
      "86400/649600\tLoss: 52.161\tL0 Loss: 0.558\n",
      "89600/649600\tLoss: 55.920\tL0 Loss: 0.558\n",
      "92800/649600\tLoss: 52.604\tL0 Loss: 0.558\n",
      "96000/649600\tLoss: 51.535\tL0 Loss: 0.558\n",
      "99200/649600\tLoss: 52.394\tL0 Loss: 0.558\n",
      "102400/649600\tLoss: 54.591\tL0 Loss: 0.558\n",
      "105600/649600\tLoss: 52.608\tL0 Loss: 0.558\n",
      "108800/649600\tLoss: 52.232\tL0 Loss: 0.558\n",
      "112000/649600\tLoss: 51.334\tL0 Loss: 0.558\n",
      "115200/649600\tLoss: 51.842\tL0 Loss: 0.558\n",
      "118400/649600\tLoss: 52.543\tL0 Loss: 0.558\n",
      "121600/649600\tLoss: 53.163\tL0 Loss: 0.558\n",
      "124800/649600\tLoss: 57.977\tL0 Loss: 0.558\n",
      "128000/649600\tLoss: 62.617\tL0 Loss: 0.558\n",
      "131200/649600\tLoss: 61.879\tL0 Loss: 0.558\n",
      "134400/649600\tLoss: 59.131\tL0 Loss: 0.558\n",
      "137600/649600\tLoss: 60.565\tL0 Loss: 0.558\n",
      "140800/649600\tLoss: 61.051\tL0 Loss: 0.558\n",
      "144000/649600\tLoss: 59.496\tL0 Loss: 0.558\n",
      "147200/649600\tLoss: 59.986\tL0 Loss: 0.558\n",
      "150400/649600\tLoss: 60.360\tL0 Loss: 0.558\n",
      "153600/649600\tLoss: 59.652\tL0 Loss: 0.558\n",
      "156800/649600\tLoss: 57.630\tL0 Loss: 0.558\n",
      "160000/649600\tLoss: 59.660\tL0 Loss: 0.558\n",
      "163200/649600\tLoss: 58.366\tL0 Loss: 0.558\n",
      "166400/649600\tLoss: 70.623\tL0 Loss: 0.558\n",
      "169600/649600\tLoss: 70.740\tL0 Loss: 0.558\n",
      "172800/649600\tLoss: 68.106\tL0 Loss: 0.558\n",
      "176000/649600\tLoss: 70.245\tL0 Loss: 0.558\n",
      "179200/649600\tLoss: 68.565\tL0 Loss: 0.558\n",
      "182400/649600\tLoss: 69.659\tL0 Loss: 0.558\n",
      "185600/649600\tLoss: 68.369\tL0 Loss: 0.558\n",
      "188800/649600\tLoss: 68.085\tL0 Loss: 0.558\n",
      "192000/649600\tLoss: 68.596\tL0 Loss: 0.558\n",
      "195200/649600\tLoss: 70.580\tL0 Loss: 0.558\n",
      "198400/649600\tLoss: 67.562\tL0 Loss: 0.558\n",
      "201600/649600\tLoss: 70.305\tL0 Loss: 0.558\n",
      "204800/649600\tLoss: 72.194\tL0 Loss: 0.558\n",
      "208000/649600\tLoss: 79.288\tL0 Loss: 0.558\n",
      "211200/649600\tLoss: 77.549\tL0 Loss: 0.558\n",
      "214400/649600\tLoss: 77.801\tL0 Loss: 0.558\n",
      "217600/649600\tLoss: 73.554\tL0 Loss: 0.558\n",
      "220800/649600\tLoss: 78.832\tL0 Loss: 0.559\n",
      "224000/649600\tLoss: 80.421\tL0 Loss: 0.559\n",
      "227200/649600\tLoss: 77.778\tL0 Loss: 0.559\n",
      "230400/649600\tLoss: 77.318\tL0 Loss: 0.559\n",
      "233600/649600\tLoss: 78.154\tL0 Loss: 0.559\n",
      "236800/649600\tLoss: 74.824\tL0 Loss: 0.559\n",
      "240000/649600\tLoss: 77.562\tL0 Loss: 0.559\n",
      "243200/649600\tLoss: 76.818\tL0 Loss: 0.559\n",
      "246400/649600\tLoss: 89.249\tL0 Loss: 0.559\n",
      "249600/649600\tLoss: 85.828\tL0 Loss: 0.559\n",
      "252800/649600\tLoss: 61.601\tL0 Loss: 0.559\n",
      "256000/649600\tLoss: 50.008\tL0 Loss: 0.559\n",
      "259200/649600\tLoss: 47.056\tL0 Loss: 0.559\n",
      "262400/649600\tLoss: 45.649\tL0 Loss: 0.559\n",
      "265600/649600\tLoss: 45.726\tL0 Loss: 0.559\n",
      "268800/649600\tLoss: 44.641\tL0 Loss: 0.559\n",
      "272000/649600\tLoss: 43.768\tL0 Loss: 0.559\n",
      "275200/649600\tLoss: 44.754\tL0 Loss: 0.559\n",
      "278400/649600\tLoss: 44.164\tL0 Loss: 0.559\n",
      "281600/649600\tLoss: 43.093\tL0 Loss: 0.559\n",
      "284800/649600\tLoss: 43.989\tL0 Loss: 0.559\n",
      "288000/649600\tLoss: 47.320\tL0 Loss: 0.559\n",
      "291200/649600\tLoss: 50.632\tL0 Loss: 0.559\n",
      "294400/649600\tLoss: 50.453\tL0 Loss: 0.559\n",
      "297600/649600\tLoss: 51.171\tL0 Loss: 0.559\n",
      "300800/649600\tLoss: 51.779\tL0 Loss: 0.559\n",
      "304000/649600\tLoss: 51.027\tL0 Loss: 0.559\n",
      "307200/649600\tLoss: 50.729\tL0 Loss: 0.559\n",
      "310400/649600\tLoss: 49.578\tL0 Loss: 0.559\n",
      "313600/649600\tLoss: 49.815\tL0 Loss: 0.559\n",
      "316800/649600\tLoss: 51.794\tL0 Loss: 0.559\n",
      "320000/649600\tLoss: 51.056\tL0 Loss: 0.559\n",
      "323200/649600\tLoss: 49.926\tL0 Loss: 0.559\n",
      "326400/649600\tLoss: 49.629\tL0 Loss: 0.559\n",
      "329600/649600\tLoss: 55.292\tL0 Loss: 0.559\n",
      "332800/649600\tLoss: 56.402\tL0 Loss: 0.559\n",
      "336000/649600\tLoss: 58.694\tL0 Loss: 0.559\n",
      "339200/649600\tLoss: 60.469\tL0 Loss: 0.559\n",
      "342400/649600\tLoss: 59.115\tL0 Loss: 0.559\n",
      "345600/649600\tLoss: 61.492\tL0 Loss: 0.559\n",
      "348800/649600\tLoss: 57.449\tL0 Loss: 0.559\n",
      "352000/649600\tLoss: 57.196\tL0 Loss: 0.559\n",
      "355200/649600\tLoss: 59.122\tL0 Loss: 0.559\n",
      "358400/649600\tLoss: 60.135\tL0 Loss: 0.559\n",
      "361600/649600\tLoss: 57.973\tL0 Loss: 0.559\n",
      "364800/649600\tLoss: 58.601\tL0 Loss: 0.559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368000/649600\tLoss: 58.364\tL0 Loss: 0.559\n",
      "371200/649600\tLoss: 61.784\tL0 Loss: 0.559\n",
      "374400/649600\tLoss: 64.604\tL0 Loss: 0.559\n",
      "377600/649600\tLoss: 68.544\tL0 Loss: 0.559\n",
      "380800/649600\tLoss: 66.658\tL0 Loss: 0.559\n",
      "384000/649600\tLoss: 66.178\tL0 Loss: 0.559\n",
      "387200/649600\tLoss: 67.248\tL0 Loss: 0.559\n",
      "390400/649600\tLoss: 62.088\tL0 Loss: 0.559\n",
      "393600/649600\tLoss: 63.181\tL0 Loss: 0.559\n",
      "396800/649600\tLoss: 69.217\tL0 Loss: 0.559\n",
      "400000/649600\tLoss: 65.755\tL0 Loss: 0.559\n",
      "403200/649600\tLoss: 62.690\tL0 Loss: 0.559\n",
      "406400/649600\tLoss: 67.921\tL0 Loss: 0.559\n",
      "409600/649600\tLoss: 63.967\tL0 Loss: 0.559\n",
      "412800/649600\tLoss: 66.178\tL0 Loss: 0.559\n",
      "416000/649600\tLoss: 76.541\tL0 Loss: 0.559\n",
      "419200/649600\tLoss: 77.726\tL0 Loss: 0.559\n",
      "422400/649600\tLoss: 71.323\tL0 Loss: 0.559\n",
      "425600/649600\tLoss: 74.968\tL0 Loss: 0.559\n",
      "428800/649600\tLoss: 74.232\tL0 Loss: 0.559\n",
      "432000/649600\tLoss: 68.474\tL0 Loss: 0.559\n",
      "435200/649600\tLoss: 75.680\tL0 Loss: 0.559\n",
      "438400/649600\tLoss: 81.788\tL0 Loss: 0.559\n",
      "441600/649600\tLoss: 74.127\tL0 Loss: 0.560\n",
      "444800/649600\tLoss: 77.235\tL0 Loss: 0.560\n",
      "448000/649600\tLoss: 76.836\tL0 Loss: 0.560\n",
      "451200/649600\tLoss: 72.145\tL0 Loss: 0.560\n",
      "454400/649600\tLoss: 76.359\tL0 Loss: 0.560\n",
      "457600/649600\tLoss: 87.064\tL0 Loss: 0.560\n",
      "460800/649600\tLoss: 88.749\tL0 Loss: 0.560\n",
      "464000/649600\tLoss: 77.817\tL0 Loss: 0.560\n",
      "467200/649600\tLoss: 85.346\tL0 Loss: 0.560\n",
      "470400/649600\tLoss: 79.097\tL0 Loss: 0.560\n",
      "473600/649600\tLoss: 70.629\tL0 Loss: 0.560\n",
      "476800/649600\tLoss: 82.376\tL0 Loss: 0.560\n",
      "480000/649600\tLoss: 91.423\tL0 Loss: 0.560\n",
      "483200/649600\tLoss: 80.262\tL0 Loss: 0.560\n",
      "486400/649600\tLoss: 80.965\tL0 Loss: 0.560\n",
      "489600/649600\tLoss: 81.544\tL0 Loss: 0.560\n",
      "492800/649600\tLoss: 83.452\tL0 Loss: 0.560\n",
      "496000/649600\tLoss: 69.615\tL0 Loss: 0.560\n",
      "499200/649600\tLoss: 53.124\tL0 Loss: 0.560\n",
      "502400/649600\tLoss: 46.351\tL0 Loss: 0.560\n",
      "505600/649600\tLoss: 42.839\tL0 Loss: 0.560\n",
      "508800/649600\tLoss: 40.617\tL0 Loss: 0.560\n",
      "512000/649600\tLoss: 40.063\tL0 Loss: 0.560\n",
      "515200/649600\tLoss: 40.183\tL0 Loss: 0.560\n",
      "518400/649600\tLoss: 39.821\tL0 Loss: 0.560\n",
      "521600/649600\tLoss: 40.302\tL0 Loss: 0.560\n",
      "524800/649600\tLoss: 39.745\tL0 Loss: 0.560\n",
      "528000/649600\tLoss: 38.903\tL0 Loss: 0.560\n",
      "531200/649600\tLoss: 37.769\tL0 Loss: 0.560\n",
      "534400/649600\tLoss: 42.528\tL0 Loss: 0.560\n",
      "537600/649600\tLoss: 42.486\tL0 Loss: 0.560\n",
      "540800/649600\tLoss: 43.074\tL0 Loss: 0.560\n",
      "544000/649600\tLoss: 43.092\tL0 Loss: 0.560\n",
      "547200/649600\tLoss: 42.967\tL0 Loss: 0.560\n",
      "550400/649600\tLoss: 42.943\tL0 Loss: 0.560\n",
      "553600/649600\tLoss: 41.778\tL0 Loss: 0.560\n",
      "556800/649600\tLoss: 42.551\tL0 Loss: 0.560\n",
      "560000/649600\tLoss: 42.278\tL0 Loss: 0.560\n",
      "563200/649600\tLoss: 43.022\tL0 Loss: 0.560\n",
      "566400/649600\tLoss: 42.673\tL0 Loss: 0.560\n",
      "569600/649600\tLoss: 43.096\tL0 Loss: 0.560\n",
      "572800/649600\tLoss: 41.802\tL0 Loss: 0.560\n",
      "576000/649600\tLoss: 48.552\tL0 Loss: 0.560\n",
      "579200/649600\tLoss: 46.802\tL0 Loss: 0.560\n",
      "582400/649600\tLoss: 48.789\tL0 Loss: 0.560\n",
      "585600/649600\tLoss: 49.064\tL0 Loss: 0.560\n",
      "588800/649600\tLoss: 49.146\tL0 Loss: 0.560\n",
      "592000/649600\tLoss: 47.256\tL0 Loss: 0.560\n",
      "595200/649600\tLoss: 47.470\tL0 Loss: 0.560\n",
      "598400/649600\tLoss: 47.396\tL0 Loss: 0.560\n",
      "601600/649600\tLoss: 47.841\tL0 Loss: 0.560\n",
      "604800/649600\tLoss: 48.547\tL0 Loss: 0.560\n",
      "608000/649600\tLoss: 47.904\tL0 Loss: 0.560\n",
      "611200/649600\tLoss: 48.463\tL0 Loss: 0.560\n",
      "614400/649600\tLoss: 48.480\tL0 Loss: 0.560\n",
      "617600/649600\tLoss: 54.698\tL0 Loss: 0.560\n",
      "620800/649600\tLoss: 53.096\tL0 Loss: 0.560\n",
      "624000/649600\tLoss: 55.481\tL0 Loss: 0.560\n",
      "627200/649600\tLoss: 57.399\tL0 Loss: 0.560\n",
      "630400/649600\tLoss: 54.054\tL0 Loss: 0.560\n",
      "633600/649600\tLoss: 53.649\tL0 Loss: 0.560\n",
      "636800/649600\tLoss: 54.601\tL0 Loss: 0.560\n",
      "640000/649600\tLoss: 54.276\tL0 Loss: 0.560\n",
      "643200/649600\tLoss: 53.927\tL0 Loss: 0.560\n",
      "646400/649600\tLoss: 55.084\tL0 Loss: 0.560\n",
      "Valid Loss: 189.250, Recon Error: 0.018\n",
      "189.25021728725008\n",
      "Epoch: 9 Average loss: 57.61 Valid loss: 189.25021728725008\tRecon Error:0.018\n",
      "0/649600\tLoss: 54.883\tL0 Loss: 0.560\n",
      "3200/649600\tLoss: 45.438\tL0 Loss: 0.560\n",
      "6400/649600\tLoss: 41.281\tL0 Loss: 0.560\n",
      "9600/649600\tLoss: 40.195\tL0 Loss: 0.560\n",
      "12800/649600\tLoss: 40.661\tL0 Loss: 0.560\n",
      "16000/649600\tLoss: 40.340\tL0 Loss: 0.560\n",
      "19200/649600\tLoss: 39.572\tL0 Loss: 0.560\n",
      "22400/649600\tLoss: 39.220\tL0 Loss: 0.560\n",
      "25600/649600\tLoss: 40.364\tL0 Loss: 0.560\n",
      "28800/649600\tLoss: 39.906\tL0 Loss: 0.561\n",
      "32000/649600\tLoss: 39.208\tL0 Loss: 0.561\n",
      "35200/649600\tLoss: 41.459\tL0 Loss: 0.561\n",
      "38400/649600\tLoss: 39.502\tL0 Loss: 0.561\n",
      "41600/649600\tLoss: 42.674\tL0 Loss: 0.561\n",
      "44800/649600\tLoss: 47.562\tL0 Loss: 0.561\n",
      "48000/649600\tLoss: 45.618\tL0 Loss: 0.561\n",
      "51200/649600\tLoss: 45.090\tL0 Loss: 0.561\n",
      "54400/649600\tLoss: 44.775\tL0 Loss: 0.561\n",
      "57600/649600\tLoss: 44.100\tL0 Loss: 0.561\n",
      "60800/649600\tLoss: 46.508\tL0 Loss: 0.561\n",
      "64000/649600\tLoss: 46.444\tL0 Loss: 0.561\n",
      "67200/649600\tLoss: 45.201\tL0 Loss: 0.561\n",
      "70400/649600\tLoss: 43.612\tL0 Loss: 0.561\n",
      "73600/649600\tLoss: 45.078\tL0 Loss: 0.561\n",
      "76800/649600\tLoss: 44.692\tL0 Loss: 0.561\n",
      "80000/649600\tLoss: 45.856\tL0 Loss: 0.561\n",
      "83200/649600\tLoss: 50.058\tL0 Loss: 0.561\n",
      "86400/649600\tLoss: 52.147\tL0 Loss: 0.561\n",
      "89600/649600\tLoss: 50.746\tL0 Loss: 0.561\n",
      "92800/649600\tLoss: 52.674\tL0 Loss: 0.561\n",
      "96000/649600\tLoss: 53.036\tL0 Loss: 0.561\n",
      "99200/649600\tLoss: 55.718\tL0 Loss: 0.561\n",
      "102400/649600\tLoss: 50.638\tL0 Loss: 0.561\n",
      "105600/649600\tLoss: 53.954\tL0 Loss: 0.561\n",
      "108800/649600\tLoss: 51.853\tL0 Loss: 0.561\n",
      "112000/649600\tLoss: 53.867\tL0 Loss: 0.561\n",
      "115200/649600\tLoss: 51.715\tL0 Loss: 0.561\n",
      "118400/649600\tLoss: 54.547\tL0 Loss: 0.561\n",
      "121600/649600\tLoss: 52.913\tL0 Loss: 0.561\n",
      "124800/649600\tLoss: 58.104\tL0 Loss: 0.561\n",
      "128000/649600\tLoss: 60.068\tL0 Loss: 0.561\n",
      "131200/649600\tLoss: 61.568\tL0 Loss: 0.561\n",
      "134400/649600\tLoss: 57.211\tL0 Loss: 0.561\n",
      "137600/649600\tLoss: 60.092\tL0 Loss: 0.561\n",
      "140800/649600\tLoss: 58.522\tL0 Loss: 0.561\n",
      "144000/649600\tLoss: 59.499\tL0 Loss: 0.561\n",
      "147200/649600\tLoss: 55.750\tL0 Loss: 0.561\n",
      "150400/649600\tLoss: 61.320\tL0 Loss: 0.561\n",
      "153600/649600\tLoss: 57.884\tL0 Loss: 0.561\n",
      "156800/649600\tLoss: 57.869\tL0 Loss: 0.561\n",
      "160000/649600\tLoss: 59.731\tL0 Loss: 0.561\n",
      "163200/649600\tLoss: 58.944\tL0 Loss: 0.561\n",
      "166400/649600\tLoss: 68.410\tL0 Loss: 0.561\n",
      "169600/649600\tLoss: 70.315\tL0 Loss: 0.561\n",
      "172800/649600\tLoss: 69.532\tL0 Loss: 0.561\n",
      "176000/649600\tLoss: 64.878\tL0 Loss: 0.561\n",
      "179200/649600\tLoss: 67.560\tL0 Loss: 0.561\n",
      "182400/649600\tLoss: 68.094\tL0 Loss: 0.561\n",
      "185600/649600\tLoss: 65.853\tL0 Loss: 0.561\n",
      "188800/649600\tLoss: 66.175\tL0 Loss: 0.561\n",
      "192000/649600\tLoss: 66.603\tL0 Loss: 0.561\n",
      "195200/649600\tLoss: 64.938\tL0 Loss: 0.561\n",
      "198400/649600\tLoss: 64.159\tL0 Loss: 0.561\n",
      "201600/649600\tLoss: 66.569\tL0 Loss: 0.561\n",
      "204800/649600\tLoss: 68.431\tL0 Loss: 0.561\n",
      "208000/649600\tLoss: 76.054\tL0 Loss: 0.561\n",
      "211200/649600\tLoss: 79.840\tL0 Loss: 0.561\n",
      "214400/649600\tLoss: 78.497\tL0 Loss: 0.561\n",
      "217600/649600\tLoss: 73.606\tL0 Loss: 0.561\n",
      "220800/649600\tLoss: 79.042\tL0 Loss: 0.561\n",
      "224000/649600\tLoss: 76.587\tL0 Loss: 0.561\n",
      "227200/649600\tLoss: 75.985\tL0 Loss: 0.561\n",
      "230400/649600\tLoss: 75.117\tL0 Loss: 0.561\n",
      "233600/649600\tLoss: 76.236\tL0 Loss: 0.562\n",
      "236800/649600\tLoss: 76.180\tL0 Loss: 0.562\n",
      "240000/649600\tLoss: 77.175\tL0 Loss: 0.562\n",
      "243200/649600\tLoss: 77.269\tL0 Loss: 0.562\n",
      "246400/649600\tLoss: 85.398\tL0 Loss: 0.562\n",
      "249600/649600\tLoss: 90.884\tL0 Loss: 0.562\n",
      "252800/649600\tLoss: 67.350\tL0 Loss: 0.562\n",
      "256000/649600\tLoss: 50.649\tL0 Loss: 0.562\n",
      "259200/649600\tLoss: 45.260\tL0 Loss: 0.562\n",
      "262400/649600\tLoss: 47.695\tL0 Loss: 0.562\n",
      "265600/649600\tLoss: 45.915\tL0 Loss: 0.562\n",
      "268800/649600\tLoss: 44.510\tL0 Loss: 0.562\n",
      "272000/649600\tLoss: 43.031\tL0 Loss: 0.562\n",
      "275200/649600\tLoss: 43.509\tL0 Loss: 0.562\n",
      "278400/649600\tLoss: 42.390\tL0 Loss: 0.562\n",
      "281600/649600\tLoss: 43.519\tL0 Loss: 0.562\n",
      "284800/649600\tLoss: 42.642\tL0 Loss: 0.562\n",
      "288000/649600\tLoss: 49.092\tL0 Loss: 0.562\n",
      "291200/649600\tLoss: 48.666\tL0 Loss: 0.562\n",
      "294400/649600\tLoss: 50.708\tL0 Loss: 0.562\n",
      "297600/649600\tLoss: 49.983\tL0 Loss: 0.562\n",
      "300800/649600\tLoss: 50.786\tL0 Loss: 0.562\n",
      "304000/649600\tLoss: 49.621\tL0 Loss: 0.562\n",
      "307200/649600\tLoss: 49.160\tL0 Loss: 0.562\n",
      "310400/649600\tLoss: 48.786\tL0 Loss: 0.562\n",
      "313600/649600\tLoss: 49.274\tL0 Loss: 0.562\n",
      "316800/649600\tLoss: 49.768\tL0 Loss: 0.562\n",
      "320000/649600\tLoss: 49.370\tL0 Loss: 0.562\n",
      "323200/649600\tLoss: 48.847\tL0 Loss: 0.562\n",
      "326400/649600\tLoss: 48.921\tL0 Loss: 0.562\n",
      "329600/649600\tLoss: 53.106\tL0 Loss: 0.562\n",
      "332800/649600\tLoss: 54.347\tL0 Loss: 0.562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336000/649600\tLoss: 59.264\tL0 Loss: 0.562\n",
      "339200/649600\tLoss: 57.294\tL0 Loss: 0.562\n",
      "342400/649600\tLoss: 58.793\tL0 Loss: 0.562\n",
      "345600/649600\tLoss: 57.585\tL0 Loss: 0.562\n",
      "348800/649600\tLoss: 56.605\tL0 Loss: 0.562\n",
      "352000/649600\tLoss: 55.271\tL0 Loss: 0.562\n",
      "355200/649600\tLoss: 59.230\tL0 Loss: 0.562\n",
      "358400/649600\tLoss: 57.400\tL0 Loss: 0.562\n",
      "361600/649600\tLoss: 55.434\tL0 Loss: 0.562\n",
      "364800/649600\tLoss: 57.303\tL0 Loss: 0.562\n",
      "368000/649600\tLoss: 57.021\tL0 Loss: 0.562\n",
      "371200/649600\tLoss: 59.359\tL0 Loss: 0.562\n",
      "374400/649600\tLoss: 62.100\tL0 Loss: 0.562\n",
      "377600/649600\tLoss: 66.646\tL0 Loss: 0.562\n",
      "380800/649600\tLoss: 61.507\tL0 Loss: 0.562\n",
      "384000/649600\tLoss: 62.477\tL0 Loss: 0.562\n",
      "387200/649600\tLoss: 65.907\tL0 Loss: 0.562\n",
      "390400/649600\tLoss: 61.931\tL0 Loss: 0.562\n",
      "393600/649600\tLoss: 61.313\tL0 Loss: 0.562\n",
      "396800/649600\tLoss: 67.879\tL0 Loss: 0.562\n",
      "400000/649600\tLoss: 64.049\tL0 Loss: 0.562\n",
      "403200/649600\tLoss: 59.545\tL0 Loss: 0.562\n",
      "406400/649600\tLoss: 64.960\tL0 Loss: 0.562\n",
      "409600/649600\tLoss: 62.221\tL0 Loss: 0.562\n",
      "412800/649600\tLoss: 61.663\tL0 Loss: 0.562\n",
      "416000/649600\tLoss: 71.500\tL0 Loss: 0.562\n",
      "419200/649600\tLoss: 77.782\tL0 Loss: 0.562\n",
      "422400/649600\tLoss: 69.765\tL0 Loss: 0.562\n",
      "425600/649600\tLoss: 68.355\tL0 Loss: 0.562\n",
      "428800/649600\tLoss: 68.166\tL0 Loss: 0.562\n",
      "432000/649600\tLoss: 62.039\tL0 Loss: 0.562\n",
      "435200/649600\tLoss: 68.042\tL0 Loss: 0.562\n",
      "438400/649600\tLoss: 75.651\tL0 Loss: 0.562\n",
      "441600/649600\tLoss: 73.842\tL0 Loss: 0.562\n",
      "444800/649600\tLoss: 67.510\tL0 Loss: 0.563\n",
      "448000/649600\tLoss: 69.262\tL0 Loss: 0.563\n",
      "451200/649600\tLoss: 65.791\tL0 Loss: 0.563\n",
      "454400/649600\tLoss: 68.936\tL0 Loss: 0.563\n",
      "457600/649600\tLoss: 77.671\tL0 Loss: 0.563\n",
      "460800/649600\tLoss: 82.931\tL0 Loss: 0.563\n",
      "464000/649600\tLoss: 80.984\tL0 Loss: 0.563\n",
      "467200/649600\tLoss: 74.294\tL0 Loss: 0.563\n",
      "470400/649600\tLoss: 77.757\tL0 Loss: 0.563\n",
      "473600/649600\tLoss: 67.423\tL0 Loss: 0.563\n",
      "476800/649600\tLoss: 73.343\tL0 Loss: 0.563\n",
      "480000/649600\tLoss: 82.993\tL0 Loss: 0.563\n",
      "483200/649600\tLoss: 79.326\tL0 Loss: 0.563\n",
      "486400/649600\tLoss: 73.278\tL0 Loss: 0.563\n",
      "489600/649600\tLoss: 74.324\tL0 Loss: 0.563\n",
      "492800/649600\tLoss: 84.112\tL0 Loss: 0.563\n",
      "496000/649600\tLoss: 66.905\tL0 Loss: 0.563\n",
      "499200/649600\tLoss: 51.713\tL0 Loss: 0.563\n",
      "502400/649600\tLoss: 46.897\tL0 Loss: 0.563\n",
      "505600/649600\tLoss: 43.253\tL0 Loss: 0.563\n",
      "508800/649600\tLoss: 41.408\tL0 Loss: 0.563\n",
      "512000/649600\tLoss: 39.705\tL0 Loss: 0.563\n",
      "515200/649600\tLoss: 37.956\tL0 Loss: 0.563\n",
      "518400/649600\tLoss: 38.823\tL0 Loss: 0.563\n",
      "521600/649600\tLoss: 38.890\tL0 Loss: 0.563\n",
      "524800/649600\tLoss: 38.223\tL0 Loss: 0.563\n",
      "528000/649600\tLoss: 38.554\tL0 Loss: 0.563\n",
      "531200/649600\tLoss: 38.059\tL0 Loss: 0.563\n",
      "534400/649600\tLoss: 42.812\tL0 Loss: 0.563\n",
      "537600/649600\tLoss: 41.992\tL0 Loss: 0.563\n",
      "540800/649600\tLoss: 41.993\tL0 Loss: 0.563\n",
      "544000/649600\tLoss: 42.751\tL0 Loss: 0.563\n",
      "547200/649600\tLoss: 42.589\tL0 Loss: 0.563\n",
      "550400/649600\tLoss: 42.024\tL0 Loss: 0.563\n",
      "553600/649600\tLoss: 42.265\tL0 Loss: 0.563\n",
      "556800/649600\tLoss: 41.306\tL0 Loss: 0.563\n",
      "560000/649600\tLoss: 41.882\tL0 Loss: 0.563\n",
      "563200/649600\tLoss: 41.973\tL0 Loss: 0.563\n",
      "566400/649600\tLoss: 42.266\tL0 Loss: 0.563\n",
      "569600/649600\tLoss: 42.518\tL0 Loss: 0.563\n",
      "572800/649600\tLoss: 41.050\tL0 Loss: 0.563\n",
      "576000/649600\tLoss: 47.945\tL0 Loss: 0.563\n",
      "579200/649600\tLoss: 47.428\tL0 Loss: 0.563\n",
      "582400/649600\tLoss: 48.040\tL0 Loss: 0.563\n",
      "585600/649600\tLoss: 49.160\tL0 Loss: 0.563\n",
      "588800/649600\tLoss: 48.453\tL0 Loss: 0.563\n",
      "592000/649600\tLoss: 47.650\tL0 Loss: 0.563\n",
      "595200/649600\tLoss: 46.854\tL0 Loss: 0.563\n",
      "598400/649600\tLoss: 46.645\tL0 Loss: 0.563\n",
      "601600/649600\tLoss: 49.240\tL0 Loss: 0.563\n",
      "604800/649600\tLoss: 48.105\tL0 Loss: 0.563\n",
      "608000/649600\tLoss: 47.356\tL0 Loss: 0.563\n",
      "611200/649600\tLoss: 47.628\tL0 Loss: 0.563\n",
      "614400/649600\tLoss: 46.435\tL0 Loss: 0.563\n",
      "617600/649600\tLoss: 55.316\tL0 Loss: 0.563\n",
      "620800/649600\tLoss: 54.635\tL0 Loss: 0.563\n",
      "624000/649600\tLoss: 56.568\tL0 Loss: 0.563\n",
      "627200/649600\tLoss: 55.708\tL0 Loss: 0.563\n",
      "630400/649600\tLoss: 54.954\tL0 Loss: 0.563\n",
      "633600/649600\tLoss: 53.103\tL0 Loss: 0.563\n",
      "636800/649600\tLoss: 53.587\tL0 Loss: 0.563\n",
      "640000/649600\tLoss: 53.626\tL0 Loss: 0.563\n",
      "643200/649600\tLoss: 53.832\tL0 Loss: 0.563\n",
      "646400/649600\tLoss: 53.200\tL0 Loss: 0.563\n",
      "Valid Loss: 198.416, Recon Error: 0.018\n",
      "198.4155100827356\n",
      "Epoch: 10 Average loss: 56.14 Valid loss: 198.4155100827356\tRecon Error:0.018\n",
      "1818.629601240158\n"
     ]
    }
   ],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "###1e-5 6859 1e-4 6727 5e-4 6722 try tanh/L1 loss/beta--->DIP\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "trainer.train(train_loader,valid_loader, epochs=10, save_training_gif=('./training.gif', viz))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "torch.save(trainer.best_model.state_dict(), 'model_params.pkl')\n",
    "torch.save(trainer.best_model, './model')\n",
    "##15.078 - 0.0147  17.209 - 0.0168 error tanh \n",
    "##LR 1e-3 0.019-0.023 worse should pick 5e-4\n",
    "##PLOT THE CURVE!!!!!\n",
    "###3360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_spec=latent_spec, img_size=(1, 64, 64)).cuda()\n",
    "model.load_state_dict(torch.load('model_params.pkl'))\n",
    "#path=\"figures/face/cont_{}/pruned_Beta_ {}lamba{}_ONLYPAIR\".format(n_cont,gamma,0.1)\n",
    "loss = trainer.get_losses()\n",
    "# print(len(loss[\"DIP_loss\"]))\n",
    "# print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch, labels in test_loader:\n",
    "    break\n",
    "batch = torch.unsqueeze(batch,1).cuda().to(dtype=torch.float32)\n",
    "latent, mask, reg = model.encode(batch)\n",
    "model.reparameterize(latent).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0625 12:19:14.909640 140675687122688 deprecation_wrapper.py:119] From /data/anaconda/envs/mli/lib/python3.6/site-packages/gin/tf/utils.py:34: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0625 12:19:14.910890 140675687122688 deprecation_wrapper.py:119] From /data/anaconda/envs/mli/lib/python3.6/site-packages/gin/tf/utils.py:34: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "W0625 12:19:14.911525 140675687122688 deprecation_wrapper.py:119] From /data/anaconda/envs/mli/lib/python3.6/site-packages/gin/tf/utils.py:43: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'randint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-75f9783b900e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mbeta_scores_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_beta_vae_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepresentation_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mfactor_scores_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactor_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_factor_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/disentanglement_lib/evaluation/metrics/factor_vae.py\u001b[0m in \u001b[0;36mcompute_factor_vae\u001b[0;34m(ground_truth_data, representation_function, random_state, batch_size, num_train, num_eval, num_variance_estimate)\u001b[0m\n\u001b[1;32m     57\u001b[0m   global_variances = _compute_variances(ground_truth_data,\n\u001b[1;32m     58\u001b[0m                                         \u001b[0mrepresentation_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                                         num_variance_estimate, random_state)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0mactive_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prune_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_variances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0mscores_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/disentanglement_lib/evaluation/metrics/factor_vae.py\u001b[0m in \u001b[0;36m_compute_variances\u001b[0;34m(ground_truth_data, representation_function, batch_size, random_state)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mVector\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m   \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m   \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mground_truth_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m   \u001b[0mrepresentations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/disentanglement_lib/data/ground_truth/ground_truth_data.py\u001b[0m in \u001b[0;36msample_observations\u001b[0;34m(self, num, random_state)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msample_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;34m\"\"\"Sample a batch of observations X.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/disentanglement_lib/data/ground_truth/ground_truth_data.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num, random_state)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m\"\"\"Sample a batch of factors Y and observations X.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_observations_from_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/disentanglement_lib/data/ground_truth/dsprites.py\u001b[0m in \u001b[0;36msample_factors\u001b[0;34m(self, num, random_state)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msample_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m\"\"\"Sample a batch of factors Y.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_latent_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msample_observations_from_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/disentanglement_lib/data/ground_truth/util.py\u001b[0m in \u001b[0;36msample_latent_factors\u001b[0;34m(self, num, random_state)\u001b[0m\n\u001b[1;32m     57\u001b[0m         shape=(num, len(self.latent_factor_indices)), dtype=np.int64)\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_factor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/disentanglement_lib/data/ground_truth/util.py\u001b[0m in \u001b[0;36m_sample_factor\u001b[0;34m(self, i, num, random_state)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_sample_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactor_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'randint'"
     ]
    }
   ],
   "source": [
    "from disentanglement_lib.evaluation.metrics import beta_vae, dci, factor_vae, mig, modularity_explicitness, sap_score\n",
    "from disentanglement_lib.data.ground_truth import dsprites\n",
    "dataset = dsprites.DSprites()\n",
    "##########\n",
    "def representation_fn(x, mean = True):\n",
    "    x = torch.tensor(x).cuda().to(dtype=torch.float32)\n",
    "    model = VAE(latent_spec = latent_spec, img_size=(1, 64, 64)).cuda()\n",
    "    model.load_state_dict(torch.load('model_params.pkl'))\n",
    "    latent_dict,mask,regularization = model.encode(x)\n",
    "\n",
    "    if mean:\n",
    "        return latent[\"cont\"][0][0]\n",
    "    else:\n",
    "        return model.reparameterize(latent_dict).cuda()  \n",
    "    \n",
    "    \n",
    "######\n",
    "beta_scores_dict = beta_vae.compute_beta_vae_sklearn(ground_truth_data = dataset,representation_function = representation_fn)\n",
    "factor_scores_dict = factor_vae.compute_factor_vae(ground_truth_data = dataset, representation_function = representation_fn)\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta_scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.cuda.is_available()\n",
    "# device = torch.device('cuda')\n",
    "# print(device)\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Chi-square test\n",
    "import torch\n",
    "tensor_one = torch.tensor([[1,2,3],[4,5,6]])\n",
    "tensor_two = torch.tensor([[6,8,9],[10,11,12]])\n",
    "tensor_list = [tensor_one, tensor_two]\n",
    "tens_list = []\n",
    "for tensor in tensor_list:\n",
    "    \n",
    "    print(tensor)\n",
    "    length = tensor.shape[1]\n",
    "    tens_list.append(torch.mean(tensor.float(),dim=0))\n",
    "    \n",
    "tens_list = torch.stack(tens_list).reshape(1,-1)\n",
    "tens_listT = tens_list.t()\n",
    "matrix = tens_listT.matmul(tens_list)\n",
    "print(matrix)\n",
    "print(\"--------\")\n",
    "Chi2 =0\n",
    "for i in range(len(tensor_list)):\n",
    "    for j in range(len(tensor_list)):\n",
    "        if i > j:\n",
    "            submatrix = matrix[j*length:(j+1)*length,i*length:(i+1)*length]\n",
    "            c_sum = torch.sum(submatrix,dim=0).reshape(-1,1)\n",
    "            \n",
    "            r_sum = torch.sum(submatrix,dim=1).reshape(1,-1)\n",
    "            all_sum = torch.sum(submatrix)\n",
    "            Expectation = c_sum.matmul(r_sum)/all_sum\n",
    "            print(all_sum,c_sum,r_sum,Expectation)\n",
    "            Chi2 += torch.sum((submatrix-Expectation)**2/Expectation)\n",
    "            \n",
    "        \n",
    "print(Chi2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n",
      "0.9972734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc33b07fc18>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot reconstructions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "# Get a batch of data\n",
    "for batch, labels in test_loader:\n",
    "    break\n",
    "    \n",
    "#get best model,easrly stopping\n",
    "viz = Visualizer(model)\n",
    "\n",
    "# Reconstruct data using Joint-VAE model\n",
    "recon = viz.reconstructions(torch.unsqueeze(batch,1).cuda().to(dtype=torch.float32))\n",
    "\n",
    "# face\n",
    "recon=np.rollaxis(recon.numpy(), 0, 3)  \n",
    "print(recon[265:,:,:].max())\n",
    "recon[:,:,:]=(recon[:,:,:]+1)/2\n",
    "plt.imshow(recon[:,:,:].astype(float),cmap=\"gray\")\n",
    "\n",
    "#MNIST\n",
    "# plt.imshow(recon.numpy()[0, :, :].astype(float), cmap='gray')\n",
    "#plt.savefig(path+\"/recon.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_batch = iter(test_loader)\n",
    "test_batch = next(test_batch)\n",
    "latent_dist,mask,_ = model.encode(torch.unsqueeze(test_batch[0],1).cuda().to(dtype=torch.float32))\n",
    "\n",
    "print(mask,len(torch.nonzero(mask[0]==0)))\n",
    "\n",
    "# for latent in latent_dist['cont'][0]:\n",
    "#     count=torch.zeros((1,32))\n",
    "#     latent[latent<1e-7]=0\n",
    "    \n",
    "#     for i in range(128):\n",
    "    \n",
    "#     #print(latent[i].size(),torch.nonzero(latent[i]))#len(torch.nonzero(latent[0]==0))\n",
    "#         count[latent[i].reshape(1,32)!=0] += 1\n",
    "#     print(count)\n",
    "def show_idx(mask):\n",
    "    a = mask.cpu().detach().numpy().squeeze()\n",
    "    return np.array(np.where(a==1))+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCR():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist, mask, reg = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont'][0]\n",
    "        cov = covmatrix(mean)\n",
    "        cov[torch.abs(cov)<=1e-6]=0\n",
    "        cor = cov2cor(cov)\n",
    "        totalc += np.sum(cor) \n",
    "\n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "def TCV():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist,mask, reg = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont'][0]\n",
    "        cov = covmatrix(mean).cpu().detach().numpy()\n",
    "        cov = cov-np.diag(np.diag(cov))\n",
    "        totalc += np.sum(cov**2) \n",
    "        \n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "\n",
    "def covmatrix(mean):\n",
    "    exp_mu = torch.mean(mean, dim=0)  #####mean through batch\n",
    "\n",
    "    # expectation of mu mu.tranpose\n",
    "    mu_expand1 = mean.unsqueeze(1)  #####(batch_size, 1, number of mean of latent variables)\n",
    "    mu_expand2 = mean.unsqueeze(2)  #####(batch_size, number of mean of latent variables, 1) ignore batch_size, only transpose the means\n",
    "    exp_mu_mu_t = torch.mean(mu_expand1 * mu_expand2, dim=0)\n",
    "\n",
    "    # covariance of model mean\n",
    "    cov = exp_mu_mu_t - exp_mu.unsqueeze(0) * exp_mu.unsqueeze(1) \n",
    "    return cov\n",
    "def cov2cor(c):\n",
    "    #input batch * n_cont\n",
    "    c = c.cpu().detach()\n",
    "    d=np.zeros_like(c)\n",
    "    for i in range(c.shape[0]):\n",
    "        for j in range(c.shape[1]):\n",
    "            d[i,j]=c[i,j]/(np.sqrt(c[i,i]*c[j,j]+1e-10))\n",
    "    return d\n",
    "tcor=TCR()\n",
    "tcov=TCV()\n",
    "print(tcor,tcov)\n",
    "trainer.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###latent space T-SNE visualization\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "samples = torch.zeros(1)\n",
    "labels = torch.zeros(1)\n",
    "for i in range(10):\n",
    "    test_batch = iter(test_loader)\n",
    "    test_batch = next(test_batch)\n",
    "    new_labels =torch.tensor(test_batch[1])\n",
    "    latent_dist,_ ,_= model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "    new_samples = model.reparameterize(latent_dist)\n",
    "    if torch.sum(samples) == 0:\n",
    "        samples =new_samples\n",
    "        labels = new_labels\n",
    "    else:\n",
    "        samples = torch.cat((samples,new_samples),0)\n",
    "        labels = torch.cat((labels, new_labels),0)\n",
    "    #print(samples.shape)\n",
    "    \n",
    "##latent_varibales should be N,D--->N,2\n",
    "\n",
    "\n",
    "# latent_variables = samples.reshape(samples[0],-1)\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "tsne.fit_transform(samples.detach().cpu().numpy())\n",
    "\n",
    "plt.scatter(tsne.embedding_[:,0],tsne.embedding_[:,1])\n",
    "#plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 10 # Number of labels\n",
    "\n",
    "# setup the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "# define the data\n",
    "x = tsne.embedding_[:,0]\n",
    "y = tsne.embedding_[:,1]\n",
    "tag = labels# Tag each point with a corresponding label    \n",
    "\n",
    "# define the colormap\n",
    "cmap = plt.cm.jet\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# create the new map\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0,N,N+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# make the scatter\n",
    "scat = ax.scatter(x,y,c=tag,s=np.random.randint(100,110,N),cmap=cmap,     norm=norm)\n",
    "# create the colorbar\n",
    "cb = plt.colorbar(scat, spacing='proportional',ticks=bounds)\n",
    "cb.set_label('Custom cbar')\n",
    "ax.set_title('Discrete color mappings')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "plt.savefig(path+\"/scatter.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t-SNE demo\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "X = np.arange(40).reshape(5,4,2)\n",
    "\n",
    "X_new = X.reshape(5,-1)\n",
    "#X = np.array([[[0,0], [0,0], [0,0]], [[0,0], [0,1], [1,1]], [[1,1], [1,0], [0,1]], [[1,1], [1,1], [1,1]]])\n",
    "print(X.shape,X)\n",
    "print(\"--------\")\n",
    "print(X_new)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne.fit_transform(X)\n",
    "print(tsne.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot samples\n",
    "\n",
    "samples = viz.samples()\n",
    "plt.imshow(samples.numpy()[0, :174, :], cmap='gray')\n",
    "print(np.sum(samples.numpy()[0, :174, :]))\n",
    "print(samples.numpy()[0, :, :].shape)\n",
    "####origin\n",
    "4*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot samples\n",
    "import matplotlib as mpl\n",
    "\n",
    "#MNIST\n",
    "# samples = viz.samples()\n",
    "# sample=samples.numpy()[0, :, :]/2+0.5\n",
    "# plt.imshow(sample, cmap='gray')\n",
    "# plt.imsave(path+\"/samples\",samples.numpy()[0, :, :]/2+0.5, cmap='gray')\n",
    "\n",
    "# face\n",
    "fig = plt.figure(figsize=(50, 50)) \n",
    "samples = viz.samples()\n",
    "samples = np.rollaxis(samples.numpy(), 0, 3)  \n",
    "print(samples[:,:,0].max())\n",
    "samples=(samples+1)/2\n",
    "plt.imshow(samples.astype(float),norm = norm)\n",
    "plt.imsave(path+\"/samples\",samples)\n",
    "###DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot all traversals\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "traversals = viz.all_latent_traversals(size=10)\n",
    "\n",
    "#MNIST\n",
    "# plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "# plt.imsave(path+\"/all_traversals\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "#face\n",
    "traversals = np.rollaxis(traversals.numpy(), 0, 3)  \n",
    "traversals=(traversals+1)/2\n",
    "plt.imshow(traversals)\n",
    "plt.imsave(path+\"/all_traversals\",traversals)\n",
    "###dip[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
    "#         0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=5, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "#MNIST\n",
    "# plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "# plt.imsave(path+\"/contVSdisc\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "traversals.numpy()[0, :, :].max()\n",
    "show_idx(mask)\n",
    "#face\n",
    "traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "traversals=(traversals+1)/2\n",
    "plt.imshow(traversals)\n",
    "plt.imsave(path+\"/contVSdisc\",traversals)\n",
    "##origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_t = viz.all_latent_traversals()\n",
    "print(all_t.shape)\n",
    "plt.imshow(all_t.numpy()[0, :, :], cmap='gray')\n",
    "plt.imsave(\"figures/beta/all_\",traversals.numpy()[0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "# Plot a grid of some traversals\n",
    "\n",
    "fig = plt.figure(figsize=(70, 70))  # width, height in inches\n",
    "print(\"continuous\")\n",
    "for i in range(n_cont):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=i, disc_idx=None,size=12)\n",
    "    \n",
    "    #MNIST\n",
    "#     sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "#     plt.savefig(path+\"/cont{}.png\".format(i))\n",
    "#     plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "    \n",
    "    #FACE\n",
    "    traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "    sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "    traversals=(traversals+1)/2\n",
    "    plt.imshow(traversals)   \n",
    "plt.savefig(path+\"/cont.png\")\n",
    "\n",
    "show_idx(mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"discrete\")\n",
    "for i in range(n_disc):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=None, disc_idx=i,size=10)\n",
    "    ##MNIST\n",
    "#     sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "#     plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "#     plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "    #FACE\n",
    "    traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "    sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "traversals=(traversals+1)/2\n",
    "plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "plt.imshow(traversals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "    \n",
    "# face    \n",
    "def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "        # Generate latent traversal\n",
    "#         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "#                                                              disc_idx=disc_idx,\n",
    "#                                                              size=size)\n",
    "        dim = n_cont + sum(disc)\n",
    "        if prior:\n",
    "            latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "        else:\n",
    "            latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "        latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "        latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "        # Map samples through decoder\n",
    "        generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "        generated  = np.rollaxis(generated.detach().numpy(), 0, 3)\n",
    "        generated = (generated +1)/2\n",
    "        print(generated.min(),generated.max())\n",
    "        plt.imshow(generated)\n",
    "\n",
    "        \n",
    "def decode_latents(model, latent_samples):\n",
    "\n",
    "        latent_samples = Variable(latent_samples)\n",
    "        if model.use_cuda:\n",
    "            latent_samples = latent_samples.cuda()\n",
    "            result = model.decode(latent_samples).cpu()\n",
    "        return result\n",
    "\n",
    "#MNIST\n",
    "# def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "#        # Generate latent traversal\n",
    "#         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "#                                                              disc_idx=disc_idx,\n",
    "#                                                              size=size)\n",
    "#         dim = n_cont + sum(disc)\n",
    "#         if prior:\n",
    "#             latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "#         else:\n",
    "#             latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "#         latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "#         latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "#         # Map samples through decoder\n",
    "#         generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "#         plt.imshow(generated.detach().numpy(),cmap=\"gray\")\n",
    "\n",
    "        \n",
    "# def decode_latents(model, latent_samples):\n",
    "\n",
    "#         latent_samples = Variable(latent_samples)\n",
    "#         if model.use_cuda:\n",
    "#             latent_samples = latent_samples.cuda()\n",
    "#         return model.decode(latent_samples).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "def interactive_view(model,n_cont,disc):\n",
    "    \n",
    "    \n",
    "    interact(single_traversal,model=fixed(model),\n",
    "             n_cont=fixed(n_cont), cont_idx=(0,n_cont,1), cont_v=(-2.5,2.5,0.5),\n",
    "             disc=fixed(disc),disc_idx=(0,9,1),\n",
    "             prior=True);\n",
    "             \n",
    "interactive_view(model,n_cont,disc)\n",
    "show_idx(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
