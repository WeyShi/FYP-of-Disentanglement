{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "IMAGE_PATH = 'img_align_celeba/'\n",
    "image_size = 64\n",
    "# SAMPLE_PATH = '../'\n",
    "\n",
    "# if not os.path.exists(SAMPLE_PATH):\n",
    "#     os.makedirs(SAMPLE_PATH)\n",
    "    \n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    #transforms.Scale(image_size),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop((image_size,image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "data_loader = ImageFolder(IMAGE_PATH, transform)\n",
    "\n",
    "\n",
    "#data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "valid_loader, train_loader, test_loader = get_celeba_dataloader(data_loader, \n",
    "                                                                batch_size=128)\n",
    "test_batch = iter(test_loader)\n",
    "test_batch = next(test_batch)\n",
    "new_labels =torch.tensor(test_batch[1])\n",
    "print(torch.tensor(test_batch[0]).shape)\n",
    "#latent_dist = model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "limit_a, limit_b, epsilon = -.5, 1.5, 1e-6\n",
    "eps = np.linspace(0.1,0.9,20)\n",
    "def sigmoid(x):\n",
    "    y = 1./(1.+np.exp(-x))\n",
    "    return y\n",
    "def quantile_concrete(x,temperature,qz_loga):\n",
    "        \n",
    "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
    "        y = sigmoid((np.log(x) - np.log(1 - x) + qz_loga) / temperature)\n",
    "        return y * (limit_b - limit_a) + limit_a\n",
    "\n",
    "z = quantile_concrete(eps,1/20,2)\n",
    "z[z>=1]=1\n",
    "z[z<=0]=0\n",
    "print(z)\n",
    "plt.plot(eps,z)\n",
    "droprate_init = 0.2\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.zeros(4), requires_grad=False).data.normal_(math.log(1 - droprate_init) - math.log(droprate_init), 1e-2)\n",
    "z=(x+y).detach()\n",
    "print(y)\n",
    "#z.is_leaf \n",
    "a=torch.ones((64,32))\n",
    "b=torch.ones((1,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (img_to_features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (features_to_hidden): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_mean): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc_log_var): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc_alphas): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (latent_to_features): Sequential(\n",
      "    (0): Linear(in_features=74, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (features_to_img): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader \n",
    "from torchvision import transforms \n",
    "from torchvision.datasets import ImageFolder \n",
    "from torch.utils.data import DataLoader \n",
    "import os \n",
    "import torch\n",
    "from jointvae.VAEmodel import VAE\n",
    "from jointvae.training_l import Trainer\n",
    "from torch import optim\n",
    "from viz.visualize_l import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "valid_loader, train_loader, test_loader = get_mnist_dataloaders(batch_size=64)\n",
    "\n",
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions  7-14\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "n_cont = 64\n",
    "disc = [10]\n",
    "n_disc = len(disc)\n",
    "latent_spec = {'cont': n_cont,\n",
    "               'disc': disc}\n",
    "\n",
    "device = torch.device('cuda')\n",
    "#model = VAE(latent_spec=latent_spec, img_size=(3, 64, 64)).cuda()\n",
    "model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32)).cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "lr=5e-4\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "gamma=1.0\n",
    "cont_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "\n",
    "lambda_d = 2\n",
    "lambda_od = 10*lambda_d\n",
    "lambda_dis = 30*lambda_d \n",
    "path=\"ReportFig/DIP-VAE/cont_{}/gamma_ {}lambda{}\".format(n_cont,gamma,lambda_d)\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity,lambda_d = lambda_d,\n",
    "                  lambda_od = lambda_od, lambda_dis = lambda_dis )\n",
    "# Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from jointvae.training import Trainer\n",
    "\n",
    "\n",
    "trainer._train_epoch(train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/54000\tLoss: 400.819\n",
      "3200/54000\tLoss: 273.008\n",
      "6400/54000\tLoss: 193.183\n",
      "9600/54000\tLoss: 186.269\n",
      "12800/54000\tLoss: 184.589\n",
      "16000/54000\tLoss: 182.794\n",
      "19200/54000\tLoss: 180.650\n",
      "22400/54000\tLoss: 179.909\n",
      "25600/54000\tLoss: 178.248\n",
      "28800/54000\tLoss: 175.637\n",
      "32000/54000\tLoss: 173.021\n",
      "35200/54000\tLoss: 170.381\n",
      "38400/54000\tLoss: 167.937\n",
      "41600/54000\tLoss: 166.413\n",
      "44800/54000\tLoss: 164.273\n",
      "48000/54000\tLoss: 164.253\n",
      "51200/54000\tLoss: 162.929\n",
      "Valid Loss: 162.675, Recon Error: 0.038\n",
      "162.67484608102353\n",
      "Epoch: 1 Average loss: 180.83 Valid loss: 162.67484608102353\tRecon Error:0.038\n",
      "0/54000\tLoss: 158.420\n",
      "3200/54000\tLoss: 161.751\n",
      "6400/54000\tLoss: 160.947\n",
      "9600/54000\tLoss: 160.648\n",
      "12800/54000\tLoss: 159.250\n",
      "16000/54000\tLoss: 158.221\n",
      "19200/54000\tLoss: 157.968\n",
      "22400/54000\tLoss: 157.790\n",
      "25600/54000\tLoss: 157.526\n",
      "28800/54000\tLoss: 157.055\n",
      "32000/54000\tLoss: 155.820\n",
      "35200/54000\tLoss: 156.109\n",
      "38400/54000\tLoss: 155.641\n",
      "41600/54000\tLoss: 155.151\n",
      "44800/54000\tLoss: 155.618\n",
      "48000/54000\tLoss: 155.082\n",
      "51200/54000\tLoss: 155.086\n",
      "Valid Loss: 154.445, Recon Error: 0.030\n",
      "154.44504449722615\n",
      "Epoch: 2 Average loss: 157.40 Valid loss: 154.44504449722615\tRecon Error:0.030\n",
      "0/54000\tLoss: 156.730\n",
      "3200/54000\tLoss: 154.987\n",
      "6400/54000\tLoss: 154.643\n",
      "9600/54000\tLoss: 154.235\n",
      "12800/54000\tLoss: 154.263\n",
      "16000/54000\tLoss: 153.846\n",
      "19200/54000\tLoss: 153.107\n",
      "22400/54000\tLoss: 153.053\n",
      "25600/54000\tLoss: 153.153\n",
      "28800/54000\tLoss: 153.483\n",
      "32000/54000\tLoss: 152.937\n",
      "35200/54000\tLoss: 153.211\n",
      "38400/54000\tLoss: 152.648\n",
      "41600/54000\tLoss: 152.474\n",
      "44800/54000\tLoss: 152.660\n",
      "48000/54000\tLoss: 152.727\n",
      "51200/54000\tLoss: 152.323\n",
      "Valid Loss: 152.256, Recon Error: 0.023\n",
      "152.25586327086103\n",
      "Epoch: 3 Average loss: 153.36 Valid loss: 152.25586327086103\tRecon Error:0.023\n",
      "0/54000\tLoss: 151.379\n",
      "3200/54000\tLoss: 151.933\n",
      "6400/54000\tLoss: 151.781\n",
      "9600/54000\tLoss: 151.913\n",
      "12800/54000\tLoss: 151.958\n",
      "16000/54000\tLoss: 151.929\n",
      "19200/54000\tLoss: 151.835\n",
      "22400/54000\tLoss: 151.035\n",
      "25600/54000\tLoss: 151.321\n",
      "28800/54000\tLoss: 151.529\n",
      "32000/54000\tLoss: 151.313\n",
      "35200/54000\tLoss: 151.513\n",
      "38400/54000\tLoss: 151.625\n",
      "41600/54000\tLoss: 151.362\n",
      "44800/54000\tLoss: 150.802\n",
      "48000/54000\tLoss: 150.134\n",
      "51200/54000\tLoss: 151.174\n",
      "Valid Loss: 150.904, Recon Error: 0.021\n",
      "150.9039426763007\n",
      "Epoch: 4 Average loss: 151.45 Valid loss: 150.9039426763007\tRecon Error:0.021\n",
      "0/54000\tLoss: 147.583\n",
      "3200/54000\tLoss: 150.671\n",
      "6400/54000\tLoss: 150.519\n",
      "9600/54000\tLoss: 150.627\n",
      "12800/54000\tLoss: 150.350\n",
      "16000/54000\tLoss: 150.444\n",
      "19200/54000\tLoss: 149.950\n",
      "22400/54000\tLoss: 150.351\n",
      "25600/54000\tLoss: 150.095\n",
      "28800/54000\tLoss: 149.642\n",
      "32000/54000\tLoss: 149.650\n",
      "35200/54000\tLoss: 149.198\n",
      "38400/54000\tLoss: 149.358\n",
      "41600/54000\tLoss: 149.719\n",
      "44800/54000\tLoss: 149.241\n",
      "48000/54000\tLoss: 149.059\n",
      "51200/54000\tLoss: 148.808\n",
      "Valid Loss: 149.193, Recon Error: 0.021\n",
      "149.19261023338805\n",
      "Epoch: 5 Average loss: 149.85 Valid loss: 149.19261023338805\tRecon Error:0.021\n",
      "0/54000\tLoss: 151.716\n",
      "3200/54000\tLoss: 148.900\n",
      "6400/54000\tLoss: 148.577\n",
      "9600/54000\tLoss: 149.031\n",
      "12800/54000\tLoss: 148.691\n",
      "16000/54000\tLoss: 148.862\n",
      "19200/54000\tLoss: 148.630\n",
      "22400/54000\tLoss: 148.434\n",
      "25600/54000\tLoss: 148.200\n",
      "28800/54000\tLoss: 147.969\n",
      "32000/54000\tLoss: 148.202\n",
      "35200/54000\tLoss: 148.805\n",
      "38400/54000\tLoss: 148.269\n",
      "41600/54000\tLoss: 147.983\n",
      "44800/54000\tLoss: 148.213\n",
      "48000/54000\tLoss: 147.889\n",
      "51200/54000\tLoss: 147.947\n",
      "Valid Loss: 148.110, Recon Error: 0.020\n",
      "148.10952775021818\n",
      "Epoch: 6 Average loss: 148.42 Valid loss: 148.10952775021818\tRecon Error:0.020\n",
      "0/54000\tLoss: 148.845\n",
      "3200/54000\tLoss: 147.533\n",
      "6400/54000\tLoss: 147.886\n",
      "9600/54000\tLoss: 147.333\n",
      "12800/54000\tLoss: 147.777\n",
      "16000/54000\tLoss: 147.401\n",
      "19200/54000\tLoss: 147.438\n",
      "22400/54000\tLoss: 147.641\n",
      "25600/54000\tLoss: 147.501\n",
      "28800/54000\tLoss: 147.168\n",
      "32000/54000\tLoss: 146.938\n",
      "35200/54000\tLoss: 147.272\n",
      "38400/54000\tLoss: 147.381\n",
      "41600/54000\tLoss: 146.767\n",
      "44800/54000\tLoss: 147.086\n",
      "48000/54000\tLoss: 147.211\n",
      "51200/54000\tLoss: 146.881\n",
      "Valid Loss: 146.744, Recon Error: 0.018\n",
      "146.74365007116438\n",
      "Epoch: 7 Average loss: 147.34 Valid loss: 146.74365007116438\tRecon Error:0.018\n",
      "0/54000\tLoss: 146.081\n",
      "3200/54000\tLoss: 146.729\n",
      "6400/54000\tLoss: 147.022\n",
      "9600/54000\tLoss: 146.199\n",
      "12800/54000\tLoss: 147.170\n",
      "16000/54000\tLoss: 146.269\n",
      "19200/54000\tLoss: 146.405\n",
      "22400/54000\tLoss: 146.867\n",
      "25600/54000\tLoss: 146.445\n",
      "28800/54000\tLoss: 146.713\n",
      "32000/54000\tLoss: 146.093\n",
      "35200/54000\tLoss: 145.529\n",
      "38400/54000\tLoss: 145.862\n",
      "41600/54000\tLoss: 145.908\n",
      "44800/54000\tLoss: 146.807\n",
      "48000/54000\tLoss: 146.329\n",
      "51200/54000\tLoss: 146.288\n",
      "Valid Loss: 146.174, Recon Error: 0.019\n",
      "146.1744358793218\n",
      "Epoch: 8 Average loss: 146.44 Valid loss: 146.1744358793218\tRecon Error:0.019\n",
      "0/54000\tLoss: 146.465\n",
      "3200/54000\tLoss: 146.047\n",
      "6400/54000\tLoss: 145.963\n",
      "9600/54000\tLoss: 146.279\n",
      "12800/54000\tLoss: 145.869\n",
      "16000/54000\tLoss: 145.781\n",
      "19200/54000\tLoss: 146.040\n",
      "22400/54000\tLoss: 146.016\n",
      "25600/54000\tLoss: 145.712\n",
      "28800/54000\tLoss: 145.347\n",
      "32000/54000\tLoss: 145.587\n",
      "35200/54000\tLoss: 145.648\n",
      "38400/54000\tLoss: 145.767\n",
      "41600/54000\tLoss: 145.406\n",
      "44800/54000\tLoss: 145.681\n",
      "48000/54000\tLoss: 145.465\n",
      "51200/54000\tLoss: 145.854\n",
      "Valid Loss: 146.041, Recon Error: 0.018\n",
      "146.04056776330827\n",
      "Epoch: 9 Average loss: 145.82 Valid loss: 146.04056776330827\tRecon Error:0.018\n",
      "0/54000\tLoss: 144.552\n",
      "3200/54000\tLoss: 145.446\n",
      "6400/54000\tLoss: 145.321\n",
      "9600/54000\tLoss: 145.339\n",
      "12800/54000\tLoss: 145.210\n",
      "16000/54000\tLoss: 144.955\n",
      "19200/54000\tLoss: 145.456\n",
      "22400/54000\tLoss: 145.463\n",
      "25600/54000\tLoss: 145.360\n",
      "28800/54000\tLoss: 144.917\n",
      "32000/54000\tLoss: 145.531\n",
      "35200/54000\tLoss: 145.206\n",
      "38400/54000\tLoss: 145.061\n",
      "41600/54000\tLoss: 145.211\n",
      "44800/54000\tLoss: 145.278\n",
      "48000/54000\tLoss: 145.001\n",
      "51200/54000\tLoss: 144.979\n",
      "Valid Loss: 145.267, Recon Error: 0.018\n",
      "145.26696079335315\n",
      "Epoch: 10 Average loss: 145.24 Valid loss: 145.26696079335315\tRecon Error:0.018\n",
      "0/54000\tLoss: 147.117\n",
      "3200/54000\tLoss: 144.846\n",
      "6400/54000\tLoss: 144.764\n",
      "9600/54000\tLoss: 145.015\n",
      "12800/54000\tLoss: 144.994\n",
      "16000/54000\tLoss: 145.621\n",
      "19200/54000\tLoss: 144.917\n",
      "22400/54000\tLoss: 144.680\n",
      "25600/54000\tLoss: 144.903\n",
      "28800/54000\tLoss: 145.081\n",
      "32000/54000\tLoss: 144.831\n",
      "35200/54000\tLoss: 145.220\n",
      "38400/54000\tLoss: 144.783\n",
      "41600/54000\tLoss: 144.497\n",
      "44800/54000\tLoss: 144.551\n",
      "48000/54000\tLoss: 144.671\n",
      "51200/54000\tLoss: 144.766\n",
      "Valid Loss: 145.376, Recon Error: 0.019\n",
      "145.37572966230675\n",
      "Epoch: 11 Average loss: 144.93 Valid loss: 145.37572966230675\tRecon Error:0.019\n",
      "0/54000\tLoss: 144.295\n",
      "3200/54000\tLoss: 144.950\n",
      "6400/54000\tLoss: 144.737\n",
      "9600/54000\tLoss: 144.161\n",
      "12800/54000\tLoss: 144.365\n",
      "16000/54000\tLoss: 144.627\n",
      "19200/54000\tLoss: 144.171\n",
      "22400/54000\tLoss: 144.284\n",
      "25600/54000\tLoss: 144.737\n",
      "28800/54000\tLoss: 144.664\n",
      "32000/54000\tLoss: 144.570\n",
      "35200/54000\tLoss: 144.142\n",
      "38400/54000\tLoss: 144.571\n",
      "41600/54000\tLoss: 144.404\n",
      "44800/54000\tLoss: 144.920\n",
      "48000/54000\tLoss: 144.439\n",
      "51200/54000\tLoss: 144.265\n",
      "Valid Loss: 144.579, Recon Error: 0.018\n",
      "144.5792556113385\n",
      "Epoch: 12 Average loss: 144.54 Valid loss: 144.5792556113385\tRecon Error:0.018\n",
      "0/54000\tLoss: 145.177\n",
      "3200/54000\tLoss: 144.453\n",
      "6400/54000\tLoss: 144.392\n",
      "9600/54000\tLoss: 144.476\n",
      "12800/54000\tLoss: 144.261\n",
      "16000/54000\tLoss: 144.445\n",
      "19200/54000\tLoss: 144.062\n",
      "22400/54000\tLoss: 144.036\n",
      "25600/54000\tLoss: 144.506\n",
      "28800/54000\tLoss: 144.149\n",
      "32000/54000\tLoss: 143.948\n",
      "35200/54000\tLoss: 143.889\n",
      "38400/54000\tLoss: 144.193\n",
      "41600/54000\tLoss: 143.936\n",
      "44800/54000\tLoss: 144.022\n",
      "48000/54000\tLoss: 143.850\n",
      "51200/54000\tLoss: 144.099\n",
      "Valid Loss: 144.340, Recon Error: 0.017\n",
      "144.33999763651096\n",
      "Epoch: 13 Average loss: 144.21 Valid loss: 144.33999763651096\tRecon Error:0.017\n",
      "0/54000\tLoss: 144.049\n",
      "3200/54000\tLoss: 143.772\n",
      "6400/54000\tLoss: 143.530\n",
      "9600/54000\tLoss: 143.556\n",
      "12800/54000\tLoss: 143.720\n",
      "16000/54000\tLoss: 143.723\n",
      "19200/54000\tLoss: 143.528\n",
      "22400/54000\tLoss: 143.440\n",
      "25600/54000\tLoss: 143.824\n",
      "28800/54000\tLoss: 143.698\n",
      "32000/54000\tLoss: 143.334\n",
      "35200/54000\tLoss: 143.837\n",
      "38400/54000\tLoss: 143.364\n",
      "41600/54000\tLoss: 143.577\n",
      "44800/54000\tLoss: 143.846\n",
      "48000/54000\tLoss: 143.447\n",
      "51200/54000\tLoss: 143.730\n",
      "Valid Loss: 144.228, Recon Error: 0.018\n",
      "144.2280927617499\n",
      "Epoch: 14 Average loss: 143.65 Valid loss: 144.2280927617499\tRecon Error:0.018\n",
      "0/54000\tLoss: 142.077\n",
      "3200/54000\tLoss: 143.928\n",
      "6400/54000\tLoss: 143.910\n",
      "9600/54000\tLoss: 144.013\n",
      "12800/54000\tLoss: 143.192\n",
      "16000/54000\tLoss: 143.479\n",
      "19200/54000\tLoss: 144.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22400/54000\tLoss: 143.586\n",
      "25600/54000\tLoss: 143.427\n",
      "28800/54000\tLoss: 143.075\n",
      "32000/54000\tLoss: 143.321\n",
      "35200/54000\tLoss: 143.629\n",
      "38400/54000\tLoss: 143.179\n",
      "41600/54000\tLoss: 143.541\n",
      "44800/54000\tLoss: 142.879\n",
      "48000/54000\tLoss: 143.029\n",
      "51200/54000\tLoss: 143.362\n",
      "Valid Loss: 143.627, Recon Error: 0.019\n",
      "143.6268024850399\n",
      "Epoch: 15 Average loss: 143.50 Valid loss: 143.6268024850399\tRecon Error:0.019\n",
      "0/54000\tLoss: 142.353\n",
      "3200/54000\tLoss: 143.470\n",
      "6400/54000\tLoss: 143.111\n",
      "9600/54000\tLoss: 142.782\n",
      "12800/54000\tLoss: 143.096\n",
      "16000/54000\tLoss: 143.237\n",
      "19200/54000\tLoss: 143.065\n",
      "22400/54000\tLoss: 143.020\n",
      "25600/54000\tLoss: 142.854\n",
      "28800/54000\tLoss: 142.609\n",
      "32000/54000\tLoss: 143.163\n",
      "35200/54000\tLoss: 142.583\n",
      "38400/54000\tLoss: 142.773\n",
      "41600/54000\tLoss: 143.168\n",
      "44800/54000\tLoss: 143.257\n",
      "48000/54000\tLoss: 142.624\n",
      "51200/54000\tLoss: 142.887\n",
      "Valid Loss: 142.941, Recon Error: 0.018\n",
      "142.94111000223361\n",
      "Epoch: 16 Average loss: 143.04 Valid loss: 142.94111000223361\tRecon Error:0.018\n",
      "0/54000\tLoss: 143.268\n",
      "3200/54000\tLoss: 142.819\n",
      "6400/54000\tLoss: 142.911\n",
      "9600/54000\tLoss: 142.300\n",
      "12800/54000\tLoss: 142.978\n",
      "16000/54000\tLoss: 142.585\n",
      "19200/54000\tLoss: 142.505\n",
      "22400/54000\tLoss: 142.296\n",
      "25600/54000\tLoss: 142.525\n",
      "28800/54000\tLoss: 142.707\n",
      "32000/54000\tLoss: 142.652\n",
      "35200/54000\tLoss: 142.797\n",
      "38400/54000\tLoss: 142.558\n",
      "41600/54000\tLoss: 142.579\n",
      "44800/54000\tLoss: 143.019\n",
      "48000/54000\tLoss: 143.441\n",
      "51200/54000\tLoss: 142.593\n",
      "Valid Loss: 143.213, Recon Error: 0.016\n",
      "143.21280329278176\n",
      "Epoch: 17 Average loss: 142.76 Valid loss: 143.21280329278176\tRecon Error:0.016\n",
      "0/54000\tLoss: 146.647\n",
      "3200/54000\tLoss: 142.157\n",
      "6400/54000\tLoss: 142.371\n",
      "9600/54000\tLoss: 142.945\n",
      "12800/54000\tLoss: 142.266\n",
      "16000/54000\tLoss: 142.544\n",
      "19200/54000\tLoss: 143.106\n",
      "22400/54000\tLoss: 142.490\n",
      "25600/54000\tLoss: 142.469\n",
      "28800/54000\tLoss: 142.625\n",
      "32000/54000\tLoss: 142.485\n",
      "35200/54000\tLoss: 142.563\n",
      "38400/54000\tLoss: 142.190\n",
      "41600/54000\tLoss: 142.757\n",
      "44800/54000\tLoss: 142.334\n",
      "48000/54000\tLoss: 142.466\n",
      "51200/54000\tLoss: 141.855\n",
      "Valid Loss: 142.847, Recon Error: 0.019\n",
      "142.84715887840758\n",
      "Epoch: 18 Average loss: 142.52 Valid loss: 142.84715887840758\tRecon Error:0.019\n",
      "0/54000\tLoss: 143.571\n",
      "3200/54000\tLoss: 142.283\n",
      "6400/54000\tLoss: 142.529\n",
      "9600/54000\tLoss: 142.446\n",
      "12800/54000\tLoss: 142.017\n",
      "16000/54000\tLoss: 142.231\n",
      "19200/54000\tLoss: 142.080\n",
      "22400/54000\tLoss: 142.013\n",
      "25600/54000\tLoss: 142.346\n",
      "28800/54000\tLoss: 142.481\n",
      "32000/54000\tLoss: 141.985\n",
      "35200/54000\tLoss: 142.600\n",
      "38400/54000\tLoss: 142.121\n",
      "41600/54000\tLoss: 142.078\n",
      "44800/54000\tLoss: 141.832\n",
      "48000/54000\tLoss: 142.232\n",
      "51200/54000\tLoss: 141.665\n",
      "Valid Loss: 142.793, Recon Error: 0.018\n",
      "142.7927737946206\n",
      "Epoch: 19 Average loss: 142.24 Valid loss: 142.7927737946206\tRecon Error:0.018\n",
      "0/54000\tLoss: 141.599\n",
      "3200/54000\tLoss: 142.142\n",
      "6400/54000\tLoss: 142.063\n",
      "9600/54000\tLoss: 142.205\n",
      "12800/54000\tLoss: 142.416\n",
      "16000/54000\tLoss: 142.200\n",
      "19200/54000\tLoss: 142.057\n",
      "22400/54000\tLoss: 142.256\n",
      "25600/54000\tLoss: 141.675\n",
      "28800/54000\tLoss: 142.498\n",
      "32000/54000\tLoss: 142.112\n",
      "35200/54000\tLoss: 142.144\n",
      "38400/54000\tLoss: 142.368\n",
      "41600/54000\tLoss: 141.878\n",
      "44800/54000\tLoss: 142.188\n",
      "48000/54000\tLoss: 141.969\n",
      "51200/54000\tLoss: 141.803\n",
      "Valid Loss: 142.243, Recon Error: 0.017\n",
      "142.24273227123504\n",
      "Epoch: 20 Average loss: 142.13 Valid loss: 142.24273227123504\tRecon Error:0.017\n",
      "0/54000\tLoss: 141.411\n",
      "3200/54000\tLoss: 142.565\n",
      "6400/54000\tLoss: 141.899\n",
      "9600/54000\tLoss: 141.273\n",
      "12800/54000\tLoss: 141.787\n",
      "16000/54000\tLoss: 141.749\n",
      "19200/54000\tLoss: 141.986\n",
      "22400/54000\tLoss: 141.305\n",
      "25600/54000\tLoss: 141.466\n",
      "28800/54000\tLoss: 141.221\n",
      "32000/54000\tLoss: 142.146\n",
      "35200/54000\tLoss: 141.744\n",
      "38400/54000\tLoss: 141.627\n",
      "41600/54000\tLoss: 141.635\n",
      "44800/54000\tLoss: 141.591\n",
      "48000/54000\tLoss: 141.714\n",
      "51200/54000\tLoss: 141.848\n",
      "Valid Loss: 142.283, Recon Error: 0.018\n",
      "142.28349563923288\n",
      "Epoch: 21 Average loss: 141.74 Valid loss: 142.28349563923288\tRecon Error:0.018\n",
      "0/54000\tLoss: 143.853\n",
      "3200/54000\tLoss: 141.664\n",
      "6400/54000\tLoss: 141.578\n",
      "9600/54000\tLoss: 141.796\n",
      "12800/54000\tLoss: 141.806\n",
      "16000/54000\tLoss: 141.633\n",
      "19200/54000\tLoss: 141.334\n",
      "22400/54000\tLoss: 141.659\n",
      "25600/54000\tLoss: 141.831\n",
      "28800/54000\tLoss: 141.711\n",
      "32000/54000\tLoss: 141.547\n",
      "35200/54000\tLoss: 141.382\n",
      "38400/54000\tLoss: 141.015\n",
      "41600/54000\tLoss: 141.807\n",
      "44800/54000\tLoss: 142.232\n",
      "48000/54000\tLoss: 141.736\n",
      "51200/54000\tLoss: 141.426\n",
      "Valid Loss: 141.815, Recon Error: 0.017\n",
      "141.81518067704872\n",
      "Epoch: 22 Average loss: 141.68 Valid loss: 141.81518067704872\tRecon Error:0.017\n",
      "0/54000\tLoss: 142.432\n",
      "3200/54000\tLoss: 141.407\n",
      "6400/54000\tLoss: 141.219\n",
      "9600/54000\tLoss: 141.587\n",
      "12800/54000\tLoss: 141.495\n",
      "16000/54000\tLoss: 141.574\n",
      "19200/54000\tLoss: 141.467\n",
      "22400/54000\tLoss: 141.783\n",
      "25600/54000\tLoss: 141.366\n",
      "28800/54000\tLoss: 140.974\n",
      "32000/54000\tLoss: 140.952\n",
      "35200/54000\tLoss: 141.321\n",
      "38400/54000\tLoss: 141.338\n",
      "41600/54000\tLoss: 141.492\n",
      "44800/54000\tLoss: 141.589\n",
      "48000/54000\tLoss: 141.243\n",
      "51200/54000\tLoss: 141.771\n",
      "Valid Loss: 141.806, Recon Error: 0.016\n",
      "141.80598823060382\n",
      "Epoch: 23 Average loss: 141.47 Valid loss: 141.80598823060382\tRecon Error:0.016\n",
      "0/54000\tLoss: 140.805\n",
      "3200/54000\tLoss: 141.386\n",
      "6400/54000\tLoss: 141.292\n",
      "9600/54000\tLoss: 141.341\n",
      "12800/54000\tLoss: 140.938\n",
      "16000/54000\tLoss: 141.120\n",
      "19200/54000\tLoss: 140.691\n",
      "22400/54000\tLoss: 140.871\n",
      "25600/54000\tLoss: 140.966\n",
      "28800/54000\tLoss: 141.444\n",
      "32000/54000\tLoss: 140.995\n",
      "35200/54000\tLoss: 141.259\n",
      "38400/54000\tLoss: 141.483\n",
      "41600/54000\tLoss: 141.696\n",
      "44800/54000\tLoss: 141.106\n",
      "48000/54000\tLoss: 140.917\n",
      "51200/54000\tLoss: 141.126\n",
      "Valid Loss: 141.564, Recon Error: 0.019\n",
      "141.5635268840384\n",
      "Epoch: 24 Average loss: 141.22 Valid loss: 141.5635268840384\tRecon Error:0.019\n",
      "0/54000\tLoss: 141.834\n",
      "3200/54000\tLoss: 140.814\n",
      "6400/54000\tLoss: 141.703\n",
      "9600/54000\tLoss: 141.203\n",
      "12800/54000\tLoss: 141.206\n",
      "16000/54000\tLoss: 141.165\n",
      "19200/54000\tLoss: 141.246\n",
      "22400/54000\tLoss: 140.609\n",
      "25600/54000\tLoss: 141.204\n",
      "28800/54000\tLoss: 140.876\n",
      "32000/54000\tLoss: 141.091\n",
      "35200/54000\tLoss: 140.715\n",
      "38400/54000\tLoss: 141.021\n",
      "41600/54000\tLoss: 140.896\n",
      "44800/54000\tLoss: 141.317\n",
      "48000/54000\tLoss: 140.492\n",
      "51200/54000\tLoss: 140.410\n",
      "Valid Loss: 141.362, Recon Error: 0.016\n",
      "141.36241084971326\n",
      "Epoch: 25 Average loss: 141.04 Valid loss: 141.36241084971326\tRecon Error:0.016\n",
      "0/54000\tLoss: 142.423\n",
      "3200/54000\tLoss: 140.921\n",
      "6400/54000\tLoss: 141.190\n",
      "9600/54000\tLoss: 140.897\n",
      "12800/54000\tLoss: 141.180\n",
      "16000/54000\tLoss: 140.578\n",
      "19200/54000\tLoss: 140.675\n",
      "22400/54000\tLoss: 140.820\n",
      "25600/54000\tLoss: 140.794\n",
      "28800/54000\tLoss: 140.733\n",
      "32000/54000\tLoss: 140.921\n",
      "35200/54000\tLoss: 140.495\n",
      "38400/54000\tLoss: 140.810\n",
      "41600/54000\tLoss: 140.750\n",
      "44800/54000\tLoss: 141.252\n",
      "48000/54000\tLoss: 140.232\n",
      "51200/54000\tLoss: 140.850\n",
      "Valid Loss: 141.325, Recon Error: 0.017\n",
      "141.32511009053982\n",
      "Epoch: 26 Average loss: 140.84 Valid loss: 141.32511009053982\tRecon Error:0.017\n",
      "0/54000\tLoss: 141.086\n",
      "3200/54000\tLoss: 140.540\n",
      "6400/54000\tLoss: 140.599\n",
      "9600/54000\tLoss: 140.374\n",
      "12800/54000\tLoss: 140.738\n",
      "16000/54000\tLoss: 140.701\n",
      "19200/54000\tLoss: 140.253\n",
      "22400/54000\tLoss: 140.193\n",
      "25600/54000\tLoss: 140.218\n",
      "28800/54000\tLoss: 140.324\n",
      "32000/54000\tLoss: 140.864\n",
      "35200/54000\tLoss: 140.430\n",
      "38400/54000\tLoss: 140.602\n",
      "41600/54000\tLoss: 140.312\n",
      "44800/54000\tLoss: 140.499\n",
      "48000/54000\tLoss: 140.861\n",
      "51200/54000\tLoss: 140.187\n",
      "Valid Loss: 140.887, Recon Error: 0.015\n",
      "140.8869185752057\n",
      "Epoch: 27 Average loss: 140.53 Valid loss: 140.8869185752057\tRecon Error:0.015\n",
      "0/54000\tLoss: 140.737\n",
      "3200/54000\tLoss: 140.630\n",
      "6400/54000\tLoss: 140.182\n",
      "9600/54000\tLoss: 140.260\n",
      "12800/54000\tLoss: 140.519\n",
      "16000/54000\tLoss: 140.177\n",
      "19200/54000\tLoss: 140.628\n",
      "22400/54000\tLoss: 140.745\n",
      "25600/54000\tLoss: 140.485\n",
      "28800/54000\tLoss: 140.976\n",
      "32000/54000\tLoss: 140.601\n",
      "35200/54000\tLoss: 139.676\n",
      "38400/54000\tLoss: 139.866\n",
      "41600/54000\tLoss: 140.581\n",
      "44800/54000\tLoss: 140.184\n",
      "48000/54000\tLoss: 140.183\n",
      "51200/54000\tLoss: 140.247\n",
      "Valid Loss: 140.774, Recon Error: 0.017\n",
      "140.77382383955285\n",
      "Epoch: 28 Average loss: 140.44 Valid loss: 140.77382383955285\tRecon Error:0.017\n",
      "0/54000\tLoss: 138.323\n",
      "3200/54000\tLoss: 140.046\n",
      "6400/54000\tLoss: 140.042\n",
      "9600/54000\tLoss: 140.465\n",
      "12800/54000\tLoss: 140.076\n",
      "16000/54000\tLoss: 139.929\n",
      "19200/54000\tLoss: 140.454\n",
      "22400/54000\tLoss: 140.303\n",
      "25600/54000\tLoss: 139.678\n",
      "28800/54000\tLoss: 140.073\n",
      "32000/54000\tLoss: 140.201\n",
      "35200/54000\tLoss: 139.833\n",
      "38400/54000\tLoss: 140.053\n",
      "41600/54000\tLoss: 140.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44800/54000\tLoss: 140.013\n",
      "48000/54000\tLoss: 140.510\n",
      "51200/54000\tLoss: 140.042\n",
      "Valid Loss: 140.126, Recon Error: 0.016\n",
      "140.12578404203376\n",
      "Epoch: 29 Average loss: 140.15 Valid loss: 140.12578404203376\tRecon Error:0.016\n",
      "0/54000\tLoss: 140.059\n",
      "3200/54000\tLoss: 139.815\n",
      "6400/54000\tLoss: 140.205\n",
      "9600/54000\tLoss: 140.561\n",
      "12800/54000\tLoss: 139.928\n",
      "16000/54000\tLoss: 139.924\n",
      "19200/54000\tLoss: 139.733\n",
      "22400/54000\tLoss: 139.684\n",
      "25600/54000\tLoss: 139.562\n",
      "28800/54000\tLoss: 140.032\n",
      "32000/54000\tLoss: 139.756\n",
      "35200/54000\tLoss: 139.963\n",
      "38400/54000\tLoss: 140.027\n",
      "41600/54000\tLoss: 140.087\n",
      "44800/54000\tLoss: 139.826\n",
      "48000/54000\tLoss: 139.686\n",
      "51200/54000\tLoss: 139.805\n",
      "Valid Loss: 140.513, Recon Error: 0.015\n",
      "140.51349331470246\n",
      "Epoch: 30 Average loss: 139.96 Valid loss: 140.51349331470246\tRecon Error:0.015\n",
      "0/54000\tLoss: 141.214\n",
      "3200/54000\tLoss: 140.238\n",
      "6400/54000\tLoss: 140.229\n",
      "9600/54000\tLoss: 139.354\n",
      "12800/54000\tLoss: 140.160\n",
      "16000/54000\tLoss: 139.836\n",
      "19200/54000\tLoss: 139.951\n",
      "22400/54000\tLoss: 139.923\n",
      "25600/54000\tLoss: 139.928\n",
      "28800/54000\tLoss: 139.165\n",
      "32000/54000\tLoss: 139.556\n",
      "35200/54000\tLoss: 139.593\n",
      "38400/54000\tLoss: 139.540\n",
      "41600/54000\tLoss: 139.488\n",
      "44800/54000\tLoss: 139.925\n",
      "48000/54000\tLoss: 139.862\n",
      "51200/54000\tLoss: 139.849\n",
      "Valid Loss: 140.261, Recon Error: 0.014\n",
      "140.26116975824883\n",
      "Epoch: 31 Average loss: 139.81 Valid loss: 140.26116975824883\tRecon Error:0.014\n",
      "0/54000\tLoss: 141.540\n",
      "3200/54000\tLoss: 139.667\n",
      "6400/54000\tLoss: 139.518\n",
      "9600/54000\tLoss: 139.199\n",
      "12800/54000\tLoss: 139.687\n",
      "16000/54000\tLoss: 139.883\n",
      "19200/54000\tLoss: 139.746\n",
      "22400/54000\tLoss: 139.283\n",
      "25600/54000\tLoss: 139.464\n",
      "28800/54000\tLoss: 139.952\n",
      "32000/54000\tLoss: 139.829\n",
      "35200/54000\tLoss: 139.381\n",
      "38400/54000\tLoss: 139.634\n",
      "41600/54000\tLoss: 139.479\n",
      "44800/54000\tLoss: 139.215\n",
      "48000/54000\tLoss: 139.533\n",
      "51200/54000\tLoss: 139.210\n",
      "Valid Loss: 139.990, Recon Error: 0.018\n",
      "139.98965389170544\n",
      "Epoch: 32 Average loss: 139.58 Valid loss: 139.98965389170544\tRecon Error:0.018\n",
      "0/54000\tLoss: 142.868\n",
      "3200/54000\tLoss: 139.122\n",
      "6400/54000\tLoss: 138.762\n",
      "9600/54000\tLoss: 139.703\n",
      "12800/54000\tLoss: 139.449\n",
      "16000/54000\tLoss: 139.116\n",
      "19200/54000\tLoss: 139.668\n",
      "22400/54000\tLoss: 139.604\n",
      "25600/54000\tLoss: 139.016\n",
      "28800/54000\tLoss: 139.306\n",
      "32000/54000\tLoss: 139.202\n",
      "35200/54000\tLoss: 139.566\n",
      "38400/54000\tLoss: 139.569\n",
      "41600/54000\tLoss: 138.943\n",
      "44800/54000\tLoss: 139.075\n",
      "48000/54000\tLoss: 139.019\n",
      "51200/54000\tLoss: 139.470\n",
      "Valid Loss: 140.241, Recon Error: 0.016\n",
      "140.24055156301944\n",
      "Epoch: 33 Average loss: 139.34 Valid loss: 140.24055156301944\tRecon Error:0.016\n",
      "0/54000\tLoss: 146.421\n",
      "3200/54000\tLoss: 139.280\n",
      "6400/54000\tLoss: 138.969\n",
      "9600/54000\tLoss: 139.670\n",
      "12800/54000\tLoss: 138.879\n",
      "16000/54000\tLoss: 139.258\n",
      "19200/54000\tLoss: 138.806\n",
      "22400/54000\tLoss: 138.675\n",
      "25600/54000\tLoss: 139.978\n",
      "28800/54000\tLoss: 139.491\n",
      "32000/54000\tLoss: 139.188\n",
      "35200/54000\tLoss: 139.387\n",
      "38400/54000\tLoss: 139.582\n",
      "41600/54000\tLoss: 138.989\n",
      "44800/54000\tLoss: 139.669\n",
      "48000/54000\tLoss: 138.849\n",
      "51200/54000\tLoss: 139.192\n",
      "Valid Loss: 139.888, Recon Error: 0.019\n",
      "139.88828310053398\n",
      "Epoch: 34 Average loss: 139.28 Valid loss: 139.88828310053398\tRecon Error:0.019\n",
      "0/54000\tLoss: 139.347\n",
      "3200/54000\tLoss: 138.717\n",
      "6400/54000\tLoss: 139.237\n",
      "9600/54000\tLoss: 139.265\n",
      "12800/54000\tLoss: 138.913\n",
      "16000/54000\tLoss: 138.868\n",
      "19200/54000\tLoss: 138.900\n",
      "22400/54000\tLoss: 138.728\n",
      "25600/54000\tLoss: 138.963\n",
      "28800/54000\tLoss: 139.118\n",
      "32000/54000\tLoss: 138.949\n",
      "35200/54000\tLoss: 138.690\n",
      "38400/54000\tLoss: 139.043\n",
      "41600/54000\tLoss: 139.009\n",
      "44800/54000\tLoss: 138.631\n",
      "48000/54000\tLoss: 138.852\n",
      "51200/54000\tLoss: 139.327\n",
      "Valid Loss: 139.321, Recon Error: 0.016\n",
      "139.3210194364507\n",
      "Epoch: 35 Average loss: 139.00 Valid loss: 139.3210194364507\tRecon Error:0.016\n",
      "0/54000\tLoss: 139.737\n",
      "3200/54000\tLoss: 138.825\n",
      "6400/54000\tLoss: 138.475\n",
      "9600/54000\tLoss: 138.510\n",
      "12800/54000\tLoss: 139.299\n",
      "16000/54000\tLoss: 138.867\n",
      "19200/54000\tLoss: 138.687\n",
      "22400/54000\tLoss: 139.173\n",
      "25600/54000\tLoss: 139.221\n",
      "28800/54000\tLoss: 138.935\n",
      "32000/54000\tLoss: 138.612\n",
      "35200/54000\tLoss: 138.655\n",
      "38400/54000\tLoss: 138.648\n",
      "41600/54000\tLoss: 139.305\n",
      "44800/54000\tLoss: 138.506\n",
      "48000/54000\tLoss: 138.702\n",
      "51200/54000\tLoss: 138.981\n",
      "Valid Loss: 139.400, Recon Error: 0.015\n",
      "139.39983903600813\n",
      "Epoch: 36 Average loss: 138.88 Valid loss: 139.39983903600813\tRecon Error:0.015\n",
      "0/54000\tLoss: 139.644\n",
      "3200/54000\tLoss: 138.120\n",
      "6400/54000\tLoss: 138.798\n",
      "9600/54000\tLoss: 138.856\n",
      "12800/54000\tLoss: 138.689\n",
      "16000/54000\tLoss: 138.820\n",
      "19200/54000\tLoss: 138.754\n",
      "22400/54000\tLoss: 138.546\n",
      "25600/54000\tLoss: 138.773\n",
      "28800/54000\tLoss: 139.149\n",
      "32000/54000\tLoss: 138.784\n",
      "35200/54000\tLoss: 138.715\n",
      "38400/54000\tLoss: 138.871\n",
      "41600/54000\tLoss: 138.290\n",
      "44800/54000\tLoss: 138.195\n",
      "48000/54000\tLoss: 138.776\n",
      "51200/54000\tLoss: 138.482\n",
      "Valid Loss: 138.700, Recon Error: 0.014\n",
      "138.69972066676362\n",
      "Epoch: 37 Average loss: 138.68 Valid loss: 138.69972066676362\tRecon Error:0.014\n",
      "0/54000\tLoss: 138.642\n",
      "3200/54000\tLoss: 138.640\n",
      "6400/54000\tLoss: 138.748\n",
      "9600/54000\tLoss: 138.575\n",
      "12800/54000\tLoss: 138.788\n",
      "16000/54000\tLoss: 139.106\n",
      "19200/54000\tLoss: 138.415\n",
      "22400/54000\tLoss: 138.077\n",
      "25600/54000\tLoss: 138.589\n",
      "28800/54000\tLoss: 138.375\n",
      "32000/54000\tLoss: 138.426\n",
      "35200/54000\tLoss: 138.387\n",
      "38400/54000\tLoss: 138.439\n",
      "41600/54000\tLoss: 138.407\n",
      "44800/54000\tLoss: 138.523\n",
      "48000/54000\tLoss: 137.952\n",
      "51200/54000\tLoss: 138.673\n",
      "Valid Loss: 138.952, Recon Error: 0.014\n",
      "138.95176842872132\n",
      "Epoch: 38 Average loss: 138.56 Valid loss: 138.95176842872132\tRecon Error:0.014\n",
      "0/54000\tLoss: 140.125\n",
      "3200/54000\tLoss: 138.630\n",
      "6400/54000\tLoss: 138.413\n",
      "9600/54000\tLoss: 138.357\n",
      "12800/54000\tLoss: 138.004\n",
      "16000/54000\tLoss: 138.656\n",
      "19200/54000\tLoss: 137.839\n",
      "22400/54000\tLoss: 138.091\n",
      "25600/54000\tLoss: 138.664\n",
      "28800/54000\tLoss: 138.614\n",
      "32000/54000\tLoss: 138.434\n",
      "35200/54000\tLoss: 138.286\n",
      "38400/54000\tLoss: 138.558\n",
      "41600/54000\tLoss: 137.570\n",
      "44800/54000\tLoss: 138.534\n",
      "48000/54000\tLoss: 138.709\n",
      "51200/54000\tLoss: 138.600\n",
      "Valid Loss: 138.487, Recon Error: 0.016\n",
      "138.48711297867146\n",
      "Epoch: 39 Average loss: 138.39 Valid loss: 138.48711297867146\tRecon Error:0.016\n",
      "0/54000\tLoss: 138.068\n",
      "3200/54000\tLoss: 138.274\n",
      "6400/54000\tLoss: 137.722\n",
      "9600/54000\tLoss: 138.188\n",
      "12800/54000\tLoss: 138.105\n",
      "16000/54000\tLoss: 138.581\n",
      "19200/54000\tLoss: 138.408\n",
      "22400/54000\tLoss: 138.397\n",
      "25600/54000\tLoss: 138.015\n",
      "28800/54000\tLoss: 138.027\n",
      "32000/54000\tLoss: 138.045\n",
      "35200/54000\tLoss: 138.315\n",
      "38400/54000\tLoss: 137.612\n",
      "41600/54000\tLoss: 138.408\n",
      "44800/54000\tLoss: 137.820\n",
      "48000/54000\tLoss: 138.381\n",
      "51200/54000\tLoss: 137.912\n",
      "Valid Loss: 138.588, Recon Error: 0.017\n",
      "138.5876242455016\n",
      "Epoch: 40 Average loss: 138.16 Valid loss: 138.5876242455016\tRecon Error:0.017\n",
      "0/54000\tLoss: 137.808\n",
      "3200/54000\tLoss: 138.032\n",
      "6400/54000\tLoss: 138.078\n",
      "9600/54000\tLoss: 137.767\n",
      "12800/54000\tLoss: 138.058\n",
      "16000/54000\tLoss: 138.174\n",
      "19200/54000\tLoss: 138.104\n",
      "22400/54000\tLoss: 137.613\n",
      "25600/54000\tLoss: 137.584\n",
      "28800/54000\tLoss: 137.945\n",
      "32000/54000\tLoss: 137.544\n",
      "35200/54000\tLoss: 138.014\n",
      "38400/54000\tLoss: 137.947\n",
      "41600/54000\tLoss: 138.447\n",
      "44800/54000\tLoss: 138.427\n",
      "48000/54000\tLoss: 138.471\n",
      "51200/54000\tLoss: 137.852\n",
      "Valid Loss: 138.554, Recon Error: 0.016\n",
      "138.55392829408038\n",
      "Epoch: 41 Average loss: 138.04 Valid loss: 138.55392829408038\tRecon Error:0.016\n",
      "0/54000\tLoss: 143.290\n",
      "3200/54000\tLoss: 137.386\n",
      "6400/54000\tLoss: 138.038\n",
      "9600/54000\tLoss: 138.008\n",
      "12800/54000\tLoss: 137.704\n",
      "16000/54000\tLoss: 137.921\n",
      "19200/54000\tLoss: 137.515\n",
      "22400/54000\tLoss: 137.637\n",
      "25600/54000\tLoss: 137.601\n",
      "28800/54000\tLoss: 138.105\n",
      "32000/54000\tLoss: 137.585\n",
      "35200/54000\tLoss: 137.654\n",
      "38400/54000\tLoss: 138.107\n",
      "41600/54000\tLoss: 137.984\n",
      "44800/54000\tLoss: 138.062\n",
      "48000/54000\tLoss: 138.102\n",
      "51200/54000\tLoss: 137.946\n",
      "Valid Loss: 138.450, Recon Error: 0.015\n",
      "138.45010116252493\n",
      "Epoch: 42 Average loss: 137.91 Valid loss: 138.45010116252493\tRecon Error:0.015\n",
      "0/54000\tLoss: 136.416\n",
      "3200/54000\tLoss: 137.816\n",
      "6400/54000\tLoss: 138.096\n",
      "9600/54000\tLoss: 137.658\n",
      "12800/54000\tLoss: 137.688\n",
      "16000/54000\tLoss: 137.813\n",
      "19200/54000\tLoss: 137.973\n",
      "22400/54000\tLoss: 137.450\n",
      "25600/54000\tLoss: 137.540\n",
      "28800/54000\tLoss: 137.473\n",
      "32000/54000\tLoss: 137.286\n",
      "35200/54000\tLoss: 137.298\n",
      "38400/54000\tLoss: 138.001\n",
      "41600/54000\tLoss: 137.715\n",
      "44800/54000\tLoss: 137.629\n",
      "48000/54000\tLoss: 137.533\n",
      "51200/54000\tLoss: 137.232\n",
      "Valid Loss: 138.321, Recon Error: 0.016\n",
      "138.3205063190866\n",
      "Epoch: 43 Average loss: 137.68 Valid loss: 138.3205063190866\tRecon Error:0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/54000\tLoss: 136.033\n",
      "3200/54000\tLoss: 137.996\n",
      "6400/54000\tLoss: 137.097\n",
      "9600/54000\tLoss: 137.167\n",
      "12800/54000\tLoss: 137.776\n",
      "16000/54000\tLoss: 137.575\n",
      "19200/54000\tLoss: 138.167\n",
      "22400/54000\tLoss: 137.324\n",
      "25600/54000\tLoss: 137.803\n",
      "28800/54000\tLoss: 137.489\n",
      "32000/54000\tLoss: 137.706\n",
      "35200/54000\tLoss: 137.827\n",
      "38400/54000\tLoss: 137.302\n",
      "41600/54000\tLoss: 137.370\n",
      "44800/54000\tLoss: 137.433\n",
      "48000/54000\tLoss: 137.487\n",
      "51200/54000\tLoss: 137.217\n",
      "Valid Loss: 138.021, Recon Error: 0.017\n",
      "138.020520961031\n",
      "Epoch: 44 Average loss: 137.56 Valid loss: 138.020520961031\tRecon Error:0.017\n",
      "0/54000\tLoss: 135.897\n",
      "3200/54000\tLoss: 136.792\n",
      "6400/54000\tLoss: 137.638\n",
      "9600/54000\tLoss: 137.263\n",
      "12800/54000\tLoss: 137.399\n",
      "16000/54000\tLoss: 137.527\n",
      "19200/54000\tLoss: 136.796\n",
      "22400/54000\tLoss: 137.639\n",
      "25600/54000\tLoss: 137.577\n",
      "28800/54000\tLoss: 137.171\n",
      "32000/54000\tLoss: 137.513\n",
      "35200/54000\tLoss: 136.982\n",
      "38400/54000\tLoss: 137.405\n",
      "41600/54000\tLoss: 137.217\n",
      "44800/54000\tLoss: 137.338\n",
      "48000/54000\tLoss: 137.138\n",
      "51200/54000\tLoss: 137.815\n",
      "Valid Loss: 138.095, Recon Error: 0.017\n",
      "138.09502313492146\n",
      "Epoch: 45 Average loss: 137.36 Valid loss: 138.09502313492146\tRecon Error:0.017\n",
      "0/54000\tLoss: 136.590\n",
      "3200/54000\tLoss: 137.177\n",
      "6400/54000\tLoss: 137.221\n",
      "9600/54000\tLoss: 137.171\n",
      "12800/54000\tLoss: 137.481\n",
      "16000/54000\tLoss: 136.670\n",
      "19200/54000\tLoss: 137.121\n",
      "22400/54000\tLoss: 137.342\n",
      "25600/54000\tLoss: 137.345\n",
      "28800/54000\tLoss: 136.694\n",
      "32000/54000\tLoss: 136.880\n",
      "35200/54000\tLoss: 137.249\n",
      "38400/54000\tLoss: 137.013\n",
      "41600/54000\tLoss: 137.048\n",
      "44800/54000\tLoss: 136.861\n",
      "48000/54000\tLoss: 136.953\n",
      "51200/54000\tLoss: 137.498\n",
      "Valid Loss: 137.969, Recon Error: 0.017\n",
      "137.9694644035177\n",
      "Epoch: 46 Average loss: 137.17 Valid loss: 137.9694644035177\tRecon Error:0.017\n",
      "0/54000\tLoss: 137.796\n",
      "3200/54000\tLoss: 137.002\n",
      "6400/54000\tLoss: 137.066\n",
      "9600/54000\tLoss: 136.473\n",
      "12800/54000\tLoss: 136.957\n",
      "16000/54000\tLoss: 136.829\n",
      "19200/54000\tLoss: 137.020\n",
      "22400/54000\tLoss: 137.079\n",
      "25600/54000\tLoss: 137.092\n",
      "28800/54000\tLoss: 137.030\n",
      "32000/54000\tLoss: 136.722\n",
      "35200/54000\tLoss: 136.579\n",
      "38400/54000\tLoss: 137.901\n",
      "41600/54000\tLoss: 137.490\n",
      "44800/54000\tLoss: 137.046\n",
      "48000/54000\tLoss: 136.832\n",
      "51200/54000\tLoss: 136.687\n",
      "Valid Loss: 137.722, Recon Error: 0.017\n",
      "137.72225351536528\n",
      "Epoch: 47 Average loss: 137.04 Valid loss: 137.72225351536528\tRecon Error:0.017\n",
      "0/54000\tLoss: 138.470\n",
      "3200/54000\tLoss: 136.493\n",
      "6400/54000\tLoss: 136.970\n",
      "9600/54000\tLoss: 136.754\n",
      "12800/54000\tLoss: 137.070\n",
      "16000/54000\tLoss: 137.040\n",
      "19200/54000\tLoss: 137.138\n",
      "22400/54000\tLoss: 137.470\n",
      "25600/54000\tLoss: 136.345\n",
      "28800/54000\tLoss: 136.987\n",
      "32000/54000\tLoss: 137.050\n",
      "35200/54000\tLoss: 137.081\n",
      "38400/54000\tLoss: 137.356\n",
      "41600/54000\tLoss: 137.254\n",
      "44800/54000\tLoss: 137.142\n",
      "48000/54000\tLoss: 137.287\n",
      "51200/54000\tLoss: 136.471\n",
      "Valid Loss: 137.758, Recon Error: 0.018\n",
      "137.7580415441635\n",
      "Epoch: 48 Average loss: 137.03 Valid loss: 137.7580415441635\tRecon Error:0.018\n",
      "0/54000\tLoss: 139.030\n",
      "3200/54000\tLoss: 136.074\n",
      "6400/54000\tLoss: 136.372\n",
      "9600/54000\tLoss: 136.336\n",
      "12800/54000\tLoss: 136.702\n",
      "16000/54000\tLoss: 136.530\n",
      "19200/54000\tLoss: 136.845\n",
      "22400/54000\tLoss: 136.571\n",
      "25600/54000\tLoss: 137.015\n",
      "28800/54000\tLoss: 136.589\n",
      "32000/54000\tLoss: 136.604\n",
      "35200/54000\tLoss: 136.441\n",
      "38400/54000\tLoss: 136.209\n",
      "41600/54000\tLoss: 136.313\n",
      "44800/54000\tLoss: 136.848\n",
      "48000/54000\tLoss: 137.283\n",
      "51200/54000\tLoss: 136.692\n",
      "Valid Loss: 137.552, Recon Error: 0.016\n",
      "137.55199805726397\n",
      "Epoch: 49 Average loss: 136.66 Valid loss: 137.55199805726397\tRecon Error:0.016\n",
      "0/54000\tLoss: 134.694\n",
      "3200/54000\tLoss: 136.999\n",
      "6400/54000\tLoss: 136.921\n",
      "9600/54000\tLoss: 136.505\n",
      "12800/54000\tLoss: 136.635\n",
      "16000/54000\tLoss: 136.663\n",
      "19200/54000\tLoss: 137.065\n",
      "22400/54000\tLoss: 137.204\n",
      "25600/54000\tLoss: 136.587\n",
      "28800/54000\tLoss: 136.656\n",
      "32000/54000\tLoss: 136.253\n",
      "35200/54000\tLoss: 136.512\n",
      "38400/54000\tLoss: 136.033\n",
      "41600/54000\tLoss: 136.120\n",
      "44800/54000\tLoss: 137.253\n",
      "48000/54000\tLoss: 136.755\n",
      "51200/54000\tLoss: 136.135\n",
      "Valid Loss: 137.337, Recon Error: 0.017\n",
      "137.33731355058387\n",
      "Epoch: 50 Average loss: 136.69 Valid loss: 137.33731355058387\tRecon Error:0.017\n",
      "0/54000\tLoss: 140.157\n",
      "3200/54000\tLoss: 136.353\n",
      "6400/54000\tLoss: 136.496\n",
      "9600/54000\tLoss: 136.518\n",
      "12800/54000\tLoss: 136.539\n",
      "16000/54000\tLoss: 136.911\n",
      "19200/54000\tLoss: 135.608\n",
      "22400/54000\tLoss: 136.619\n",
      "25600/54000\tLoss: 136.091\n",
      "28800/54000\tLoss: 135.966\n",
      "32000/54000\tLoss: 136.342\n",
      "35200/54000\tLoss: 136.337\n",
      "38400/54000\tLoss: 136.843\n",
      "41600/54000\tLoss: 136.239\n",
      "44800/54000\tLoss: 136.672\n",
      "48000/54000\tLoss: 136.546\n",
      "51200/54000\tLoss: 136.609\n",
      "Valid Loss: 136.956, Recon Error: 0.015\n",
      "136.95641797654173\n",
      "Epoch: 51 Average loss: 136.49 Valid loss: 136.95641797654173\tRecon Error:0.015\n",
      "0/54000\tLoss: 137.589\n",
      "3200/54000\tLoss: 135.950\n",
      "6400/54000\tLoss: 136.427\n",
      "9600/54000\tLoss: 136.014\n",
      "12800/54000\tLoss: 136.519\n",
      "16000/54000\tLoss: 137.452\n",
      "19200/54000\tLoss: 136.480\n",
      "22400/54000\tLoss: 136.607\n",
      "25600/54000\tLoss: 136.492\n",
      "28800/54000\tLoss: 136.487\n",
      "32000/54000\tLoss: 135.719\n",
      "35200/54000\tLoss: 136.029\n",
      "38400/54000\tLoss: 136.023\n",
      "41600/54000\tLoss: 136.248\n",
      "44800/54000\tLoss: 136.878\n",
      "48000/54000\tLoss: 136.027\n",
      "51200/54000\tLoss: 136.077\n",
      "Valid Loss: 137.106, Recon Error: 0.016\n",
      "137.10611075543343\n",
      "Epoch: 52 Average loss: 136.40 Valid loss: 137.10611075543343\tRecon Error:0.016\n",
      "0/54000\tLoss: 137.943\n",
      "3200/54000\tLoss: 136.248\n",
      "6400/54000\tLoss: 136.317\n",
      "9600/54000\tLoss: 136.203\n",
      "12800/54000\tLoss: 135.451\n",
      "16000/54000\tLoss: 136.488\n",
      "19200/54000\tLoss: 136.429\n",
      "22400/54000\tLoss: 136.585\n",
      "25600/54000\tLoss: 136.790\n",
      "28800/54000\tLoss: 135.980\n",
      "32000/54000\tLoss: 136.372\n",
      "35200/54000\tLoss: 136.274\n",
      "38400/54000\tLoss: 136.633\n",
      "41600/54000\tLoss: 136.175\n",
      "44800/54000\tLoss: 135.872\n",
      "48000/54000\tLoss: 136.573\n",
      "51200/54000\tLoss: 136.763\n",
      "Valid Loss: 136.990, Recon Error: 0.018\n",
      "136.98983423760598\n",
      "Epoch: 53 Average loss: 136.35 Valid loss: 136.98983423760598\tRecon Error:0.018\n",
      "0/54000\tLoss: 136.657\n",
      "3200/54000\tLoss: 136.467\n",
      "6400/54000\tLoss: 136.655\n",
      "9600/54000\tLoss: 135.839\n",
      "12800/54000\tLoss: 135.663\n",
      "16000/54000\tLoss: 135.785\n",
      "19200/54000\tLoss: 135.837\n",
      "22400/54000\tLoss: 136.874\n"
     ]
    }
   ],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "###1e-5 6859 1e-4 6727 5e-4 6722 try tanh/L1 loss/beta--->DIP\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "trainer.train(train_loader,valid_loader, epochs=100, save_training_gif=('./training.gif', viz))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "torch.save(model.state_dict(), 'modelDIP_params.pkl')\n",
    "torch.save(model, './modelDIP')\n",
    "##15.078 - 0.0147  17.209 - 0.0168 error tanh \n",
    "##LR 1e-3 0.019-0.023 worse should pick 5e-4\n",
    "##PLOT THE CURVE!!!!!\n",
    "#16-2033.294813156128 32-2846.2241864204407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32)).cuda()\n",
    "model.load_state_dict(torch.load('modelDIP_params.pkl'))\n",
    "#path=\"figures/face/cont_{}/pruned_Beta_ {}lamba{}_ONLYPAIR\".format(n_cont,gamma,0.1)\n",
    "loss = trainer.get_losses()\n",
    "print(len(loss[\"DIP_loss\"]))\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.cuda.is_available()\n",
    "# device = torch.device('cuda')\n",
    "# print(device)\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Chi-square test\n",
    "import torch\n",
    "tensor_one = torch.tensor([[1,2,3],[4,5,6]])\n",
    "tensor_two = torch.tensor([[6,8,9],[10,11,12]])\n",
    "tensor_list = [tensor_one, tensor_two]\n",
    "tens_list = []\n",
    "for tensor in tensor_list:\n",
    "    \n",
    "    print(tensor)\n",
    "    length = tensor.shape[1]\n",
    "    tens_list.append(torch.mean(tensor.float(),dim=0))\n",
    "    \n",
    "tens_list = torch.stack(tens_list).reshape(1,-1)\n",
    "tens_listT = tens_list.t()\n",
    "matrix = tens_listT.matmul(tens_list)\n",
    "print(matrix)\n",
    "print(\"--------\")\n",
    "Chi2 =0\n",
    "for i in range(len(tensor_list)):\n",
    "    for j in range(len(tensor_list)):\n",
    "        if i > j:\n",
    "            submatrix = matrix[j*length:(j+1)*length,i*length:(i+1)*length]\n",
    "            c_sum = torch.sum(submatrix,dim=0).reshape(-1,1)\n",
    "            \n",
    "            r_sum = torch.sum(submatrix,dim=1).reshape(1,-1)\n",
    "            all_sum = torch.sum(submatrix)\n",
    "            Expectation = c_sum.matmul(r_sum)/all_sum\n",
    "            print(all_sum,c_sum,r_sum,Expectation)\n",
    "            Chi2 += torch.sum((submatrix-Expectation)**2/Expectation)\n",
    "            \n",
    "        \n",
    "print(Chi2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "# Get a batch of data\n",
    "for batch, labels in test_loader:\n",
    "    break\n",
    "    \n",
    "#get best model,easrly stopping\n",
    "\n",
    "viz = Visualizer(model)\n",
    "\n",
    "# Reconstruct data using Joint-VAE model\n",
    "recon = viz.reconstructions(batch)\n",
    "\n",
    "# face\n",
    "# recon=np.rollaxis(recon.numpy(), 0, 3)  \n",
    "# print(recon[265:,:,:].max())\n",
    "# recon[:,:,:]=(recon[:,:,:]+1)/2\n",
    "# plt.imshow(recon[:,:,:].astype(float))\n",
    "\n",
    "#MNIST\n",
    "plt.imshow(recon.numpy()[0, :, :].astype(float), cmap='gray')\n",
    "#plt.savefig(path+\"/recon.png\")\n",
    "print(recon.numpy()[0, :, :].max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCR():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont']\n",
    "        cov = covmatrix(mean)\n",
    "        cov[torch.abs(cov)<=1e-6]=0\n",
    "        cor = cov2cor(cov)\n",
    "        totalc += np.sum(cor) \n",
    "\n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "def TCV():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont']\n",
    "        cov = covmatrix(mean).cpu().detach().numpy()\n",
    "        cov = cov-np.diag(np.diag(cov))\n",
    "        #print(np.sum(cov**2) )\n",
    "        totalc += np.sum(cov**2) \n",
    "        \n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "\n",
    "def covmatrix(mean):\n",
    "    exp_mu = torch.mean(mean, dim=0)  #####mean through batch\n",
    "\n",
    "    # expectation of mu mu.tranpose\n",
    "    mu_expand1 = mean.unsqueeze(1)  #####(batch_size, 1, number of mean of latent variables)\n",
    "    mu_expand2 = mean.unsqueeze(2)  #####(batch_size, number of mean of latent variables, 1) ignore batch_size, only transpose the means\n",
    "    exp_mu_mu_t = torch.mean(mu_expand1 * mu_expand2, dim=0)\n",
    "\n",
    "    # covariance of model mean\n",
    "    cov = exp_mu_mu_t - exp_mu.unsqueeze(0) * exp_mu.unsqueeze(1) \n",
    "    return cov\n",
    "def cov2cor(c):\n",
    "    #input batch * n_cont\n",
    "    c = c.cpu().detach()\n",
    "    d=np.zeros_like(c)\n",
    "    for i in range(c.shape[0]):\n",
    "        for j in range(c.shape[1]):\n",
    "            d[i,j]=c[i,j]/(np.sqrt(c[i,i]*c[j,j]+1e-10))\n",
    "    return d\n",
    "tcor=TCR()\n",
    "tcov=TCV()\n",
    "print(tcor,tcov)\n",
    "trainer.evaluate(test_loader)\n",
    "#16  12.551628477254491 2.2266025315596838e-05 Valid Loss: 220.690, Recon Error: 0.185\n",
    "#32  32.79880483590873 0.26285673431150475 Valid Loss: 68.768, Recon Error: 0.0145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###latent space T-SNE visualization\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "samples = torch.zeros(1)\n",
    "labels = torch.zeros(1)\n",
    "for i in range(10):\n",
    "    test_batch = iter(test_loader)\n",
    "    test_batch = next(test_batch)\n",
    "    new_labels =torch.tensor(test_batch[1])\n",
    "    latent_dist= model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "    new_samples = model.reparameterize(latent_dist)\n",
    "    if torch.sum(samples) == 0:\n",
    "        samples =new_samples\n",
    "        labels = new_labels\n",
    "    else:\n",
    "        samples = torch.cat((samples,new_samples),0)\n",
    "        labels = torch.cat((labels, new_labels),0)\n",
    "    #print(samples.shape)\n",
    "    \n",
    "##latent_varibales should be N,D--->N,2\n",
    "\n",
    "\n",
    "# latent_variables = samples.reshape(samples[0],-1)\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "tsne.fit_transform(samples.detach().cpu().numpy())\n",
    "\n",
    "plt.scatter(tsne.embedding_[:,0],tsne.embedding_[:,1])\n",
    "#plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 10 # Number of labels\n",
    "\n",
    "# setup the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "# define the data\n",
    "x = tsne.embedding_[:,0]\n",
    "y = tsne.embedding_[:,1]\n",
    "tag = labels# Tag each point with a corresponding label    \n",
    "\n",
    "# define the colormap\n",
    "cmap = plt.cm.jet\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# create the new map\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0,N,N+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# make the scatter\n",
    "scat = ax.scatter(x,y,c=tag,s=np.random.randint(100,110,N),cmap=cmap,     norm=norm)\n",
    "# create the colorbar\n",
    "cb = plt.colorbar(scat, spacing='proportional',ticks=bounds)\n",
    "cb.set_label('Custom cbar')\n",
    "ax.set_title('Discrete color mappings')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "plt.savefig(path+\"/scatter.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t-SNE demo\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "X = np.arange(40).reshape(5,4,2)\n",
    "\n",
    "X_new = X.reshape(5,-1)\n",
    "#X = np.array([[[0,0], [0,0], [0,0]], [[0,0], [0,1], [1,1]], [[1,1], [1,0], [0,1]], [[1,1], [1,1], [1,1]]])\n",
    "print(X.shape,X)\n",
    "print(\"--------\")\n",
    "print(X_new)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne.fit_transform(X)\n",
    "print(tsne.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot samples\n",
    "\n",
    "samples = viz.samples()\n",
    "plt.imshow(samples.numpy()[0, :174, :], cmap='gray')\n",
    "print(np.sum(samples.numpy()[0, :174, :]))\n",
    "print(samples.numpy()[0, :, :].shape)\n",
    "####origin\n",
    "4*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot samples\n",
    "import matplotlib as mpl\n",
    "\n",
    "#MNIST\n",
    "samples = viz.samples()\n",
    "sample=samples.numpy()[0, :, :]/2+0.5\n",
    "plt.imshow(sample, cmap='gray')\n",
    "plt.imsave(path+\"/samples\",samples.numpy()[0, :, :]/2+0.5, cmap='gray')\n",
    "\n",
    "print((sample).min())\n",
    "\n",
    "# face\n",
    "# fig = plt.figure(figsize=(50, 50)) \n",
    "# samples = viz.samples()\n",
    "# samples = np.rollaxis(samples.numpy(), 0, 3)  \n",
    "# print(samples[:,:,0].max())\n",
    "# samples=(samples+1)/2\n",
    "# plt.imshow(samples.astype(float),norm = norm)\n",
    "# plt.imsave(path+\"/samples\",samples)\n",
    "###DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot all traversals\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "traversals = viz.all_latent_traversals(size=10)\n",
    "\n",
    "#MNIST\n",
    "plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "plt.imsave(path+\"/all_traversals\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "#face\n",
    "# traversals = np.rollaxis(traversals.numpy(), 0, 3)  \n",
    "# traversals=(traversals+1)/2\n",
    "# plt.imshow(traversals)\n",
    "# plt.imsave(path+\"/all_traversals\",traversals)\n",
    "###dip[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
    "#         0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=5, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "#MNIST\n",
    "plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "plt.imsave(path+\"/contVSdisc\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "traversals.numpy()[0, :, :].max()\n",
    "#face\n",
    "# traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "# traversals=(traversals+1)/2\n",
    "# plt.imshow(traversals)\n",
    "# plt.imsave(path+\"/contVSdisc\",traversals)\n",
    "##origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_t = viz.all_latent_traversals()\n",
    "print(all_t.shape)\n",
    "plt.imshow(all_t.numpy()[0, :, :], cmap='gray')\n",
    "plt.imsave(\"figures/beta/all_\",traversals.numpy()[0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "# Plot a grid of some traversals\n",
    "\n",
    "fig = plt.figure(figsize=(70, 70))  # width, height in inches\n",
    "print(\"continuous\")\n",
    "for i in range(n_cont):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=i, disc_idx=None,size=12)\n",
    "    \n",
    "    #MNIST\n",
    "    sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "    plt.savefig(path+\"/cont{}.png\".format(i))\n",
    "    plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "    \n",
    "    #FACE\n",
    "#     traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "#     sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "#     traversals=(traversals+1)/2\n",
    "#     plt.imshow(traversals)   \n",
    "plt.savefig(path+\"/cont.png\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"discrete\")\n",
    "for i in range(n_disc):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=None, disc_idx=i,size=10)\n",
    "    ##MNIST\n",
    "    sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "    plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "    plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "    #FACE\n",
    "#     traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "#     sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "# traversals=(traversals+1)/2\n",
    "# plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "# plt.imshow(traversals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "    \n",
    "# face    \n",
    "# def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "#         # Generate latent traversal\n",
    "# #         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "# #                                                              disc_idx=disc_idx,\n",
    "# #                                                              size=size)\n",
    "#         dim = n_cont + sum(disc)\n",
    "#         if prior:\n",
    "#             latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "#         else:\n",
    "#             latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "#         latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "#         latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "#         # Map samples through decoder\n",
    "#         generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "#         generated  = np.rollaxis(generated.detach().numpy(), 0, 3)\n",
    "#         generated = (generated +1)/2\n",
    "#         print(generated.min(),generated.max())\n",
    "#         plt.imshow(generated)\n",
    "\n",
    "        \n",
    "# def decode_latents(model, latent_samples):\n",
    "\n",
    "#         latent_samples = Variable(latent_samples)\n",
    "#         if model.use_cuda:\n",
    "#             latent_samples = latent_samples.cuda()\n",
    "#             result = model.decode(latent_samples).cpu()\n",
    "#         return result\n",
    "\n",
    "#MNIST\n",
    "def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "        # Generate latent traversal\n",
    "#         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "#                                                              disc_idx=disc_idx,\n",
    "#                                                              size=size)\n",
    "        dim = n_cont + sum(disc)\n",
    "        if prior:\n",
    "            latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "        else:\n",
    "            latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "        latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "        latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "        # Map samples through decoder\n",
    "        generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "        plt.imshow(generated.detach().numpy(),cmap=\"gray\")\n",
    "\n",
    "        \n",
    "def decode_latents(model, latent_samples):\n",
    "\n",
    "        latent_samples = Variable(latent_samples)\n",
    "        if model.use_cuda:\n",
    "            latent_samples = latent_samples.cuda()\n",
    "        return model.decode(latent_samples).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "def interactive_view(model,n_cont,disc):\n",
    "    \n",
    "    \n",
    "    interact(single_traversal,model=fixed(model),\n",
    "             n_cont=fixed(n_cont), cont_idx=(0,n_cont,1), cont_v=(-2.5,2.5,0.5),\n",
    "             disc=fixed(disc),disc_idx=(0,9,1),\n",
    "             prior=True);\n",
    "             \n",
    "interactive_view(model,n_cont,disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
