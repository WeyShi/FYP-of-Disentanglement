{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162079 20259 20261\n",
      "torch.Size([128, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "IMAGE_PATH = 'img_align_celeba/'\n",
    "image_size = 64\n",
    "# SAMPLE_PATH = '../'\n",
    "\n",
    "# if not os.path.exists(SAMPLE_PATH):\n",
    "#     os.makedirs(SAMPLE_PATH)\n",
    "    \n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    #transforms.Scale(image_size),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop((image_size,image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "data_loader = ImageFolder(IMAGE_PATH, transform)\n",
    "\n",
    "\n",
    "#data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "valid_loader, train_loader, test_loader = get_celeba_dataloader(data_loader, \n",
    "                                                                batch_size=128)\n",
    "test_batch = iter(test_loader)\n",
    "test_batch = next(test_batch)\n",
    "new_labels =torch.tensor(test_batch[1])\n",
    "print(torch.tensor(test_batch[0]).shape)\n",
    "#latent_dist = model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "limit_a, limit_b, epsilon = -.5, 1.5, 1e-6\n",
    "eps = np.linspace(0.1,0.9,20)\n",
    "def sigmoid(x):\n",
    "    y = 1./(1.+np.exp(-x))\n",
    "    return y\n",
    "def quantile_concrete(x,temperature,qz_loga):\n",
    "        \n",
    "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
    "        y = sigmoid((np.log(x) - np.log(1 - x) + qz_loga) / temperature)\n",
    "        return y * (limit_b - limit_a) + limit_a\n",
    "\n",
    "z = quantile_concrete(eps,1/20,2)\n",
    "z[z>=1]=1\n",
    "z[z<=0]=0\n",
    "print(z)\n",
    "plt.plot(eps,z)\n",
    "droprate_init = 0.2\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.zeros(4), requires_grad=False).data.normal_(math.log(1 - droprate_init) - math.log(droprate_init), 1e-2)\n",
    "z=(x+y).detach()\n",
    "print(y)\n",
    "#z.is_leaf \n",
    "a=torch.ones((64,32))\n",
    "b=torch.ones((1,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (img_to_features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (features_to_hidden): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_mean): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (fc_log_var): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (fc_alphas): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (latent_to_features): Sequential(\n",
      "    (0): Linear(in_features=42, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (features_to_img): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader \n",
    "from torchvision import transforms \n",
    "from torchvision.datasets import ImageFolder \n",
    "from torch.utils.data import DataLoader \n",
    "import os \n",
    "import torch\n",
    "from jointvae.VAEmodel_f import VAE\n",
    "from jointvae.training_l import Trainer\n",
    "from torch import optim\n",
    "from viz.visualize_l import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#valid_loader, train_loader, test_loader = get_mnist_dataloaders(batch_size=64)\n",
    "\n",
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions  7-14\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "n_cont = 32\n",
    "disc = [10]\n",
    "n_disc = len(disc)\n",
    "latent_spec = {'cont': n_cont,\n",
    "               'disc': disc}\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = VAE(latent_spec=latent_spec, img_size=(3, 64, 64)).cuda()\n",
    "#model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32)).cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "lr=5e-4\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "gamma=1.0\n",
    "cont_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "\n",
    "lambda_d = 2\n",
    "lambda_od = 10*lambda_d\n",
    "lambda_dis = 30*lambda_d \n",
    "path=\"ReportFig/DIP-VAE/face/cont_{}/gamma_ {}lambda{}\".format(n_cont,gamma,lambda_d)\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity,lambda_d = lambda_d,\n",
    "                  lambda_od = lambda_od, lambda_dis = lambda_dis )\n",
    "# Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from jointvae.training import Trainer\n",
    "\n",
    "\n",
    "trainer._train_epoch(train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/162079\tLoss: 1502.399\n",
      "6400/162079\tLoss: 1400.561\n",
      "12800/162079\tLoss: 1044.854\n",
      "19200/162079\tLoss: 710.496\n",
      "25600/162079\tLoss: 592.199\n",
      "32000/162079\tLoss: 555.055\n",
      "38400/162079\tLoss: 528.516\n",
      "44800/162079\tLoss: 508.156\n",
      "51200/162079\tLoss: 489.663\n",
      "57600/162079\tLoss: 473.128\n",
      "64000/162079\tLoss: 462.169\n",
      "70400/162079\tLoss: 452.076\n",
      "76800/162079\tLoss: 445.350\n",
      "83200/162079\tLoss: 438.965\n",
      "89600/162079\tLoss: 431.180\n",
      "96000/162079\tLoss: 425.167\n",
      "102400/162079\tLoss: 421.128\n",
      "108800/162079\tLoss: 414.498\n",
      "115200/162079\tLoss: 409.632\n",
      "121600/162079\tLoss: 404.323\n",
      "128000/162079\tLoss: 406.054\n",
      "134400/162079\tLoss: 401.276\n",
      "140800/162079\tLoss: 395.525\n",
      "147200/162079\tLoss: 396.513\n",
      "153600/162079\tLoss: 387.841\n",
      "160000/162079\tLoss: 386.463\n",
      "Valid Loss: 388.442, Recon Error: 0.078\n",
      "388.44219548447325\n",
      "Epoch: 1 Average loss: 518.62 Valid loss: 388.44219548447325\tRecon Error:0.078\n",
      "0/162079\tLoss: 385.745\n",
      "6400/162079\tLoss: 382.347\n",
      "12800/162079\tLoss: 380.802\n",
      "19200/162079\tLoss: 376.099\n",
      "25600/162079\tLoss: 374.636\n",
      "32000/162079\tLoss: 369.195\n",
      "38400/162079\tLoss: 367.900\n",
      "44800/162079\tLoss: 368.220\n",
      "51200/162079\tLoss: 364.802\n",
      "57600/162079\tLoss: 361.651\n",
      "64000/162079\tLoss: 359.125\n",
      "70400/162079\tLoss: 361.061\n",
      "76800/162079\tLoss: 356.472\n",
      "83200/162079\tLoss: 356.112\n",
      "89600/162079\tLoss: 352.426\n",
      "96000/162079\tLoss: 353.456\n",
      "102400/162079\tLoss: 352.499\n",
      "108800/162079\tLoss: 352.820\n",
      "115200/162079\tLoss: 349.221\n",
      "121600/162079\tLoss: 348.457\n",
      "128000/162079\tLoss: 349.191\n",
      "134400/162079\tLoss: 348.976\n",
      "140800/162079\tLoss: 343.529\n",
      "147200/162079\tLoss: 347.678\n",
      "153600/162079\tLoss: 346.306\n",
      "160000/162079\tLoss: 343.527\n",
      "Valid Loss: 346.837, Recon Error: 0.055\n",
      "346.8368639676076\n",
      "Epoch: 2 Average loss: 358.72 Valid loss: 346.8368639676076\tRecon Error:0.055\n",
      "0/162079\tLoss: 352.260\n",
      "6400/162079\tLoss: 345.344\n",
      "12800/162079\tLoss: 344.597\n",
      "19200/162079\tLoss: 340.460\n",
      "25600/162079\tLoss: 342.494\n",
      "32000/162079\tLoss: 341.731\n",
      "38400/162079\tLoss: 342.499\n",
      "44800/162079\tLoss: 340.044\n",
      "51200/162079\tLoss: 338.499\n",
      "57600/162079\tLoss: 337.870\n",
      "64000/162079\tLoss: 337.926\n",
      "70400/162079\tLoss: 336.993\n",
      "76800/162079\tLoss: 338.437\n",
      "83200/162079\tLoss: 334.718\n",
      "89600/162079\tLoss: 334.442\n",
      "96000/162079\tLoss: 334.576\n",
      "102400/162079\tLoss: 334.752\n",
      "108800/162079\tLoss: 335.602\n",
      "115200/162079\tLoss: 334.138\n",
      "121600/162079\tLoss: 337.083\n",
      "128000/162079\tLoss: 331.252\n",
      "134400/162079\tLoss: 330.678\n",
      "140800/162079\tLoss: 333.046\n",
      "147200/162079\tLoss: 330.844\n",
      "153600/162079\tLoss: 332.177\n",
      "160000/162079\tLoss: 331.161\n",
      "Valid Loss: 335.396, Recon Error: 0.060\n",
      "335.39593467472486\n",
      "Epoch: 3 Average loss: 337.06 Valid loss: 335.39593467472486\tRecon Error:0.060\n",
      "0/162079\tLoss: 315.631\n",
      "6400/162079\tLoss: 331.092\n",
      "12800/162079\tLoss: 330.288\n",
      "19200/162079\tLoss: 331.093\n",
      "25600/162079\tLoss: 329.396\n",
      "32000/162079\tLoss: 330.757\n",
      "38400/162079\tLoss: 328.329\n",
      "44800/162079\tLoss: 331.786\n",
      "51200/162079\tLoss: 328.287\n",
      "57600/162079\tLoss: 327.964\n",
      "64000/162079\tLoss: 325.991\n",
      "70400/162079\tLoss: 327.547\n",
      "76800/162079\tLoss: 326.110\n",
      "83200/162079\tLoss: 328.181\n",
      "89600/162079\tLoss: 328.942\n",
      "96000/162079\tLoss: 329.377\n",
      "102400/162079\tLoss: 326.617\n",
      "108800/162079\tLoss: 324.234\n",
      "115200/162079\tLoss: 323.861\n",
      "121600/162079\tLoss: 326.522\n",
      "128000/162079\tLoss: 324.662\n",
      "134400/162079\tLoss: 327.401\n",
      "140800/162079\tLoss: 324.464\n",
      "147200/162079\tLoss: 324.908\n",
      "153600/162079\tLoss: 322.483\n",
      "160000/162079\tLoss: 328.623\n",
      "Valid Loss: 330.622, Recon Error: 0.054\n",
      "330.62248028449295\n",
      "Epoch: 4 Average loss: 327.77 Valid loss: 330.62248028449295\tRecon Error:0.054\n",
      "0/162079\tLoss: 333.899\n",
      "6400/162079\tLoss: 324.919\n",
      "12800/162079\tLoss: 322.274\n",
      "19200/162079\tLoss: 321.502\n",
      "25600/162079\tLoss: 321.802\n",
      "32000/162079\tLoss: 320.897\n",
      "38400/162079\tLoss: 323.719\n",
      "44800/162079\tLoss: 322.040\n",
      "51200/162079\tLoss: 321.688\n",
      "57600/162079\tLoss: 321.655\n",
      "64000/162079\tLoss: 320.255\n",
      "70400/162079\tLoss: 321.273\n",
      "76800/162079\tLoss: 319.726\n",
      "83200/162079\tLoss: 319.232\n",
      "89600/162079\tLoss: 318.549\n",
      "96000/162079\tLoss: 318.534\n",
      "102400/162079\tLoss: 317.524\n",
      "108800/162079\tLoss: 319.814\n",
      "115200/162079\tLoss: 321.974\n",
      "121600/162079\tLoss: 319.097\n",
      "128000/162079\tLoss: 319.198\n",
      "134400/162079\tLoss: 318.781\n",
      "140800/162079\tLoss: 319.868\n",
      "147200/162079\tLoss: 315.869\n",
      "153600/162079\tLoss: 317.737\n",
      "160000/162079\tLoss: 316.439\n",
      "Valid Loss: 320.703, Recon Error: 0.057\n",
      "320.7028347951061\n",
      "Epoch: 5 Average loss: 320.37 Valid loss: 320.7028347951061\tRecon Error:0.057\n",
      "0/162079\tLoss: 318.356\n",
      "6400/162079\tLoss: 319.067\n",
      "12800/162079\tLoss: 317.333\n",
      "19200/162079\tLoss: 315.854\n",
      "25600/162079\tLoss: 315.145\n",
      "32000/162079\tLoss: 316.889\n",
      "38400/162079\tLoss: 318.036\n",
      "44800/162079\tLoss: 317.815\n",
      "51200/162079\tLoss: 316.388\n",
      "57600/162079\tLoss: 315.506\n",
      "64000/162079\tLoss: 317.007\n",
      "70400/162079\tLoss: 313.457\n",
      "76800/162079\tLoss: 314.274\n",
      "83200/162079\tLoss: 314.653\n",
      "89600/162079\tLoss: 317.204\n",
      "96000/162079\tLoss: 314.231\n",
      "102400/162079\tLoss: 314.450\n",
      "108800/162079\tLoss: 313.310\n",
      "115200/162079\tLoss: 315.004\n",
      "121600/162079\tLoss: 314.479\n",
      "128000/162079\tLoss: 314.664\n",
      "134400/162079\tLoss: 313.854\n",
      "140800/162079\tLoss: 312.434\n",
      "147200/162079\tLoss: 313.010\n",
      "153600/162079\tLoss: 311.279\n",
      "160000/162079\tLoss: 312.769\n",
      "Valid Loss: 319.358, Recon Error: 0.052\n",
      "319.3576756123477\n",
      "Epoch: 6 Average loss: 315.27 Valid loss: 319.3576756123477\tRecon Error:0.052\n",
      "0/162079\tLoss: 318.696\n",
      "6400/162079\tLoss: 315.449\n",
      "12800/162079\tLoss: 311.411\n",
      "19200/162079\tLoss: 310.483\n",
      "25600/162079\tLoss: 314.336\n",
      "32000/162079\tLoss: 311.932\n",
      "38400/162079\tLoss: 313.405\n",
      "44800/162079\tLoss: 312.138\n",
      "51200/162079\tLoss: 313.848\n",
      "57600/162079\tLoss: 312.167\n",
      "64000/162079\tLoss: 313.553\n",
      "70400/162079\tLoss: 312.695\n",
      "76800/162079\tLoss: 310.123\n",
      "83200/162079\tLoss: 313.724\n",
      "89600/162079\tLoss: 311.667\n",
      "96000/162079\tLoss: 310.421\n",
      "102400/162079\tLoss: 312.147\n",
      "108800/162079\tLoss: 309.842\n",
      "115200/162079\tLoss: 313.514\n",
      "121600/162079\tLoss: 312.469\n",
      "128000/162079\tLoss: 311.311\n",
      "134400/162079\tLoss: 311.618\n",
      "140800/162079\tLoss: 310.458\n",
      "147200/162079\tLoss: 310.048\n",
      "153600/162079\tLoss: 309.812\n",
      "160000/162079\tLoss: 310.328\n",
      "Valid Loss: 312.120, Recon Error: 0.048\n",
      "312.11958495326013\n",
      "Epoch: 7 Average loss: 312.12 Valid loss: 312.11958495326013\tRecon Error:0.048\n",
      "0/162079\tLoss: 305.956\n",
      "6400/162079\tLoss: 311.739\n",
      "12800/162079\tLoss: 307.267\n",
      "19200/162079\tLoss: 309.977\n",
      "25600/162079\tLoss: 309.765\n",
      "32000/162079\tLoss: 309.306\n",
      "38400/162079\tLoss: 311.307\n",
      "44800/162079\tLoss: 308.976\n",
      "51200/162079\tLoss: 308.543\n",
      "57600/162079\tLoss: 309.460\n",
      "64000/162079\tLoss: 307.544\n",
      "70400/162079\tLoss: 309.963\n",
      "76800/162079\tLoss: 309.612\n",
      "83200/162079\tLoss: 309.760\n",
      "89600/162079\tLoss: 309.647\n",
      "96000/162079\tLoss: 308.333\n",
      "102400/162079\tLoss: 308.134\n",
      "108800/162079\tLoss: 308.068\n",
      "115200/162079\tLoss: 310.947\n",
      "121600/162079\tLoss: 309.147\n",
      "128000/162079\tLoss: 306.620\n",
      "134400/162079\tLoss: 310.591\n",
      "140800/162079\tLoss: 308.553\n",
      "147200/162079\tLoss: 306.614\n",
      "153600/162079\tLoss: 308.299\n",
      "160000/162079\tLoss: 307.038\n",
      "Valid Loss: 308.573, Recon Error: 0.052\n",
      "308.573118197843\n",
      "Epoch: 8 Average loss: 309.22 Valid loss: 308.573118197843\tRecon Error:0.052\n",
      "0/162079\tLoss: 320.838\n",
      "6400/162079\tLoss: 307.644\n",
      "12800/162079\tLoss: 307.273\n",
      "19200/162079\tLoss: 306.867\n",
      "25600/162079\tLoss: 309.653\n",
      "32000/162079\tLoss: 305.841\n",
      "38400/162079\tLoss: 307.126\n",
      "44800/162079\tLoss: 307.193\n",
      "51200/162079\tLoss: 306.172\n",
      "57600/162079\tLoss: 307.895\n",
      "64000/162079\tLoss: 303.518\n",
      "70400/162079\tLoss: 304.605\n",
      "76800/162079\tLoss: 305.961\n",
      "83200/162079\tLoss: 305.124\n",
      "89600/162079\tLoss: 305.882\n",
      "96000/162079\tLoss: 303.847\n",
      "102400/162079\tLoss: 305.028\n",
      "108800/162079\tLoss: 304.433\n",
      "115200/162079\tLoss: 306.188\n",
      "121600/162079\tLoss: 306.015\n",
      "128000/162079\tLoss: 306.710\n",
      "134400/162079\tLoss: 304.843\n",
      "140800/162079\tLoss: 308.031\n",
      "147200/162079\tLoss: 306.656\n",
      "153600/162079\tLoss: 304.865\n",
      "160000/162079\tLoss: 305.167\n",
      "Valid Loss: 310.661, Recon Error: 0.047\n",
      "310.66082283835743\n",
      "Epoch: 9 Average loss: 306.38 Valid loss: 310.66082283835743\tRecon Error:0.047\n"
     ]
    }
   ],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "###1e-5 6859 1e-4 6727 5e-4 6722 try tanh/L1 loss/beta--->DIP\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "trainer.train(train_loader,valid_loader, epochs=100, save_training_gif=('./training.gif', viz))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "torch.save(model.state_dict(), 'modelDIPf_params.pkl')\n",
    "torch.save(model, './modelDIPf')\n",
    "##15.078 - 0.0147  17.209 - 0.0168 error tanh \n",
    "##LR 1e-3 0.019-0.023 worse should pick 5e-4\n",
    "##PLOT THE CURVE!!!!!\n",
    "#16-2033.294813156128 32-2846.2241864204407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_spec=latent_spec, img_size=(3, 64, 64)).cuda()\n",
    "model.load_state_dict(torch.load('modelDIPf_params.pkl'))\n",
    "#path=\"figures/face/cont_{}/pruned_Beta_ {}lamba{}_ONLYPAIR\".format(n_cont,gamma,0.1)\n",
    "loss = trainer.get_losses()\n",
    "print(len(loss[\"DIP_loss\"]))\n",
    "print(lr)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.cuda.is_available()\n",
    "# device = torch.device('cuda')\n",
    "# print(device)\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Chi-square test\n",
    "import torch\n",
    "tensor_one = torch.tensor([[1,2,3],[4,5,6]])\n",
    "tensor_two = torch.tensor([[6,8,9],[10,11,12]])\n",
    "tensor_list = [tensor_one, tensor_two]\n",
    "tens_list = []\n",
    "for tensor in tensor_list:\n",
    "    \n",
    "    print(tensor)\n",
    "    length = tensor.shape[1]\n",
    "    tens_list.append(torch.mean(tensor.float(),dim=0))\n",
    "    \n",
    "tens_list = torch.stack(tens_list).reshape(1,-1)\n",
    "tens_listT = tens_list.t()\n",
    "matrix = tens_listT.matmul(tens_list)\n",
    "print(matrix)\n",
    "print(\"--------\")\n",
    "Chi2 =0\n",
    "for i in range(len(tensor_list)):\n",
    "    for j in range(len(tensor_list)):\n",
    "        if i > j:\n",
    "            submatrix = matrix[j*length:(j+1)*length,i*length:(i+1)*length]\n",
    "            c_sum = torch.sum(submatrix,dim=0).reshape(-1,1)\n",
    "            \n",
    "            r_sum = torch.sum(submatrix,dim=1).reshape(1,-1)\n",
    "            all_sum = torch.sum(submatrix)\n",
    "            Expectation = c_sum.matmul(r_sum)/all_sum\n",
    "            print(all_sum,c_sum,r_sum,Expectation)\n",
    "            Chi2 += torch.sum((submatrix-Expectation)**2/Expectation)\n",
    "            \n",
    "        \n",
    "print(Chi2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "# Get a batch of data\n",
    "for batch, labels in test_loader:\n",
    "    break\n",
    "    \n",
    "#get best model,easrly stopping\n",
    "\n",
    "viz = Visualizer(model)\n",
    "\n",
    "# Reconstruct data using Joint-VAE model\n",
    "recon = viz.reconstructions(batch)\n",
    "\n",
    "# face\n",
    "recon=np.rollaxis(recon.numpy(), 0, 3)  \n",
    "print(recon[265:,:,:].max())\n",
    "recon[:,:,:]=(recon[:,:,:]+1)/2\n",
    "plt.imshow(recon[:,:,:].astype(float))\n",
    "print(recon[:,:,:].min())\n",
    "\n",
    "#MNIST\n",
    "# plt.imshow(recon.numpy()[0, :, :].astype(float), cmap='gray')\n",
    "# print(recon.numpy()[0, :, :].max())\n",
    "plt.savefig(path+\"/recon.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCR():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont']\n",
    "        cov = covmatrix(mean)\n",
    "        cov[torch.abs(cov)<=1e-6]=0\n",
    "        cor = cov2cor(cov)\n",
    "        totalc += np.sum(cor) \n",
    "\n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "def TCV():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont']\n",
    "        cov = covmatrix(mean).cpu().detach().numpy()\n",
    "        cov = cov-np.diag(np.diag(cov))\n",
    "        #print(np.sum(cov**2) )\n",
    "        totalc += np.sum(cov**2) \n",
    "        \n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "\n",
    "def covmatrix(mean):\n",
    "    exp_mu = torch.mean(mean, dim=0)  #####mean through batch\n",
    "\n",
    "    # expectation of mu mu.tranpose\n",
    "    mu_expand1 = mean.unsqueeze(1)  #####(batch_size, 1, number of mean of latent variables)\n",
    "    mu_expand2 = mean.unsqueeze(2)  #####(batch_size, number of mean of latent variables, 1) ignore batch_size, only transpose the means\n",
    "    exp_mu_mu_t = torch.mean(mu_expand1 * mu_expand2, dim=0)\n",
    "\n",
    "    # covariance of model mean\n",
    "    cov = exp_mu_mu_t - exp_mu.unsqueeze(0) * exp_mu.unsqueeze(1) \n",
    "    return cov\n",
    "def cov2cor(c):\n",
    "    #input batch * n_cont\n",
    "    c = c.cpu().detach()\n",
    "    d=np.zeros_like(c)\n",
    "    for i in range(c.shape[0]):\n",
    "        for j in range(c.shape[1]):\n",
    "            d[i,j]=c[i,j]/(np.sqrt(c[i,i]*c[j,j]+1e-10))\n",
    "    return d\n",
    "tcor=TCR()\n",
    "tcov=TCV()\n",
    "print(tcor,tcov)\n",
    "trainer.evaluate(test_loader)\n",
    "#16  12.551628477254491 2.2266025315596838e-05 Valid Loss: 220.690, Recon Error: 0.185\n",
    "#32  32.79880483590873 0.26285673431150475 Valid Loss: 68.768, Recon Error: 0.0145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###latent space T-SNE visualization\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "samples = torch.zeros(1)\n",
    "labels = torch.zeros(1)\n",
    "for i in range(10):\n",
    "    test_batch = iter(test_loader)\n",
    "    test_batch = next(test_batch)\n",
    "    new_labels =torch.tensor(test_batch[1])\n",
    "    latent_dist= model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "    new_samples = model.reparameterize(latent_dist)\n",
    "    if torch.sum(samples) == 0:\n",
    "        samples =new_samples\n",
    "        labels = new_labels\n",
    "    else:\n",
    "        samples = torch.cat((samples,new_samples),0)\n",
    "        labels = torch.cat((labels, new_labels),0)\n",
    "    #print(samples.shape)\n",
    "    \n",
    "##latent_varibales should be N,D--->N,2\n",
    "\n",
    "\n",
    "# latent_variables = samples.reshape(samples[0],-1)\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "tsne.fit_transform(samples.detach().cpu().numpy())\n",
    "\n",
    "plt.scatter(tsne.embedding_[:,0],tsne.embedding_[:,1])\n",
    "#plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 10 # Number of labels\n",
    "\n",
    "# setup the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "# define the data\n",
    "x = tsne.embedding_[:,0]\n",
    "y = tsne.embedding_[:,1]\n",
    "tag = labels# Tag each point with a corresponding label    \n",
    "\n",
    "# define the colormap\n",
    "cmap = plt.cm.jet\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# create the new map\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0,N,N+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# make the scatter\n",
    "scat = ax.scatter(x,y,c=tag,s=np.random.randint(100,110,N),cmap=cmap,     norm=norm)\n",
    "# create the colorbar\n",
    "cb = plt.colorbar(scat, spacing='proportional',ticks=bounds)\n",
    "cb.set_label('Custom cbar')\n",
    "ax.set_title('Discrete color mappings')\n",
    "\n",
    "plt.savefig(path+\"/scatter.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t-SNE demo\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "X = np.arange(40).reshape(5,4,2)\n",
    "\n",
    "X_new = X.reshape(5,-1)\n",
    "#X = np.array([[[0,0], [0,0], [0,0]], [[0,0], [0,1], [1,1]], [[1,1], [1,0], [0,1]], [[1,1], [1,1], [1,1]]])\n",
    "print(X.shape,X)\n",
    "print(\"--------\")\n",
    "print(X_new)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne.fit_transform(X)\n",
    "print(tsne.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot samples\n",
    "\n",
    "samples = viz.samples()\n",
    "plt.imshow(samples.numpy()[0, :174, :], cmap='gray')\n",
    "print(np.sum(samples.numpy()[0, :174, :]))\n",
    "print(samples.numpy()[0, :, :].shape)\n",
    "####origin\n",
    "4*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot samples\n",
    "import matplotlib as mpl\n",
    "\n",
    "#MNIST\n",
    "# samples = viz.samples()\n",
    "# sample=samples.numpy()[0, :, :]/2+0.5\n",
    "# plt.imshow(sample, cmap='gray')\n",
    "# plt.imsave(path+\"/samples\",samples.numpy()[0, :, :]/2+0.5, cmap='gray')\n",
    "\n",
    "# face\n",
    "fig = plt.figure(figsize=(30, 30)) \n",
    "samples = viz.samples()\n",
    "samples = np.rollaxis(samples.numpy(), 0, 3)  \n",
    "print(samples[:,:,0].max())\n",
    "samples=(samples+1)/2\n",
    "plt.imshow(samples.astype(float),norm = norm)\n",
    "plt.imsave(path+\"/samples\",samples)\n",
    "###DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot all traversals\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "traversals = viz.all_latent_traversals(size=10)\n",
    "\n",
    "#MNIST\n",
    "# plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "# plt.imsave(path+\"/all_traversals\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "#face\n",
    "traversals = np.rollaxis(traversals.numpy(), 0, 3)  \n",
    "traversals=(traversals+1)/2\n",
    "plt.imshow(traversals)\n",
    "plt.imsave(path+\"/all_traversals\",traversals)\n",
    "###dip[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
    "#         0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=5, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "#MNIST\n",
    "# plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "# plt.imsave(path+\"/contVSdisc\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "# traversals.numpy()[0, :, :].max()\n",
    "#face\n",
    "traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "traversals=(traversals+1)/2\n",
    "plt.imshow(traversals)\n",
    "plt.imsave(path+\"/contVSdisc\",traversals)\n",
    "##origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_t = viz.all_latent_traversals()\n",
    "print(all_t.shape)\n",
    "plt.imshow(all_t.numpy()[0, :, :], cmap='gray')\n",
    "plt.imsave(\"figures/beta/all_\",traversals.numpy()[0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "# Plot a grid of some traversals\n",
    "\n",
    "fig = plt.figure(figsize=(70, 70))  # width, height in inches\n",
    "print(\"continuous\")\n",
    "for i in range(n_cont):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=i, disc_idx=None,size=12)\n",
    "    \n",
    "    #MNIST\n",
    "#     sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "#     plt.savefig(path+\"/cont{}.png\".format(i))\n",
    "#     plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "    \n",
    "    #FACE\n",
    "    traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "    sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "    traversals=(traversals+1)/2\n",
    "    plt.imshow(traversals)   \n",
    "plt.savefig(path+\"/cont.png\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"discrete\")\n",
    "for i in range(n_disc):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=None, disc_idx=i,size=10)\n",
    "    ##MNIST\n",
    "#     sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "#     plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "#     plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "    #FACE\n",
    "    traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "    sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "traversals=(traversals+1)/2\n",
    "plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "plt.imshow(traversals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "    \n",
    "# face    \n",
    "def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "        # Generate latent traversal\n",
    "#         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "#                                                              disc_idx=disc_idx,\n",
    "#                                                              size=size)\n",
    "        dim = n_cont + sum(disc)\n",
    "        if prior:\n",
    "            latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "        else:\n",
    "            latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "        latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "        latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "        # Map samples through decoder\n",
    "        generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "        generated  = np.rollaxis(generated.detach().numpy(), 0, 3)\n",
    "        generated = (generated +1)/2\n",
    "        print(generated.min(),generated.max())\n",
    "        plt.imshow(generated)\n",
    "\n",
    "        \n",
    "def decode_latents(model, latent_samples):\n",
    "\n",
    "        latent_samples = Variable(latent_samples)\n",
    "        if model.use_cuda:\n",
    "            latent_samples = latent_samples.cuda()\n",
    "            result = model.decode(latent_samples).cpu()\n",
    "        return result\n",
    "\n",
    "#MNIST\n",
    "# def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "#         # Generate latent traversal\n",
    "# #         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "# #                                                              disc_idx=disc_idx,\n",
    "# #                                                              size=size)\n",
    "#         dim = n_cont + sum(disc)\n",
    "#         if prior:\n",
    "#             latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "#         else:\n",
    "#             latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "#         latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "#         latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "#         # Map samples through decoder\n",
    "#         generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "#         plt.imshow(generated.detach().numpy(),cmap=\"gray\")\n",
    "\n",
    "        \n",
    "# def decode_latents(model, latent_samples):\n",
    "\n",
    "#         latent_samples = Variable(latent_samples)\n",
    "#         if model.use_cuda:\n",
    "#             latent_samples = latent_samples.cuda()\n",
    "#         return model.decode(latent_samples).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "def interactive_view(model,n_cont,disc):\n",
    "    \n",
    "    \n",
    "    interact(single_traversal,model=fixed(model),\n",
    "             n_cont=fixed(n_cont), cont_idx=(0,n_cont,1), cont_v=(-2.5,2.5,0.5),\n",
    "             disc=fixed(disc),disc_idx=(0,9,1),\n",
    "             prior=True);\n",
    "             \n",
    "interactive_view(model,n_cont,disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
