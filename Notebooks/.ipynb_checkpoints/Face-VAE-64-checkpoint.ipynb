{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162079 20259 20261\n",
      "torch.Size([128, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "IMAGE_PATH = 'img_align_celeba/'\n",
    "image_size = 64\n",
    "# SAMPLE_PATH = '../'\n",
    "\n",
    "# if not os.path.exists(SAMPLE_PATH):\n",
    "#     os.makedirs(SAMPLE_PATH)\n",
    "    \n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    #transforms.Scale(image_size),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop((image_size,image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "data_loader = ImageFolder(IMAGE_PATH, transform)\n",
    "\n",
    "\n",
    "#data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "valid_loader, train_loader, test_loader = get_celeba_dataloader(data_loader, \n",
    "                                                                batch_size=128)\n",
    "test_batch = iter(test_loader)\n",
    "test_batch = next(test_batch)\n",
    "new_labels =torch.tensor(test_batch[1])\n",
    "print(torch.tensor(test_batch[0]).shape)\n",
    "#latent_dist = model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "limit_a, limit_b, epsilon = -.5, 1.5, 1e-6\n",
    "eps = np.linspace(0.1,0.9,20)\n",
    "def sigmoid(x):\n",
    "    y = 1./(1.+np.exp(-x))\n",
    "    return y\n",
    "def quantile_concrete(x,temperature,qz_loga):\n",
    "        \n",
    "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
    "        y = sigmoid((np.log(x) - np.log(1 - x) + qz_loga) / temperature)\n",
    "        return y * (limit_b - limit_a) + limit_a\n",
    "\n",
    "z = quantile_concrete(eps,1/20,2)\n",
    "z[z>=1]=1\n",
    "z[z<=0]=0\n",
    "print(z)\n",
    "plt.plot(eps,z)\n",
    "droprate_init = 0.2\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.zeros(4), requires_grad=False).data.normal_(math.log(1 - droprate_init) - math.log(droprate_init), 1e-2)\n",
    "z=(x+y).detach()\n",
    "print(y)\n",
    "#z.is_leaf \n",
    "a=torch.ones((64,32))\n",
    "b=torch.ones((1,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (img_to_features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (features_to_hidden): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_mean): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (fc_log_var): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (fc_alphas): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (latent_to_features): Sequential(\n",
      "    (0): Linear(in_features=42, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (features_to_img): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader \n",
    "from torchvision import transforms \n",
    "from torchvision.datasets import ImageFolder \n",
    "from torch.utils.data import DataLoader \n",
    "import os \n",
    "import torch\n",
    "from jointvae.VAEmodel_f import VAE\n",
    "from jointvae.training_l import Trainer\n",
    "from torch import optim\n",
    "from viz.visualize_l import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#valid_loader, train_loader, test_loader = get_mnist_dataloaders(batch_size=64)\n",
    "\n",
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions  7-14\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "n_cont = 32\n",
    "disc = [10]\n",
    "n_disc = len(disc)\n",
    "latent_spec = {'cont': n_cont,\n",
    "               'disc': disc}\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = VAE(latent_spec=latent_spec, img_size=(3, 64, 64)).cuda()\n",
    "#model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32)).cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "lr=5e-4\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "gamma=1.0\n",
    "cont_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "\n",
    "lambda_d = 0\n",
    "lambda_od = 10*lambda_d\n",
    "lambda_dis = 30*lambda_d \n",
    "path=\"ReportFig/DIP-VAE/face/cont_{}/gamma_ {}lambda{}\".format(n_cont,gamma,lambda_d)\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity,lambda_d = lambda_d,\n",
    "                  lambda_od = lambda_od, lambda_dis = lambda_dis )\n",
    "# Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from jointvae.training import Trainer\n",
    "\n",
    "\n",
    "trainer._train_epoch(train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/162079\tLoss: 1355.925\n",
      "6400/162079\tLoss: 1304.921\n",
      "12800/162079\tLoss: 864.219\n",
      "19200/162079\tLoss: 558.462\n",
      "25600/162079\tLoss: 496.092\n",
      "32000/162079\tLoss: 461.739\n",
      "38400/162079\tLoss: 432.566\n",
      "44800/162079\tLoss: 413.176\n",
      "51200/162079\tLoss: 392.333\n",
      "57600/162079\tLoss: 382.879\n",
      "64000/162079\tLoss: 375.221\n",
      "70400/162079\tLoss: 367.607\n",
      "76800/162079\tLoss: 361.655\n",
      "83200/162079\tLoss: 352.556\n",
      "89600/162079\tLoss: 348.391\n",
      "96000/162079\tLoss: 341.656\n",
      "102400/162079\tLoss: 339.093\n",
      "108800/162079\tLoss: 334.688\n",
      "115200/162079\tLoss: 335.849\n",
      "121600/162079\tLoss: 330.334\n",
      "128000/162079\tLoss: 329.600\n",
      "134400/162079\tLoss: 324.875\n",
      "140800/162079\tLoss: 324.829\n",
      "147200/162079\tLoss: 321.941\n",
      "153600/162079\tLoss: 316.409\n",
      "160000/162079\tLoss: 310.695\n",
      "Valid Loss: 314.830, Recon Error: 0.071\n",
      "314.82986354228086\n",
      "Epoch: 1 Average loss: 428.42 Valid loss: 314.82986354228086\tRecon Error:0.071\n",
      "0/162079\tLoss: 302.562\n",
      "6400/162079\tLoss: 313.094\n",
      "12800/162079\tLoss: 309.960\n",
      "19200/162079\tLoss: 305.900\n",
      "25600/162079\tLoss: 299.026\n",
      "32000/162079\tLoss: 296.915\n",
      "38400/162079\tLoss: 297.972\n",
      "44800/162079\tLoss: 292.564\n",
      "51200/162079\tLoss: 290.833\n",
      "57600/162079\tLoss: 289.117\n",
      "64000/162079\tLoss: 286.415\n",
      "70400/162079\tLoss: 288.956\n",
      "76800/162079\tLoss: 282.936\n",
      "83200/162079\tLoss: 282.806\n",
      "89600/162079\tLoss: 282.969\n",
      "96000/162079\tLoss: 283.007\n",
      "102400/162079\tLoss: 279.826\n",
      "108800/162079\tLoss: 280.324\n",
      "115200/162079\tLoss: 279.768\n",
      "121600/162079\tLoss: 277.610\n",
      "128000/162079\tLoss: 275.508\n",
      "134400/162079\tLoss: 275.914\n",
      "140800/162079\tLoss: 277.766\n",
      "147200/162079\tLoss: 276.130\n",
      "153600/162079\tLoss: 275.195\n",
      "160000/162079\tLoss: 273.455\n",
      "Valid Loss: 273.644, Recon Error: 0.058\n",
      "273.64435284692536\n",
      "Epoch: 2 Average loss: 286.95 Valid loss: 273.64435284692536\tRecon Error:0.058\n",
      "0/162079\tLoss: 283.998\n",
      "6400/162079\tLoss: 272.702\n",
      "12800/162079\tLoss: 271.768\n",
      "19200/162079\tLoss: 273.376\n",
      "25600/162079\tLoss: 271.460\n",
      "32000/162079\tLoss: 272.552\n",
      "38400/162079\tLoss: 269.661\n",
      "44800/162079\tLoss: 270.245\n",
      "51200/162079\tLoss: 267.654\n",
      "57600/162079\tLoss: 267.829\n",
      "64000/162079\tLoss: 268.361\n",
      "70400/162079\tLoss: 269.127\n",
      "76800/162079\tLoss: 266.837\n",
      "83200/162079\tLoss: 268.685\n",
      "89600/162079\tLoss: 268.776\n",
      "96000/162079\tLoss: 267.449\n",
      "102400/162079\tLoss: 268.592\n",
      "108800/162079\tLoss: 266.789\n",
      "115200/162079\tLoss: 263.638\n",
      "121600/162079\tLoss: 265.951\n",
      "128000/162079\tLoss: 268.080\n",
      "134400/162079\tLoss: 265.362\n",
      "140800/162079\tLoss: 264.499\n",
      "147200/162079\tLoss: 264.152\n",
      "153600/162079\tLoss: 262.117\n",
      "160000/162079\tLoss: 266.455\n",
      "Valid Loss: 265.209, Recon Error: 0.055\n",
      "265.20914771122006\n",
      "Epoch: 3 Average loss: 268.22 Valid loss: 265.20914771122006\tRecon Error:0.055\n",
      "0/162079\tLoss: 266.765\n",
      "6400/162079\tLoss: 263.440\n",
      "12800/162079\tLoss: 263.071\n",
      "19200/162079\tLoss: 263.525\n",
      "25600/162079\tLoss: 261.747\n",
      "32000/162079\tLoss: 264.377\n",
      "38400/162079\tLoss: 262.508\n",
      "44800/162079\tLoss: 259.578\n",
      "51200/162079\tLoss: 260.711\n",
      "57600/162079\tLoss: 260.516\n",
      "64000/162079\tLoss: 260.126\n",
      "70400/162079\tLoss: 262.205\n",
      "76800/162079\tLoss: 261.692\n",
      "83200/162079\tLoss: 260.447\n",
      "89600/162079\tLoss: 262.226\n",
      "96000/162079\tLoss: 259.845\n",
      "102400/162079\tLoss: 261.211\n",
      "108800/162079\tLoss: 260.812\n",
      "115200/162079\tLoss: 259.541\n",
      "121600/162079\tLoss: 258.063\n",
      "128000/162079\tLoss: 260.348\n",
      "134400/162079\tLoss: 259.321\n",
      "140800/162079\tLoss: 258.308\n",
      "147200/162079\tLoss: 257.416\n",
      "153600/162079\tLoss: 257.514\n",
      "160000/162079\tLoss: 258.425\n",
      "Valid Loss: 261.390, Recon Error: 0.052\n",
      "261.38984190742923\n",
      "Epoch: 4 Average loss: 260.80 Valid loss: 261.38984190742923\tRecon Error:0.052\n",
      "0/162079\tLoss: 254.568\n",
      "6400/162079\tLoss: 256.421\n",
      "12800/162079\tLoss: 257.130\n",
      "19200/162079\tLoss: 256.315\n",
      "25600/162079\tLoss: 256.699\n",
      "32000/162079\tLoss: 256.084\n",
      "38400/162079\tLoss: 255.266\n",
      "44800/162079\tLoss: 257.794\n",
      "51200/162079\tLoss: 256.178\n",
      "57600/162079\tLoss: 257.265\n",
      "64000/162079\tLoss: 254.782\n",
      "70400/162079\tLoss: 255.833\n",
      "76800/162079\tLoss: 254.915\n",
      "83200/162079\tLoss: 257.473\n",
      "89600/162079\tLoss: 253.886\n",
      "96000/162079\tLoss: 256.563\n",
      "102400/162079\tLoss: 255.908\n",
      "108800/162079\tLoss: 256.801\n",
      "115200/162079\tLoss: 256.869\n",
      "121600/162079\tLoss: 254.824\n",
      "128000/162079\tLoss: 254.195\n",
      "134400/162079\tLoss: 256.731\n",
      "140800/162079\tLoss: 256.286\n",
      "147200/162079\tLoss: 256.797\n",
      "153600/162079\tLoss: 252.834\n",
      "160000/162079\tLoss: 255.617\n",
      "Valid Loss: 255.439, Recon Error: 0.050\n",
      "255.4387517965065\n",
      "Epoch: 5 Average loss: 256.13 Valid loss: 255.4387517965065\tRecon Error:0.050\n",
      "0/162079\tLoss: 254.211\n",
      "6400/162079\tLoss: 253.647\n",
      "12800/162079\tLoss: 253.912\n",
      "19200/162079\tLoss: 256.714\n",
      "25600/162079\tLoss: 253.159\n",
      "32000/162079\tLoss: 252.142\n",
      "38400/162079\tLoss: 253.836\n",
      "44800/162079\tLoss: 253.061\n",
      "51200/162079\tLoss: 253.515\n",
      "57600/162079\tLoss: 256.640\n",
      "64000/162079\tLoss: 254.037\n",
      "70400/162079\tLoss: 254.331\n",
      "76800/162079\tLoss: 250.678\n",
      "83200/162079\tLoss: 249.811\n",
      "89600/162079\tLoss: 252.953\n",
      "96000/162079\tLoss: 252.545\n",
      "102400/162079\tLoss: 251.926\n",
      "108800/162079\tLoss: 255.507\n",
      "115200/162079\tLoss: 254.514\n",
      "121600/162079\tLoss: 250.493\n",
      "128000/162079\tLoss: 251.473\n",
      "134400/162079\tLoss: 250.847\n",
      "140800/162079\tLoss: 250.838\n",
      "147200/162079\tLoss: 251.569\n",
      "153600/162079\tLoss: 250.763\n",
      "160000/162079\tLoss: 253.607\n",
      "Valid Loss: 254.214, Recon Error: 0.049\n",
      "254.21393426859154\n",
      "Epoch: 6 Average loss: 253.04 Valid loss: 254.21393426859154\tRecon Error:0.049\n",
      "0/162079\tLoss: 267.995\n",
      "6400/162079\tLoss: 252.727\n",
      "12800/162079\tLoss: 248.682\n",
      "19200/162079\tLoss: 254.793\n",
      "25600/162079\tLoss: 251.180\n",
      "32000/162079\tLoss: 250.397\n",
      "38400/162079\tLoss: 248.224\n",
      "44800/162079\tLoss: 251.384\n",
      "51200/162079\tLoss: 251.159\n",
      "57600/162079\tLoss: 250.059\n",
      "64000/162079\tLoss: 252.114\n",
      "70400/162079\tLoss: 248.293\n",
      "76800/162079\tLoss: 250.182\n",
      "83200/162079\tLoss: 248.725\n",
      "89600/162079\tLoss: 251.547\n",
      "96000/162079\tLoss: 249.632\n",
      "102400/162079\tLoss: 248.736\n",
      "108800/162079\tLoss: 250.140\n",
      "115200/162079\tLoss: 250.185\n",
      "121600/162079\tLoss: 247.845\n",
      "128000/162079\tLoss: 249.130\n",
      "134400/162079\tLoss: 247.233\n",
      "140800/162079\tLoss: 248.074\n",
      "147200/162079\tLoss: 249.715\n",
      "153600/162079\tLoss: 248.892\n",
      "160000/162079\tLoss: 251.597\n",
      "Valid Loss: 253.705, Recon Error: 0.049\n",
      "253.70527015542083\n",
      "Epoch: 7 Average loss: 250.13 Valid loss: 253.70527015542083\tRecon Error:0.049\n",
      "0/162079\tLoss: 246.265\n",
      "6400/162079\tLoss: 249.296\n",
      "12800/162079\tLoss: 248.635\n",
      "19200/162079\tLoss: 246.806\n",
      "25600/162079\tLoss: 246.773\n",
      "32000/162079\tLoss: 248.831\n",
      "38400/162079\tLoss: 246.430\n",
      "44800/162079\tLoss: 248.496\n",
      "51200/162079\tLoss: 249.803\n",
      "57600/162079\tLoss: 247.505\n",
      "64000/162079\tLoss: 249.950\n",
      "70400/162079\tLoss: 248.223\n",
      "76800/162079\tLoss: 247.093\n",
      "83200/162079\tLoss: 246.166\n",
      "89600/162079\tLoss: 248.060\n",
      "96000/162079\tLoss: 250.669\n",
      "102400/162079\tLoss: 247.019\n",
      "108800/162079\tLoss: 246.573\n",
      "115200/162079\tLoss: 249.156\n",
      "121600/162079\tLoss: 249.008\n",
      "128000/162079\tLoss: 246.993\n",
      "134400/162079\tLoss: 245.613\n",
      "140800/162079\tLoss: 247.404\n",
      "147200/162079\tLoss: 249.234\n",
      "153600/162079\tLoss: 246.037\n",
      "160000/162079\tLoss: 247.579\n",
      "Valid Loss: 247.900, Recon Error: 0.042\n",
      "247.90016039962288\n",
      "Epoch: 8 Average loss: 248.06 Valid loss: 247.90016039962288\tRecon Error:0.042\n",
      "0/162079\tLoss: 243.952\n",
      "6400/162079\tLoss: 245.205\n",
      "12800/162079\tLoss: 245.265\n",
      "19200/162079\tLoss: 247.733\n",
      "25600/162079\tLoss: 244.777\n",
      "32000/162079\tLoss: 246.442\n",
      "38400/162079\tLoss: 244.135\n",
      "44800/162079\tLoss: 247.697\n",
      "51200/162079\tLoss: 245.226\n",
      "57600/162079\tLoss: 246.153\n",
      "64000/162079\tLoss: 248.743\n",
      "70400/162079\tLoss: 245.704\n",
      "76800/162079\tLoss: 246.066\n",
      "83200/162079\tLoss: 245.172\n",
      "89600/162079\tLoss: 247.678\n",
      "96000/162079\tLoss: 244.237\n",
      "102400/162079\tLoss: 245.215\n",
      "108800/162079\tLoss: 246.961\n",
      "115200/162079\tLoss: 243.828\n",
      "121600/162079\tLoss: 244.893\n",
      "128000/162079\tLoss: 242.675\n",
      "134400/162079\tLoss: 247.305\n",
      "140800/162079\tLoss: 243.517\n",
      "147200/162079\tLoss: 247.325\n",
      "153600/162079\tLoss: 245.574\n",
      "160000/162079\tLoss: 247.567\n",
      "Valid Loss: 248.705, Recon Error: 0.047\n",
      "248.70450069019628\n",
      "Epoch: 9 Average loss: 245.90 Valid loss: 248.70450069019628\tRecon Error:0.047\n",
      "0/162079\tLoss: 238.977\n",
      "6400/162079\tLoss: 245.094\n",
      "12800/162079\tLoss: 244.515\n",
      "19200/162079\tLoss: 244.287\n",
      "25600/162079\tLoss: 244.776\n",
      "32000/162079\tLoss: 244.939\n",
      "38400/162079\tLoss: 245.589\n",
      "44800/162079\tLoss: 245.306\n",
      "51200/162079\tLoss: 244.598\n",
      "57600/162079\tLoss: 245.578\n",
      "64000/162079\tLoss: 243.638\n",
      "70400/162079\tLoss: 243.773\n",
      "76800/162079\tLoss: 243.945\n",
      "83200/162079\tLoss: 244.968\n",
      "89600/162079\tLoss: 244.754\n",
      "96000/162079\tLoss: 245.216\n",
      "102400/162079\tLoss: 242.915\n",
      "108800/162079\tLoss: 241.600\n",
      "115200/162079\tLoss: 243.338\n",
      "121600/162079\tLoss: 243.111\n",
      "128000/162079\tLoss: 243.142\n",
      "134400/162079\tLoss: 243.621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140800/162079\tLoss: 243.006\n",
      "147200/162079\tLoss: 243.307\n",
      "153600/162079\tLoss: 243.784\n",
      "160000/162079\tLoss: 246.484\n",
      "Valid Loss: 245.039, Recon Error: 0.046\n",
      "245.0387288219524\n",
      "Epoch: 10 Average loss: 244.35 Valid loss: 245.0387288219524\tRecon Error:0.046\n"
     ]
    }
   ],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "###1e-5 6859 1e-4 6727 5e-4 6722 try tanh/L1 loss/beta--->DIP\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "trainer.train(train_loader,valid_loader, epochs=100, save_training_gif=('./training.gif', viz))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "torch.save(model.state_dict(), 'modelDIPf_params.pkl')\n",
    "torch.save(model, './modelDIPf')\n",
    "##15.078 - 0.0147  17.209 - 0.0168 error tanh \n",
    "##LR 1e-3 0.019-0.023 worse should pick 5e-4\n",
    "##PLOT THE CURVE!!!!!\n",
    "#16-2033.294813156128 32-2846.2241864204407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_spec=latent_spec, img_size=(3, 64, 64)).cuda()\n",
    "model.load_state_dict(torch.load('modelDIPf_params.pkl'))\n",
    "#path=\"figures/face/cont_{}/pruned_Beta_ {}lamba{}_ONLYPAIR\".format(n_cont,gamma,0.1)\n",
    "loss = trainer.get_losses()\n",
    "print(len(loss[\"DIP_loss\"]))\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.cuda.is_available()\n",
    "# device = torch.device('cuda')\n",
    "# print(device)\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Chi-square test\n",
    "import torch\n",
    "tensor_one = torch.tensor([[1,2,3],[4,5,6]])\n",
    "tensor_two = torch.tensor([[6,8,9],[10,11,12]])\n",
    "tensor_list = [tensor_one, tensor_two]\n",
    "tens_list = []\n",
    "for tensor in tensor_list:\n",
    "    \n",
    "    print(tensor)\n",
    "    length = tensor.shape[1]\n",
    "    tens_list.append(torch.mean(tensor.float(),dim=0))\n",
    "    \n",
    "tens_list = torch.stack(tens_list).reshape(1,-1)\n",
    "tens_listT = tens_list.t()\n",
    "matrix = tens_listT.matmul(tens_list)\n",
    "print(matrix)\n",
    "print(\"--------\")\n",
    "Chi2 =0\n",
    "for i in range(len(tensor_list)):\n",
    "    for j in range(len(tensor_list)):\n",
    "        if i > j:\n",
    "            submatrix = matrix[j*length:(j+1)*length,i*length:(i+1)*length]\n",
    "            c_sum = torch.sum(submatrix,dim=0).reshape(-1,1)\n",
    "            \n",
    "            r_sum = torch.sum(submatrix,dim=1).reshape(1,-1)\n",
    "            all_sum = torch.sum(submatrix)\n",
    "            Expectation = c_sum.matmul(r_sum)/all_sum\n",
    "            print(all_sum,c_sum,r_sum,Expectation)\n",
    "            Chi2 += torch.sum((submatrix-Expectation)**2/Expectation)\n",
    "            \n",
    "        \n",
    "print(Chi2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "# Get a batch of data\n",
    "for batch, labels in test_loader:\n",
    "    break\n",
    "    \n",
    "#get best model,easrly stopping\n",
    "\n",
    "viz = Visualizer(model)\n",
    "\n",
    "# Reconstruct data using Joint-VAE model\n",
    "recon = viz.reconstructions(batch)\n",
    "\n",
    "# face\n",
    "recon=np.rollaxis(recon.numpy(), 0, 3)  \n",
    "print(recon[265:,:,:].max())\n",
    "recon[:,:,:]=(recon[:,:,:]+1)/2\n",
    "plt.imshow(recon[:,:,:].astype(float))\n",
    "\n",
    "#MNIST\n",
    "# plt.imshow(recon.numpy()[0, :, :].astype(float), cmap='gray')\n",
    "# print(recon.numpy()[0, :, :].max())\n",
    "plt.savefig(path+\"/recon.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCR():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont']\n",
    "        cov = covmatrix(mean)\n",
    "        cov[torch.abs(cov)<=1e-6]=0\n",
    "        cor = cov2cor(cov)\n",
    "        totalc += np.sum(cor) \n",
    "\n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "def TCV():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont']\n",
    "        cov = covmatrix(mean).cpu().detach().numpy()\n",
    "        cov = cov-np.diag(np.diag(cov))\n",
    "        #print(np.sum(cov**2) )\n",
    "        totalc += np.sum(cov**2) \n",
    "        \n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "\n",
    "def covmatrix(mean):\n",
    "    exp_mu = torch.mean(mean, dim=0)  #####mean through batch\n",
    "\n",
    "    # expectation of mu mu.tranpose\n",
    "    mu_expand1 = mean.unsqueeze(1)  #####(batch_size, 1, number of mean of latent variables)\n",
    "    mu_expand2 = mean.unsqueeze(2)  #####(batch_size, number of mean of latent variables, 1) ignore batch_size, only transpose the means\n",
    "    exp_mu_mu_t = torch.mean(mu_expand1 * mu_expand2, dim=0)\n",
    "\n",
    "    # covariance of model mean\n",
    "    cov = exp_mu_mu_t - exp_mu.unsqueeze(0) * exp_mu.unsqueeze(1) \n",
    "    return cov\n",
    "def cov2cor(c):\n",
    "    #input batch * n_cont\n",
    "    c = c.cpu().detach()\n",
    "    d=np.zeros_like(c)\n",
    "    for i in range(c.shape[0]):\n",
    "        for j in range(c.shape[1]):\n",
    "            d[i,j]=c[i,j]/(np.sqrt(c[i,i]*c[j,j]+1e-10))\n",
    "    return d\n",
    "tcor=TCR()\n",
    "tcov=TCV()\n",
    "print(tcor,tcov)\n",
    "trainer.evaluate(test_loader)\n",
    "#16  12.551628477254491 2.2266025315596838e-05 Valid Loss: 220.690, Recon Error: 0.185\n",
    "#32  32.79880483590873 0.26285673431150475 Valid Loss: 68.768, Recon Error: 0.0145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###latent space T-SNE visualization\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "samples = torch.zeros(1)\n",
    "labels = torch.zeros(1)\n",
    "for i in range(10):\n",
    "    test_batch = iter(test_loader)\n",
    "    test_batch = next(test_batch)\n",
    "    new_labels =torch.tensor(test_batch[1])\n",
    "    latent_dist= model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "    new_samples = model.reparameterize(latent_dist)\n",
    "    if torch.sum(samples) == 0:\n",
    "        samples =new_samples\n",
    "        labels = new_labels\n",
    "    else:\n",
    "        samples = torch.cat((samples,new_samples),0)\n",
    "        labels = torch.cat((labels, new_labels),0)\n",
    "    #print(samples.shape)\n",
    "    \n",
    "##latent_varibales should be N,D--->N,2\n",
    "\n",
    "\n",
    "# latent_variables = samples.reshape(samples[0],-1)\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "tsne.fit_transform(samples.detach().cpu().numpy())\n",
    "\n",
    "plt.scatter(tsne.embedding_[:,0],tsne.embedding_[:,1])\n",
    "#plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 10 # Number of labels\n",
    "\n",
    "# setup the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "# define the data\n",
    "x = tsne.embedding_[:,0]\n",
    "y = tsne.embedding_[:,1]\n",
    "tag = labels# Tag each point with a corresponding label    \n",
    "\n",
    "# define the colormap\n",
    "cmap = plt.cm.jet\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# create the new map\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0,N,N+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# make the scatter\n",
    "scat = ax.scatter(x,y,c=tag,s=np.random.randint(100,110,N),cmap=cmap,     norm=norm)\n",
    "# create the colorbar\n",
    "cb = plt.colorbar(scat, spacing='proportional',ticks=bounds)\n",
    "cb.set_label('Custom cbar')\n",
    "ax.set_title('Discrete color mappings')\n",
    "\n",
    "plt.savefig(path+\"/scatter.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t-SNE demo\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "X = np.arange(40).reshape(5,4,2)\n",
    "\n",
    "X_new = X.reshape(5,-1)\n",
    "#X = np.array([[[0,0], [0,0], [0,0]], [[0,0], [0,1], [1,1]], [[1,1], [1,0], [0,1]], [[1,1], [1,1], [1,1]]])\n",
    "print(X.shape,X)\n",
    "print(\"--------\")\n",
    "print(X_new)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne.fit_transform(X)\n",
    "print(tsne.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot samples\n",
    "\n",
    "samples = viz.samples()\n",
    "plt.imshow(samples.numpy()[0, :174, :], cmap='gray')\n",
    "print(np.sum(samples.numpy()[0, :174, :]))\n",
    "print(samples.numpy()[0, :, :].shape)\n",
    "####origin\n",
    "4*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot samples\n",
    "import matplotlib as mpl\n",
    "\n",
    "#MNIST\n",
    "# samples = viz.samples()\n",
    "# sample=samples.numpy()[0, :, :]/2+0.5\n",
    "# plt.imshow(sample, cmap='gray')\n",
    "# plt.imsave(path+\"/samples\",samples.numpy()[0, :, :]/2+0.5, cmap='gray')\n",
    "\n",
    "\n",
    "# face\n",
    "fig = plt.figure(figsize=(30, 30)) \n",
    "samples = viz.samples()\n",
    "samples = np.rollaxis(samples.numpy(), 0, 3)  \n",
    "print(samples[:,:,0].max())\n",
    "samples=(samples+1)/2\n",
    "plt.imshow(samples.astype(float),norm = norm)\n",
    "plt.imsave(path+\"/samples\",samples)\n",
    "###DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot all traversals\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "traversals = viz.all_latent_traversals(size=10)\n",
    "\n",
    "#MNIST\n",
    "# plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "# plt.imsave(path+\"/all_traversals\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "#face\n",
    "traversals = np.rollaxis(traversals.numpy(), 0, 3)  \n",
    "traversals=(traversals+1)/2\n",
    "plt.imshow(traversals)\n",
    "plt.imsave(path+\"/all_traversals\",traversals)\n",
    "###dip[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
    "#         0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=5, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "#MNIST\n",
    "# plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "# plt.imsave(path+\"/contVSdisc\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "# traversals.numpy()[0, :, :].max()\n",
    "#face\n",
    "traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "traversals=(traversals+1)/2\n",
    "plt.imshow(traversals)\n",
    "plt.imsave(path+\"/contVSdisc\",traversals)\n",
    "##origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_t = viz.all_latent_traversals()\n",
    "print(all_t.shape)\n",
    "plt.imshow(all_t.numpy()[0, :, :], cmap='gray')\n",
    "plt.imsave(\"figures/beta/all_\",traversals.numpy()[0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "# Plot a grid of some traversals\n",
    "\n",
    "fig = plt.figure(figsize=(70, 70))  # width, height in inches\n",
    "print(\"continuous\")\n",
    "for i in range(n_cont):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=i, disc_idx=None,size=12)\n",
    "    \n",
    "    #MNIST\n",
    "#     sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "#     plt.savefig(path+\"/cont{}.png\".format(i))\n",
    "#     plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "    \n",
    "    #FACE\n",
    "    traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "    sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "    traversals=(traversals+1)/2\n",
    "    plt.imshow(traversals)   \n",
    "plt.savefig(path+\"/cont.png\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"discrete\")\n",
    "for i in range(n_disc):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=None, disc_idx=i,size=10)\n",
    "    ##MNIST\n",
    "#     sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "#     plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "#     plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "    #FACE\n",
    "    traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "    sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "traversals=(traversals+1)/2\n",
    "plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "plt.imshow(traversals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "    \n",
    "# face    \n",
    "def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "        # Generate latent traversal\n",
    "#         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "#                                                              disc_idx=disc_idx,\n",
    "#                                                              size=size)\n",
    "        dim = n_cont + sum(disc)\n",
    "        if prior:\n",
    "            latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "        else:\n",
    "            latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "        latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "        latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "        # Map samples through decoder\n",
    "        generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "        generated  = np.rollaxis(generated.detach().numpy(), 0, 3)\n",
    "        generated = (generated +1)/2\n",
    "        print(generated.min(),generated.max())\n",
    "        plt.imshow(generated)\n",
    "\n",
    "        \n",
    "def decode_latents(model, latent_samples):\n",
    "\n",
    "        latent_samples = Variable(latent_samples)\n",
    "        if model.use_cuda:\n",
    "            latent_samples = latent_samples.cuda()\n",
    "            result = model.decode(latent_samples).cpu()\n",
    "        return result\n",
    "\n",
    "#MNIST\n",
    "# def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "#         # Generate latent traversal\n",
    "# #         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "# #                                                              disc_idx=disc_idx,\n",
    "# #                                                              size=size)\n",
    "#         dim = n_cont + sum(disc)\n",
    "#         if prior:\n",
    "#             latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "#         else:\n",
    "#             latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "#         latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "#         latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "#         # Map samples through decoder\n",
    "#         generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "#         plt.imshow(generated.detach().numpy(),cmap=\"gray\")\n",
    "\n",
    "        \n",
    "# def decode_latents(model, latent_samples):\n",
    "\n",
    "#         latent_samples = Variable(latent_samples)\n",
    "#         if model.use_cuda:\n",
    "#             latent_samples = latent_samples.cuda()\n",
    "#         return model.decode(latent_samples).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "def interactive_view(model,n_cont,disc):\n",
    "    \n",
    "    \n",
    "    interact(single_traversal,model=fixed(model),\n",
    "             n_cont=fixed(n_cont), cont_idx=(0,n_cont,1), cont_v=(-2.5,2.5,0.5),\n",
    "             disc=fixed(disc),disc_idx=(0,9,1),\n",
    "             prior=True);\n",
    "             \n",
    "interactive_view(model,n_cont,disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
