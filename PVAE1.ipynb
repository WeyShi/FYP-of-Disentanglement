{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "Build a simple JointVAE model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162079 20259 20261\n",
      "torch.Size([128, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from disentanglement_lib.data.ground_truth import dsprites\n",
    "import os\n",
    "import torch\n",
    "from tensorflow import gfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "dsprites.DSprites\n",
    "IMAGE_PATH = 'img_align_celeba/'\n",
    "image_size = 64\n",
    "# SAMPLE_PATH = '../'\n",
    "DSPRITES_PATH = \"/home/ISO/Pruned_VAE/data/dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"\n",
    "with gfile.Open(DSPRITES_PATH, \"rb\") as data_file:\n",
    "    # Data was saved originally using python2, so we need to set the encoding.\n",
    "    data = np.load(data_file, encoding=\"latin1\", allow_pickle=True)\n",
    "# if not os.path.exists(SAMPLE_PATH):\n",
    "#     os.makedirs(SAMPLE_PATH)\n",
    "    \n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    #transforms.Scale(image_size),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop((image_size,image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "data_loader = ImageFolder(IMAGE_PATH, transform)\n",
    "\n",
    "\n",
    "#data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "valid_loader, train_loader, test_loader = get_celeba_dataloader(data_loader, \n",
    "                                                                batch_size=128)\n",
    "test_batch = iter(test_loader)\n",
    "test_batch = next(test_batch)\n",
    "new_labels =torch.tensor(test_batch[1])\n",
    "print(torch.tensor(test_batch[0]).shape)\n",
    "#latent_dist = model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "limit_a, limit_b, epsilon = -.5, 1.5, 1e-6\n",
    "eps = np.linspace(0.1,0.9,20)\n",
    "def sigmoid(x):\n",
    "    y = 1./(1.+np.exp(-x))\n",
    "    return y\n",
    "def quantile_concrete(x,temperature,qz_loga):\n",
    "        \n",
    "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
    "        y = sigmoid((np.log(x) - np.log(1 - x) + qz_loga) / temperature)\n",
    "        return y * (limit_b - limit_a) + limit_a\n",
    "\n",
    "z = quantile_concrete(eps,1/20,2)\n",
    "z[z>=1]=1\n",
    "z[z<=0]=0\n",
    "print(z)\n",
    "plt.plot(eps,z)\n",
    "droprate_init = 0.2\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.zeros(4), requires_grad=False).data.normal_(math.log(1 - droprate_init) - math.log(droprate_init), 1e-2)\n",
    "z=(x+y).detach()\n",
    "print(y)\n",
    "#z.is_leaf \n",
    "a=torch.ones((64,32))\n",
    "b=torch.ones((1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737280, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(data[\"imgs\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "\n",
    "#####train\n",
    "train_x = torch.tensor(data[\"imgs\"][:649600,:,:]) # a list of numpy arrays\n",
    "train_y = torch.zeros((data[\"imgs\"][:649600,:,:].shape[0],1)) # another list of numpy arrays (targets)\n",
    "train_set = utils.TensorDataset(train_x,train_y) # create your datset\n",
    "train_loader = utils.DataLoader(train_set, batch_size=64) # create your dataloader\n",
    "\n",
    "#####test\n",
    "test_x = torch.tensor(data[\"imgs\"][649600:704000,:,:]) # a list of numpy arrays\n",
    "test_y = torch.zeros((data[\"imgs\"][649600:704000,:,:].shape[0],1)) # another list of numpy arrays (targets)\n",
    "test_set = utils.TensorDataset(test_x,test_y) # create your datset\n",
    "test_loader = utils.DataLoader(test_set, batch_size=64) # create your dataloader\n",
    "\n",
    "#####valid\n",
    "valid_x = torch.tensor(data[\"imgs\"][700000:,:,:]) # a list of numpy arrays\n",
    "valid_y = torch.zeros((data[\"imgs\"][700000:,:,:].shape[0],1)) # another list of numpy arrays (targets)\n",
    "valid_set = utils.TensorDataset(valid_x,valid_y) # create your datset\n",
    "valid_loader = utils.DataLoader(valid_set, batch_size=64) # create your dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (img_to_features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (features_to_hidden): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_latent): Sequential(\n",
      "    (0): L0Pair(256 -> 2*10, droprate_init=0.2, lamba=0.1, temperature=0.05, weight_decay=0.001, local_rep=False)\n",
      "  )\n",
      "  (fc_alphas): ModuleList()\n",
      "  (latent_to_features): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (features_to_img): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloaders import get_mnist_dataloaders, get_celeba_dataloader \n",
    "from torchvision import transforms \n",
    "from torchvision.datasets import ImageFolder \n",
    "from torch.utils.data import DataLoader \n",
    "import os \n",
    "import torch\n",
    "from jointvae.models_f import VAE\n",
    "from jointvae.training import Trainer\n",
    "from torch import optim\n",
    "from viz.visualize_c import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#valid_loader, train_loader, test_loader = get_mnist_dataloaders(batch_size=64)\n",
    "\n",
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions  7-14\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "n_cont = 10\n",
    "disc = []\n",
    "n_disc = len(disc)\n",
    "latent_spec = {'cont': n_cont,\n",
    "               'disc': disc}\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = VAE(latent_spec=latent_spec, img_size=(1, 64, 64)).cuda()\n",
    "# model = VAE(latent_spec=latent_spec, img_size=(1, 32, 32)).cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "lr=0.00005\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "gamma=1.0\n",
    "cont_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 0.0, 25000, gamma]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "\n",
    "lambda_d = 2\n",
    "lambda_od = 10*lambda_d\n",
    "lambda_dis = 20*lambda_d \n",
    "path=\"./Evaluations/\".format(n_cont,gamma,lambda_d)\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity,lambda_d = lambda_d,\n",
    "                  lambda_od = lambda_od, lambda_dis = lambda_dis )\n",
    "# Build a visualizer which will be passed to trainer to visualize progress during training\n",
    "viz = Visualizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from jointvae.training import Trainer\n",
    "\n",
    "\n",
    "trainer._train_epoch(train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/649600\tLoss: 1646.198\tL0 Loss: 0.530\n",
      "3200/649600\tLoss: 1637.827\tL0 Loss: 0.530\n",
      "6400/649600\tLoss: 1614.321\tL0 Loss: 0.530\n",
      "9600/649600\tLoss: 1319.093\tL0 Loss: 0.530\n",
      "12800/649600\tLoss: 1115.676\tL0 Loss: 0.530\n",
      "16000/649600\tLoss: 1092.714\tL0 Loss: 0.530\n",
      "19200/649600\tLoss: 1082.595\tL0 Loss: 0.530\n",
      "22400/649600\tLoss: 1075.984\tL0 Loss: 0.530\n",
      "25600/649600\tLoss: 1073.178\tL0 Loss: 0.530\n",
      "28800/649600\tLoss: 1070.502\tL0 Loss: 0.530\n",
      "32000/649600\tLoss: 1069.526\tL0 Loss: 0.530\n",
      "35200/649600\tLoss: 1068.093\tL0 Loss: 0.530\n",
      "38400/649600\tLoss: 1066.423\tL0 Loss: 0.530\n",
      "41600/649600\tLoss: 1065.856\tL0 Loss: 0.530\n",
      "44800/649600\tLoss: 1065.699\tL0 Loss: 0.530\n",
      "48000/649600\tLoss: 1064.404\tL0 Loss: 0.530\n",
      "51200/649600\tLoss: 1064.516\tL0 Loss: 0.530\n",
      "54400/649600\tLoss: 1063.879\tL0 Loss: 0.530\n",
      "57600/649600\tLoss: 1063.883\tL0 Loss: 0.530\n",
      "60800/649600\tLoss: 1063.450\tL0 Loss: 0.530\n",
      "64000/649600\tLoss: 1062.933\tL0 Loss: 0.530\n",
      "67200/649600\tLoss: 1063.245\tL0 Loss: 0.530\n",
      "70400/649600\tLoss: 1063.633\tL0 Loss: 0.530\n",
      "73600/649600\tLoss: 1063.912\tL0 Loss: 0.530\n",
      "76800/649600\tLoss: 1062.833\tL0 Loss: 0.530\n",
      "80000/649600\tLoss: 1062.612\tL0 Loss: 0.530\n",
      "83200/649600\tLoss: 1063.031\tL0 Loss: 0.530\n",
      "86400/649600\tLoss: 1063.200\tL0 Loss: 0.530\n",
      "89600/649600\tLoss: 1063.045\tL0 Loss: 0.530\n",
      "92800/649600\tLoss: 1061.790\tL0 Loss: 0.530\n",
      "96000/649600\tLoss: 1062.100\tL0 Loss: 0.530\n",
      "99200/649600\tLoss: 1063.182\tL0 Loss: 0.530\n",
      "102400/649600\tLoss: 1062.030\tL0 Loss: 0.530\n",
      "105600/649600\tLoss: 1062.221\tL0 Loss: 0.530\n",
      "108800/649600\tLoss: 1063.189\tL0 Loss: 0.530\n",
      "112000/649600\tLoss: 1062.672\tL0 Loss: 0.530\n",
      "115200/649600\tLoss: 1061.887\tL0 Loss: 0.530\n",
      "118400/649600\tLoss: 1062.835\tL0 Loss: 0.530\n",
      "121600/649600\tLoss: 1062.841\tL0 Loss: 0.530\n",
      "124800/649600\tLoss: 1061.862\tL0 Loss: 0.530\n",
      "128000/649600\tLoss: 1060.803\tL0 Loss: 0.530\n",
      "131200/649600\tLoss: 1061.978\tL0 Loss: 0.529\n",
      "134400/649600\tLoss: 1060.844\tL0 Loss: 0.529\n",
      "137600/649600\tLoss: 1059.731\tL0 Loss: 0.529\n",
      "140800/649600\tLoss: 1061.135\tL0 Loss: 0.530\n",
      "144000/649600\tLoss: 1060.317\tL0 Loss: 0.530\n",
      "147200/649600\tLoss: 1061.218\tL0 Loss: 0.530\n",
      "150400/649600\tLoss: 1061.051\tL0 Loss: 0.530\n",
      "153600/649600\tLoss: 1059.059\tL0 Loss: 0.529\n",
      "156800/649600\tLoss: 1060.030\tL0 Loss: 0.529\n",
      "160000/649600\tLoss: 1059.536\tL0 Loss: 0.529\n",
      "163200/649600\tLoss: 1053.781\tL0 Loss: 0.529\n",
      "166400/649600\tLoss: 1045.756\tL0 Loss: 0.529\n",
      "169600/649600\tLoss: 1043.128\tL0 Loss: 0.530\n",
      "172800/649600\tLoss: 1042.625\tL0 Loss: 0.530\n",
      "176000/649600\tLoss: 1042.889\tL0 Loss: 0.530\n",
      "179200/649600\tLoss: 1042.984\tL0 Loss: 0.530\n",
      "182400/649600\tLoss: 1042.516\tL0 Loss: 0.530\n",
      "185600/649600\tLoss: 1042.387\tL0 Loss: 0.530\n",
      "188800/649600\tLoss: 1042.459\tL0 Loss: 0.530\n",
      "192000/649600\tLoss: 1042.247\tL0 Loss: 0.530\n",
      "195200/649600\tLoss: 1041.697\tL0 Loss: 0.530\n",
      "198400/649600\tLoss: 1042.145\tL0 Loss: 0.530\n",
      "201600/649600\tLoss: 1043.030\tL0 Loss: 0.530\n",
      "204800/649600\tLoss: 1042.526\tL0 Loss: 0.530\n",
      "208000/649600\tLoss: 1041.913\tL0 Loss: 0.530\n",
      "211200/649600\tLoss: 1041.928\tL0 Loss: 0.530\n",
      "214400/649600\tLoss: 1041.566\tL0 Loss: 0.530\n",
      "217600/649600\tLoss: 1041.583\tL0 Loss: 0.530\n",
      "220800/649600\tLoss: 1041.977\tL0 Loss: 0.530\n",
      "224000/649600\tLoss: 1042.167\tL0 Loss: 0.530\n",
      "227200/649600\tLoss: 1042.126\tL0 Loss: 0.530\n",
      "230400/649600\tLoss: 1041.857\tL0 Loss: 0.530\n",
      "233600/649600\tLoss: 1041.549\tL0 Loss: 0.530\n",
      "236800/649600\tLoss: 1041.811\tL0 Loss: 0.530\n",
      "240000/649600\tLoss: 1042.024\tL0 Loss: 0.530\n",
      "243200/649600\tLoss: 1041.411\tL0 Loss: 0.530\n",
      "246400/649600\tLoss: 1042.535\tL0 Loss: 0.530\n",
      "249600/649600\tLoss: 1044.582\tL0 Loss: 0.530\n",
      "252800/649600\tLoss: 1042.460\tL0 Loss: 0.530\n",
      "256000/649600\tLoss: 1041.544\tL0 Loss: 0.530\n",
      "259200/649600\tLoss: 1041.113\tL0 Loss: 0.530\n",
      "262400/649600\tLoss: 1041.508\tL0 Loss: 0.530\n",
      "265600/649600\tLoss: 1041.256\tL0 Loss: 0.530\n",
      "268800/649600\tLoss: 1041.400\tL0 Loss: 0.530\n",
      "272000/649600\tLoss: 1041.300\tL0 Loss: 0.530\n",
      "275200/649600\tLoss: 1041.324\tL0 Loss: 0.530\n",
      "278400/649600\tLoss: 1041.414\tL0 Loss: 0.530\n",
      "281600/649600\tLoss: 1041.801\tL0 Loss: 0.530\n",
      "284800/649600\tLoss: 1040.884\tL0 Loss: 0.530\n",
      "288000/649600\tLoss: 1041.298\tL0 Loss: 0.530\n",
      "291200/649600\tLoss: 1041.390\tL0 Loss: 0.530\n",
      "294400/649600\tLoss: 1040.894\tL0 Loss: 0.530\n",
      "297600/649600\tLoss: 1040.734\tL0 Loss: 0.530\n",
      "300800/649600\tLoss: 1041.107\tL0 Loss: 0.530\n",
      "304000/649600\tLoss: 1040.619\tL0 Loss: 0.530\n",
      "307200/649600\tLoss: 1040.661\tL0 Loss: 0.530\n",
      "310400/649600\tLoss: 1040.379\tL0 Loss: 0.530\n",
      "313600/649600\tLoss: 1040.104\tL0 Loss: 0.530\n",
      "316800/649600\tLoss: 1040.655\tL0 Loss: 0.530\n",
      "320000/649600\tLoss: 1040.122\tL0 Loss: 0.530\n",
      "323200/649600\tLoss: 1040.161\tL0 Loss: 0.531\n",
      "326400/649600\tLoss: 1040.088\tL0 Loss: 0.531\n",
      "329600/649600\tLoss: 1039.936\tL0 Loss: 0.531\n",
      "332800/649600\tLoss: 1040.353\tL0 Loss: 0.531\n",
      "336000/649600\tLoss: 1040.290\tL0 Loss: 0.531\n",
      "339200/649600\tLoss: 1041.092\tL0 Loss: 0.531\n",
      "342400/649600\tLoss: 1040.129\tL0 Loss: 0.531\n",
      "345600/649600\tLoss: 1039.801\tL0 Loss: 0.531\n",
      "348800/649600\tLoss: 1040.041\tL0 Loss: 0.531\n",
      "352000/649600\tLoss: 1039.616\tL0 Loss: 0.531\n",
      "355200/649600\tLoss: 1040.619\tL0 Loss: 0.531\n",
      "358400/649600\tLoss: 1039.756\tL0 Loss: 0.531\n",
      "361600/649600\tLoss: 1040.273\tL0 Loss: 0.531\n",
      "364800/649600\tLoss: 1040.668\tL0 Loss: 0.531\n",
      "368000/649600\tLoss: 1039.900\tL0 Loss: 0.531\n",
      "371200/649600\tLoss: 1040.387\tL0 Loss: 0.531\n",
      "374400/649600\tLoss: 1039.543\tL0 Loss: 0.531\n",
      "377600/649600\tLoss: 1040.316\tL0 Loss: 0.531\n",
      "380800/649600\tLoss: 1040.467\tL0 Loss: 0.531\n",
      "384000/649600\tLoss: 1040.239\tL0 Loss: 0.531\n",
      "387200/649600\tLoss: 1039.841\tL0 Loss: 0.531\n",
      "390400/649600\tLoss: 1040.083\tL0 Loss: 0.531\n",
      "393600/649600\tLoss: 1039.816\tL0 Loss: 0.531\n",
      "396800/649600\tLoss: 1040.114\tL0 Loss: 0.531\n",
      "400000/649600\tLoss: 1039.683\tL0 Loss: 0.531\n",
      "403200/649600\tLoss: 1040.369\tL0 Loss: 0.531\n",
      "406400/649600\tLoss: 1039.842\tL0 Loss: 0.531\n",
      "409600/649600\tLoss: 1039.742\tL0 Loss: 0.531\n",
      "412800/649600\tLoss: 1039.661\tL0 Loss: 0.531\n",
      "416000/649600\tLoss: 1040.303\tL0 Loss: 0.531\n",
      "419200/649600\tLoss: 1040.242\tL0 Loss: 0.531\n",
      "422400/649600\tLoss: 1040.609\tL0 Loss: 0.531\n",
      "425600/649600\tLoss: 1040.184\tL0 Loss: 0.531\n",
      "428800/649600\tLoss: 1039.695\tL0 Loss: 0.531\n",
      "432000/649600\tLoss: 1039.554\tL0 Loss: 0.531\n",
      "435200/649600\tLoss: 1039.547\tL0 Loss: 0.531\n",
      "438400/649600\tLoss: 1039.982\tL0 Loss: 0.531\n",
      "441600/649600\tLoss: 1040.042\tL0 Loss: 0.531\n",
      "444800/649600\tLoss: 1040.028\tL0 Loss: 0.531\n",
      "448000/649600\tLoss: 1039.477\tL0 Loss: 0.531\n",
      "451200/649600\tLoss: 1039.366\tL0 Loss: 0.531\n",
      "454400/649600\tLoss: 1039.842\tL0 Loss: 0.532\n",
      "457600/649600\tLoss: 1040.045\tL0 Loss: 0.532\n",
      "460800/649600\tLoss: 1040.330\tL0 Loss: 0.532\n",
      "464000/649600\tLoss: 1040.662\tL0 Loss: 0.532\n",
      "467200/649600\tLoss: 1040.476\tL0 Loss: 0.532\n",
      "470400/649600\tLoss: 1039.806\tL0 Loss: 0.532\n",
      "473600/649600\tLoss: 1039.542\tL0 Loss: 0.532\n",
      "476800/649600\tLoss: 1039.445\tL0 Loss: 0.532\n",
      "480000/649600\tLoss: 1039.879\tL0 Loss: 0.532\n",
      "483200/649600\tLoss: 1040.331\tL0 Loss: 0.532\n",
      "486400/649600\tLoss: 1040.079\tL0 Loss: 0.532\n",
      "489600/649600\tLoss: 1039.645\tL0 Loss: 0.532\n",
      "492800/649600\tLoss: 1042.098\tL0 Loss: 0.532\n",
      "496000/649600\tLoss: 1044.588\tL0 Loss: 0.532\n",
      "499200/649600\tLoss: 1041.629\tL0 Loss: 0.532\n",
      "502400/649600\tLoss: 1039.370\tL0 Loss: 0.532\n",
      "505600/649600\tLoss: 1038.976\tL0 Loss: 0.532\n",
      "508800/649600\tLoss: 1039.156\tL0 Loss: 0.532\n",
      "512000/649600\tLoss: 1039.181\tL0 Loss: 0.532\n",
      "515200/649600\tLoss: 1038.759\tL0 Loss: 0.532\n",
      "518400/649600\tLoss: 1038.373\tL0 Loss: 0.532\n",
      "521600/649600\tLoss: 1038.729\tL0 Loss: 0.532\n",
      "524800/649600\tLoss: 1038.363\tL0 Loss: 0.532\n",
      "528000/649600\tLoss: 1038.073\tL0 Loss: 0.532\n",
      "531200/649600\tLoss: 1038.436\tL0 Loss: 0.532\n",
      "534400/649600\tLoss: 1038.802\tL0 Loss: 0.532\n",
      "537600/649600\tLoss: 1037.400\tL0 Loss: 0.532\n",
      "540800/649600\tLoss: 1037.989\tL0 Loss: 0.532\n",
      "544000/649600\tLoss: 1037.962\tL0 Loss: 0.532\n",
      "547200/649600\tLoss: 1037.401\tL0 Loss: 0.532\n",
      "550400/649600\tLoss: 1037.570\tL0 Loss: 0.532\n",
      "553600/649600\tLoss: 1036.944\tL0 Loss: 0.532\n",
      "556800/649600\tLoss: 1037.022\tL0 Loss: 0.532\n",
      "560000/649600\tLoss: 1037.604\tL0 Loss: 0.532\n",
      "563200/649600\tLoss: 1037.001\tL0 Loss: 0.532\n",
      "566400/649600\tLoss: 1037.188\tL0 Loss: 0.532\n",
      "569600/649600\tLoss: 1036.645\tL0 Loss: 0.533\n",
      "572800/649600\tLoss: 1036.081\tL0 Loss: 0.533\n",
      "576000/649600\tLoss: 1037.939\tL0 Loss: 0.533\n",
      "579200/649600\tLoss: 1036.120\tL0 Loss: 0.533\n",
      "582400/649600\tLoss: 1036.411\tL0 Loss: 0.533\n",
      "585600/649600\tLoss: 1036.206\tL0 Loss: 0.533\n",
      "588800/649600\tLoss: 1036.267\tL0 Loss: 0.533\n",
      "592000/649600\tLoss: 1035.750\tL0 Loss: 0.533\n",
      "595200/649600\tLoss: 1035.418\tL0 Loss: 0.533\n",
      "598400/649600\tLoss: 1035.759\tL0 Loss: 0.533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601600/649600\tLoss: 1035.211\tL0 Loss: 0.533\n",
      "604800/649600\tLoss: 1035.557\tL0 Loss: 0.533\n",
      "608000/649600\tLoss: 1035.953\tL0 Loss: 0.533\n",
      "611200/649600\tLoss: 1035.621\tL0 Loss: 0.533\n",
      "614400/649600\tLoss: 1035.506\tL0 Loss: 0.533\n",
      "617600/649600\tLoss: 1035.933\tL0 Loss: 0.533\n",
      "620800/649600\tLoss: 1035.234\tL0 Loss: 0.533\n",
      "624000/649600\tLoss: 1035.347\tL0 Loss: 0.533\n",
      "627200/649600\tLoss: 1035.338\tL0 Loss: 0.533\n",
      "630400/649600\tLoss: 1035.484\tL0 Loss: 0.533\n",
      "633600/649600\tLoss: 1034.932\tL0 Loss: 0.533\n",
      "636800/649600\tLoss: 1034.762\tL0 Loss: 0.533\n",
      "640000/649600\tLoss: 1034.600\tL0 Loss: 0.533\n",
      "643200/649600\tLoss: 1035.681\tL0 Loss: 0.533\n",
      "646400/649600\tLoss: 1035.587\tL0 Loss: 0.533\n",
      "Valid Loss: 1180.861, Recon Error: 0.250\n",
      "1180.8606411392448\n",
      "Epoch: 1 Average loss: 1052.97 Valid loss: 1180.8606411392448\tRecon Error:0.250\n",
      "0/649600\tLoss: 1039.178\tL0 Loss: 0.533\n",
      "3200/649600\tLoss: 1036.080\tL0 Loss: 0.533\n",
      "6400/649600\tLoss: 1034.325\tL0 Loss: 0.533\n",
      "9600/649600\tLoss: 1034.773\tL0 Loss: 0.533\n",
      "12800/649600\tLoss: 1034.593\tL0 Loss: 0.533\n",
      "16000/649600\tLoss: 1034.527\tL0 Loss: 0.533\n",
      "19200/649600\tLoss: 1034.016\tL0 Loss: 0.534\n",
      "22400/649600\tLoss: 1034.264\tL0 Loss: 0.534\n",
      "25600/649600\tLoss: 1034.094\tL0 Loss: 0.534\n",
      "28800/649600\tLoss: 1034.456\tL0 Loss: 0.534\n",
      "32000/649600\tLoss: 1034.234\tL0 Loss: 0.534\n",
      "35200/649600\tLoss: 1034.381\tL0 Loss: 0.534\n",
      "38400/649600\tLoss: 1034.394\tL0 Loss: 0.534\n",
      "41600/649600\tLoss: 1035.308\tL0 Loss: 0.534\n",
      "44800/649600\tLoss: 1035.672\tL0 Loss: 0.534\n",
      "48000/649600\tLoss: 1034.581\tL0 Loss: 0.534\n",
      "51200/649600\tLoss: 1034.443\tL0 Loss: 0.534\n",
      "54400/649600\tLoss: 1034.619\tL0 Loss: 0.534\n",
      "57600/649600\tLoss: 1033.702\tL0 Loss: 0.534\n",
      "60800/649600\tLoss: 1033.967\tL0 Loss: 0.534\n",
      "64000/649600\tLoss: 1034.112\tL0 Loss: 0.534\n",
      "67200/649600\tLoss: 1033.621\tL0 Loss: 0.534\n",
      "70400/649600\tLoss: 1033.704\tL0 Loss: 0.534\n",
      "73600/649600\tLoss: 1034.140\tL0 Loss: 0.534\n",
      "76800/649600\tLoss: 1033.775\tL0 Loss: 0.534\n",
      "80000/649600\tLoss: 1034.045\tL0 Loss: 0.534\n",
      "83200/649600\tLoss: 1035.980\tL0 Loss: 0.534\n",
      "86400/649600\tLoss: 1035.304\tL0 Loss: 0.534\n",
      "89600/649600\tLoss: 1034.087\tL0 Loss: 0.534\n",
      "92800/649600\tLoss: 1034.296\tL0 Loss: 0.534\n",
      "96000/649600\tLoss: 1034.256\tL0 Loss: 0.534\n",
      "99200/649600\tLoss: 1033.754\tL0 Loss: 0.534\n",
      "102400/649600\tLoss: 1033.710\tL0 Loss: 0.534\n",
      "105600/649600\tLoss: 1034.266\tL0 Loss: 0.534\n",
      "108800/649600\tLoss: 1033.696\tL0 Loss: 0.534\n",
      "112000/649600\tLoss: 1033.419\tL0 Loss: 0.534\n",
      "115200/649600\tLoss: 1034.028\tL0 Loss: 0.534\n",
      "118400/649600\tLoss: 1033.364\tL0 Loss: 0.534\n",
      "121600/649600\tLoss: 1033.164\tL0 Loss: 0.535\n",
      "124800/649600\tLoss: 1035.749\tL0 Loss: 0.535\n",
      "128000/649600\tLoss: 1034.263\tL0 Loss: 0.535\n",
      "131200/649600\tLoss: 1033.535\tL0 Loss: 0.535\n",
      "134400/649600\tLoss: 1033.776\tL0 Loss: 0.535\n",
      "137600/649600\tLoss: 1033.267\tL0 Loss: 0.535\n",
      "140800/649600\tLoss: 1033.339\tL0 Loss: 0.535\n",
      "144000/649600\tLoss: 1033.657\tL0 Loss: 0.535\n",
      "147200/649600\tLoss: 1033.874\tL0 Loss: 0.535\n",
      "150400/649600\tLoss: 1033.822\tL0 Loss: 0.535\n",
      "153600/649600\tLoss: 1032.873\tL0 Loss: 0.535\n",
      "156800/649600\tLoss: 1033.268\tL0 Loss: 0.535\n",
      "160000/649600\tLoss: 1033.014\tL0 Loss: 0.535\n",
      "163200/649600\tLoss: 1033.312\tL0 Loss: 0.535\n",
      "166400/649600\tLoss: 1035.294\tL0 Loss: 0.535\n",
      "169600/649600\tLoss: 1033.145\tL0 Loss: 0.535\n",
      "172800/649600\tLoss: 1033.102\tL0 Loss: 0.535\n",
      "176000/649600\tLoss: 1032.904\tL0 Loss: 0.535\n",
      "179200/649600\tLoss: 1033.325\tL0 Loss: 0.535\n",
      "182400/649600\tLoss: 1033.538\tL0 Loss: 0.535\n",
      "185600/649600\tLoss: 1032.877\tL0 Loss: 0.535\n",
      "188800/649600\tLoss: 1032.590\tL0 Loss: 0.535\n",
      "192000/649600\tLoss: 1032.744\tL0 Loss: 0.535\n",
      "195200/649600\tLoss: 1032.957\tL0 Loss: 0.535\n",
      "198400/649600\tLoss: 1032.973\tL0 Loss: 0.535\n",
      "201600/649600\tLoss: 1033.035\tL0 Loss: 0.535\n",
      "204800/649600\tLoss: 1033.052\tL0 Loss: 0.535\n",
      "208000/649600\tLoss: 1035.426\tL0 Loss: 0.535\n",
      "211200/649600\tLoss: 1033.615\tL0 Loss: 0.535\n",
      "214400/649600\tLoss: 1033.489\tL0 Loss: 0.535\n",
      "217600/649600\tLoss: 1033.410\tL0 Loss: 0.535\n",
      "220800/649600\tLoss: 1033.271\tL0 Loss: 0.535\n",
      "224000/649600\tLoss: 1032.854\tL0 Loss: 0.536\n",
      "227200/649600\tLoss: 1032.639\tL0 Loss: 0.536\n",
      "230400/649600\tLoss: 1033.089\tL0 Loss: 0.536\n",
      "233600/649600\tLoss: 1032.617\tL0 Loss: 0.536\n",
      "236800/649600\tLoss: 1032.773\tL0 Loss: 0.536\n",
      "240000/649600\tLoss: 1032.475\tL0 Loss: 0.536\n",
      "243200/649600\tLoss: 1032.868\tL0 Loss: 0.536\n",
      "246400/649600\tLoss: 1035.920\tL0 Loss: 0.536\n",
      "249600/649600\tLoss: 1044.460\tL0 Loss: 0.536\n",
      "252800/649600\tLoss: 1039.643\tL0 Loss: 0.536\n",
      "256000/649600\tLoss: 1036.108\tL0 Loss: 0.536\n",
      "259200/649600\tLoss: 1034.403\tL0 Loss: 0.536\n",
      "262400/649600\tLoss: 1033.488\tL0 Loss: 0.536\n",
      "265600/649600\tLoss: 1033.199\tL0 Loss: 0.536\n",
      "268800/649600\tLoss: 1032.577\tL0 Loss: 0.536\n",
      "272000/649600\tLoss: 1032.104\tL0 Loss: 0.536\n",
      "275200/649600\tLoss: 1032.116\tL0 Loss: 0.536\n",
      "278400/649600\tLoss: 1032.130\tL0 Loss: 0.536\n",
      "281600/649600\tLoss: 1031.809\tL0 Loss: 0.536\n",
      "284800/649600\tLoss: 1032.369\tL0 Loss: 0.536\n",
      "288000/649600\tLoss: 1033.763\tL0 Loss: 0.536\n",
      "291200/649600\tLoss: 1032.968\tL0 Loss: 0.536\n",
      "294400/649600\tLoss: 1032.881\tL0 Loss: 0.536\n",
      "297600/649600\tLoss: 1031.753\tL0 Loss: 0.536\n",
      "300800/649600\tLoss: 1031.923\tL0 Loss: 0.536\n",
      "304000/649600\tLoss: 1032.163\tL0 Loss: 0.536\n",
      "307200/649600\tLoss: 1031.771\tL0 Loss: 0.536\n",
      "310400/649600\tLoss: 1031.452\tL0 Loss: 0.536\n",
      "313600/649600\tLoss: 1032.386\tL0 Loss: 0.536\n",
      "316800/649600\tLoss: 1032.029\tL0 Loss: 0.536\n",
      "320000/649600\tLoss: 1031.899\tL0 Loss: 0.536\n",
      "323200/649600\tLoss: 1031.832\tL0 Loss: 0.536\n",
      "326400/649600\tLoss: 1031.652\tL0 Loss: 0.536\n",
      "329600/649600\tLoss: 1033.203\tL0 Loss: 0.537\n",
      "332800/649600\tLoss: 1032.350\tL0 Loss: 0.537\n",
      "336000/649600\tLoss: 1032.471\tL0 Loss: 0.537\n",
      "339200/649600\tLoss: 1031.904\tL0 Loss: 0.537\n",
      "342400/649600\tLoss: 1032.350\tL0 Loss: 0.537\n",
      "345600/649600\tLoss: 1032.324\tL0 Loss: 0.537\n",
      "348800/649600\tLoss: 1031.975\tL0 Loss: 0.537\n",
      "352000/649600\tLoss: 1031.473\tL0 Loss: 0.537\n",
      "355200/649600\tLoss: 1031.493\tL0 Loss: 0.537\n",
      "358400/649600\tLoss: 1031.637\tL0 Loss: 0.537\n",
      "361600/649600\tLoss: 1032.078\tL0 Loss: 0.537\n",
      "364800/649600\tLoss: 1031.758\tL0 Loss: 0.537\n",
      "368000/649600\tLoss: 1031.296\tL0 Loss: 0.537\n",
      "371200/649600\tLoss: 1032.741\tL0 Loss: 0.537\n",
      "374400/649600\tLoss: 1031.592\tL0 Loss: 0.537\n",
      "377600/649600\tLoss: 1032.379\tL0 Loss: 0.537\n",
      "380800/649600\tLoss: 1031.618\tL0 Loss: 0.537\n",
      "384000/649600\tLoss: 1032.894\tL0 Loss: 0.537\n",
      "387200/649600\tLoss: 1032.187\tL0 Loss: 0.537\n",
      "390400/649600\tLoss: 1031.728\tL0 Loss: 0.537\n",
      "393600/649600\tLoss: 1031.458\tL0 Loss: 0.537\n",
      "396800/649600\tLoss: 1031.452\tL0 Loss: 0.537\n",
      "400000/649600\tLoss: 1032.032\tL0 Loss: 0.537\n",
      "403200/649600\tLoss: 1032.912\tL0 Loss: 0.537\n",
      "406400/649600\tLoss: 1032.337\tL0 Loss: 0.537\n",
      "409600/649600\tLoss: 1032.179\tL0 Loss: 0.537\n",
      "412800/649600\tLoss: 1031.791\tL0 Loss: 0.537\n",
      "416000/649600\tLoss: 1032.268\tL0 Loss: 0.537\n",
      "419200/649600\tLoss: 1032.616\tL0 Loss: 0.537\n",
      "422400/649600\tLoss: 1033.498\tL0 Loss: 0.537\n",
      "425600/649600\tLoss: 1033.234\tL0 Loss: 0.537\n",
      "428800/649600\tLoss: 1032.274\tL0 Loss: 0.537\n",
      "432000/649600\tLoss: 1031.655\tL0 Loss: 0.537\n",
      "435200/649600\tLoss: 1031.791\tL0 Loss: 0.537\n",
      "438400/649600\tLoss: 1031.703\tL0 Loss: 0.538\n",
      "441600/649600\tLoss: 1031.984\tL0 Loss: 0.538\n",
      "444800/649600\tLoss: 1033.057\tL0 Loss: 0.538\n",
      "448000/649600\tLoss: 1032.294\tL0 Loss: 0.538\n",
      "451200/649600\tLoss: 1032.449\tL0 Loss: 0.538\n",
      "454400/649600\tLoss: 1031.924\tL0 Loss: 0.538\n",
      "457600/649600\tLoss: 1032.741\tL0 Loss: 0.538\n",
      "460800/649600\tLoss: 1032.809\tL0 Loss: 0.538\n",
      "464000/649600\tLoss: 1033.384\tL0 Loss: 0.538\n",
      "467200/649600\tLoss: 1033.443\tL0 Loss: 0.538\n",
      "470400/649600\tLoss: 1031.907\tL0 Loss: 0.538\n",
      "473600/649600\tLoss: 1031.593\tL0 Loss: 0.538\n",
      "476800/649600\tLoss: 1032.349\tL0 Loss: 0.538\n",
      "480000/649600\tLoss: 1032.238\tL0 Loss: 0.538\n",
      "483200/649600\tLoss: 1032.486\tL0 Loss: 0.538\n",
      "486400/649600\tLoss: 1033.078\tL0 Loss: 0.538\n",
      "489600/649600\tLoss: 1032.994\tL0 Loss: 0.538\n",
      "492800/649600\tLoss: 1037.640\tL0 Loss: 0.538\n",
      "496000/649600\tLoss: 1045.608\tL0 Loss: 0.538\n",
      "499200/649600\tLoss: 1044.767\tL0 Loss: 0.538\n",
      "502400/649600\tLoss: 1044.412\tL0 Loss: 0.538\n",
      "505600/649600\tLoss: 1043.953\tL0 Loss: 0.538\n",
      "508800/649600\tLoss: 1042.762\tL0 Loss: 0.538\n",
      "512000/649600\tLoss: 1038.763\tL0 Loss: 0.538\n",
      "515200/649600\tLoss: 1033.405\tL0 Loss: 0.538\n",
      "518400/649600\tLoss: 1031.881\tL0 Loss: 0.538\n",
      "521600/649600\tLoss: 1031.976\tL0 Loss: 0.538\n",
      "524800/649600\tLoss: 1031.579\tL0 Loss: 0.538\n",
      "528000/649600\tLoss: 1032.169\tL0 Loss: 0.538\n",
      "531200/649600\tLoss: 1031.598\tL0 Loss: 0.538\n",
      "534400/649600\tLoss: 1033.299\tL0 Loss: 0.538\n",
      "537600/649600\tLoss: 1031.944\tL0 Loss: 0.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540800/649600\tLoss: 1031.838\tL0 Loss: 0.538\n",
      "544000/649600\tLoss: 1031.494\tL0 Loss: 0.538\n",
      "547200/649600\tLoss: 1031.321\tL0 Loss: 0.538\n",
      "550400/649600\tLoss: 1031.678\tL0 Loss: 0.538\n",
      "553600/649600\tLoss: 1031.513\tL0 Loss: 0.538\n",
      "556800/649600\tLoss: 1031.286\tL0 Loss: 0.538\n",
      "560000/649600\tLoss: 1031.446\tL0 Loss: 0.539\n",
      "563200/649600\tLoss: 1031.805\tL0 Loss: 0.539\n",
      "566400/649600\tLoss: 1031.598\tL0 Loss: 0.539\n",
      "569600/649600\tLoss: 1031.826\tL0 Loss: 0.539\n",
      "572800/649600\tLoss: 1031.213\tL0 Loss: 0.539\n",
      "576000/649600\tLoss: 1032.962\tL0 Loss: 0.539\n",
      "579200/649600\tLoss: 1031.365\tL0 Loss: 0.539\n",
      "582400/649600\tLoss: 1031.578\tL0 Loss: 0.539\n",
      "585600/649600\tLoss: 1031.616\tL0 Loss: 0.539\n",
      "588800/649600\tLoss: 1031.103\tL0 Loss: 0.539\n",
      "592000/649600\tLoss: 1031.826\tL0 Loss: 0.539\n",
      "595200/649600\tLoss: 1031.703\tL0 Loss: 0.539\n",
      "598400/649600\tLoss: 1031.068\tL0 Loss: 0.539\n",
      "601600/649600\tLoss: 1031.183\tL0 Loss: 0.539\n",
      "604800/649600\tLoss: 1031.177\tL0 Loss: 0.539\n",
      "608000/649600\tLoss: 1031.448\tL0 Loss: 0.539\n",
      "611200/649600\tLoss: 1031.730\tL0 Loss: 0.539\n",
      "614400/649600\tLoss: 1031.163\tL0 Loss: 0.539\n",
      "617600/649600\tLoss: 1032.223\tL0 Loss: 0.539\n",
      "620800/649600\tLoss: 1031.957\tL0 Loss: 0.539\n",
      "624000/649600\tLoss: 1031.414\tL0 Loss: 0.539\n",
      "627200/649600\tLoss: 1031.544\tL0 Loss: 0.539\n",
      "630400/649600\tLoss: 1031.266\tL0 Loss: 0.539\n",
      "633600/649600\tLoss: 1031.705\tL0 Loss: 0.539\n",
      "636800/649600\tLoss: 1031.542\tL0 Loss: 0.539\n",
      "640000/649600\tLoss: 1031.028\tL0 Loss: 0.539\n",
      "643200/649600\tLoss: 1031.438\tL0 Loss: 0.539\n",
      "646400/649600\tLoss: 1031.572\tL0 Loss: 0.539\n",
      "Valid Loss: 1155.421, Recon Error: 0.250\n",
      "1155.4211030047304\n",
      "Epoch: 2 Average loss: 1033.23 Valid loss: 1155.4211030047304\tRecon Error:0.250\n",
      "0/649600\tLoss: 1038.412\tL0 Loss: 0.539\n",
      "3200/649600\tLoss: 1033.153\tL0 Loss: 0.539\n",
      "6400/649600\tLoss: 1031.376\tL0 Loss: 0.539\n",
      "9600/649600\tLoss: 1031.219\tL0 Loss: 0.539\n",
      "12800/649600\tLoss: 1030.590\tL0 Loss: 0.540\n",
      "16000/649600\tLoss: 1030.941\tL0 Loss: 0.540\n",
      "19200/649600\tLoss: 1031.064\tL0 Loss: 0.540\n",
      "22400/649600\tLoss: 1030.830\tL0 Loss: 0.540\n",
      "25600/649600\tLoss: 1030.842\tL0 Loss: 0.540\n",
      "28800/649600\tLoss: 1031.112\tL0 Loss: 0.540\n",
      "32000/649600\tLoss: 1032.053\tL0 Loss: 0.540\n",
      "35200/649600\tLoss: 1030.976\tL0 Loss: 0.540\n",
      "38400/649600\tLoss: 1031.071\tL0 Loss: 0.540\n",
      "41600/649600\tLoss: 1032.637\tL0 Loss: 0.540\n",
      "44800/649600\tLoss: 1032.666\tL0 Loss: 0.540\n",
      "48000/649600\tLoss: 1031.513\tL0 Loss: 0.540\n",
      "51200/649600\tLoss: 1031.547\tL0 Loss: 0.540\n",
      "54400/649600\tLoss: 1030.977\tL0 Loss: 0.540\n",
      "57600/649600\tLoss: 1030.585\tL0 Loss: 0.540\n",
      "60800/649600\tLoss: 1030.614\tL0 Loss: 0.540\n",
      "64000/649600\tLoss: 1031.108\tL0 Loss: 0.540\n",
      "67200/649600\tLoss: 1031.095\tL0 Loss: 0.540\n",
      "70400/649600\tLoss: 1030.578\tL0 Loss: 0.540\n",
      "73600/649600\tLoss: 1031.105\tL0 Loss: 0.540\n",
      "76800/649600\tLoss: 1030.625\tL0 Loss: 0.540\n",
      "80000/649600\tLoss: 1031.164\tL0 Loss: 0.540\n",
      "83200/649600\tLoss: 1033.206\tL0 Loss: 0.540\n",
      "86400/649600\tLoss: 1032.597\tL0 Loss: 0.540\n",
      "89600/649600\tLoss: 1030.739\tL0 Loss: 0.540\n",
      "92800/649600\tLoss: 1031.009\tL0 Loss: 0.540\n",
      "96000/649600\tLoss: 1031.318\tL0 Loss: 0.540\n",
      "99200/649600\tLoss: 1030.641\tL0 Loss: 0.540\n",
      "102400/649600\tLoss: 1030.255\tL0 Loss: 0.540\n",
      "105600/649600\tLoss: 1031.085\tL0 Loss: 0.540\n",
      "108800/649600\tLoss: 1031.402\tL0 Loss: 0.540\n",
      "112000/649600\tLoss: 1030.603\tL0 Loss: 0.540\n",
      "115200/649600\tLoss: 1031.278\tL0 Loss: 0.540\n",
      "118400/649600\tLoss: 1030.953\tL0 Loss: 0.540\n",
      "121600/649600\tLoss: 1031.448\tL0 Loss: 0.540\n",
      "124800/649600\tLoss: 1032.822\tL0 Loss: 0.540\n",
      "128000/649600\tLoss: 1031.561\tL0 Loss: 0.541\n",
      "131200/649600\tLoss: 1031.154\tL0 Loss: 0.541\n",
      "134400/649600\tLoss: 1030.888\tL0 Loss: 0.541\n",
      "137600/649600\tLoss: 1031.231\tL0 Loss: 0.541\n",
      "140800/649600\tLoss: 1031.017\tL0 Loss: 0.541\n",
      "144000/649600\tLoss: 1031.006\tL0 Loss: 0.541\n",
      "147200/649600\tLoss: 1030.958\tL0 Loss: 0.541\n",
      "150400/649600\tLoss: 1031.206\tL0 Loss: 0.541\n",
      "153600/649600\tLoss: 1030.924\tL0 Loss: 0.541\n",
      "156800/649600\tLoss: 1030.809\tL0 Loss: 0.541\n",
      "160000/649600\tLoss: 1030.736\tL0 Loss: 0.541\n",
      "163200/649600\tLoss: 1030.978\tL0 Loss: 0.541\n",
      "166400/649600\tLoss: 1031.980\tL0 Loss: 0.541\n",
      "169600/649600\tLoss: 1031.220\tL0 Loss: 0.541\n",
      "172800/649600\tLoss: 1031.664\tL0 Loss: 0.541\n",
      "176000/649600\tLoss: 1030.928\tL0 Loss: 0.541\n",
      "179200/649600\tLoss: 1030.844\tL0 Loss: 0.541\n",
      "182400/649600\tLoss: 1030.907\tL0 Loss: 0.541\n",
      "185600/649600\tLoss: 1031.245\tL0 Loss: 0.541\n",
      "188800/649600\tLoss: 1031.409\tL0 Loss: 0.541\n",
      "192000/649600\tLoss: 1031.095\tL0 Loss: 0.541\n",
      "195200/649600\tLoss: 1031.065\tL0 Loss: 0.541\n",
      "198400/649600\tLoss: 1030.851\tL0 Loss: 0.541\n",
      "201600/649600\tLoss: 1030.970\tL0 Loss: 0.541\n",
      "204800/649600\tLoss: 1031.094\tL0 Loss: 0.541\n",
      "208000/649600\tLoss: 1032.334\tL0 Loss: 0.541\n",
      "211200/649600\tLoss: 1031.917\tL0 Loss: 0.541\n",
      "214400/649600\tLoss: 1031.765\tL0 Loss: 0.541\n",
      "217600/649600\tLoss: 1031.036\tL0 Loss: 0.541\n",
      "220800/649600\tLoss: 1030.867\tL0 Loss: 0.541\n",
      "224000/649600\tLoss: 1031.040\tL0 Loss: 0.541\n",
      "227200/649600\tLoss: 1031.250\tL0 Loss: 0.541\n",
      "230400/649600\tLoss: 1031.212\tL0 Loss: 0.541\n",
      "233600/649600\tLoss: 1031.313\tL0 Loss: 0.541\n",
      "236800/649600\tLoss: 1031.215\tL0 Loss: 0.541\n",
      "240000/649600\tLoss: 1031.065\tL0 Loss: 0.542\n",
      "243200/649600\tLoss: 1030.815\tL0 Loss: 0.542\n",
      "246400/649600\tLoss: 1034.064\tL0 Loss: 0.542\n",
      "249600/649600\tLoss: 1042.642\tL0 Loss: 0.542\n",
      "252800/649600\tLoss: 1037.414\tL0 Loss: 0.542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-522f5faa14f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_training_gif\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./training.gif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/jointvae/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, valid_loader, epochs, save_training_gif)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecon_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             print('Epoch: {} Average loss: {:.2f} Valid loss: {}\\tRecon Error:{:.3f}'.format(epoch + 1,\n\u001b[1;32m    114\u001b[0m                                                           self.batch_size * self.model.num_pixels * mean_epoch_loss, valid_loss,recon_error))#self.losses['recon_loss'][-1]\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/jointvae/training.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m#for batch_idx in range(len(train_loader)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m#(data, label) = next(items)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0miter_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml0_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mtotal_l0\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml0_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0miter_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ISO/Pruned_VAE/jointvae/training.py\u001b[0m in \u001b[0;36m_train_iteration\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "###1e-5 6859 1e-4 6727 5e-4 6722 try tanh/L1 loss/beta--->DIP\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "trainer.train(train_loader,valid_loader, epochs=50, save_training_gif=('./training.gif', viz))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "torch.save(trainer.best_model.state_dict(), 'model50_params.pkl')\n",
    "torch.save(trainer.best_model, './model50')\n",
    "##15.078 - 0.0147  17.209 - 0.0168 error tanh \n",
    "##LR 1e-3 0.019-0.023 worse should pick 5e-4\n",
    "##PLOT THE CURVE!!!!!\n",
    "###3360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_spec=latent_spec, img_size=(1, 64, 64)).cuda()\n",
    "model.load_state_dict(torch.load('model50_params.pkl'))\n",
    "#path=\"figures/face/cont_{}/pruned_Beta_ {}lamba{}_ONLYPAIR\".format(n_cont,gamma,0.1)\n",
    "loss = trainer.get_losses()\n",
    "# print(len(loss[\"DIP_loss\"]))\n",
    "# print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, labels in test_loader:\n",
    "    break\n",
    "batch = torch.unsqueeze(batch,1).cuda().to(dtype=torch.float32)\n",
    "latent, mask, reg = model.encode(batch)\n",
    "model.reparameterize(latent).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentanglement_lib.evaluation.metrics import beta_vae, dci, factor_vae, mig, modularity_explicitness, sap_score\n",
    "from disentanglement_lib.data.ground_truth import dsprites\n",
    "dataset = dsprites.DSprites()\n",
    "##########\n",
    "def representation_fn(x, mean = True):\n",
    "    x = torch.tensor(x).cuda().to(dtype=torch.float32)\n",
    "    model = VAE(latent_spec = latent_spec, img_size=(1, 64, 64)).cuda()\n",
    "    model.load_state_dict(torch.load('model_params.pkl'))\n",
    "    latent_dict,mask,regularization = model.encode(x)\n",
    "\n",
    "    if mean:\n",
    "        return latent[\"cont\"][0][0]\n",
    "    else:\n",
    "        return model.reparameterize(latent_dict).cuda()  \n",
    "    \n",
    "    \n",
    "######\n",
    "beta_scores_dict = beta_vae.compute_beta_vae_sklearn(ground_truth_data = dataset,representation_function = representation_fn)\n",
    "factor_scores_dict = factor_vae.compute_factor_vae(ground_truth_data = dataset, representation_function = representation_fn)\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta_scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.cuda.is_available()\n",
    "# device = torch.device('cuda')\n",
    "# print(device)\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Chi-square test\n",
    "import torch\n",
    "tensor_one = torch.tensor([[1,2,3],[4,5,6]])\n",
    "tensor_two = torch.tensor([[6,8,9],[10,11,12]])\n",
    "tensor_list = [tensor_one, tensor_two]\n",
    "tens_list = []\n",
    "for tensor in tensor_list:\n",
    "    \n",
    "    print(tensor)\n",
    "    length = tensor.shape[1]\n",
    "    tens_list.append(torch.mean(tensor.float(),dim=0))\n",
    "    \n",
    "tens_list = torch.stack(tens_list).reshape(1,-1)\n",
    "tens_listT = tens_list.t()\n",
    "matrix = tens_listT.matmul(tens_list)\n",
    "print(matrix)\n",
    "print(\"--------\")\n",
    "Chi2 =0\n",
    "for i in range(len(tensor_list)):\n",
    "    for j in range(len(tensor_list)):\n",
    "        if i > j:\n",
    "            submatrix = matrix[j*length:(j+1)*length,i*length:(i+1)*length]\n",
    "            c_sum = torch.sum(submatrix,dim=0).reshape(-1,1)\n",
    "            \n",
    "            r_sum = torch.sum(submatrix,dim=1).reshape(1,-1)\n",
    "            all_sum = torch.sum(submatrix)\n",
    "            Expectation = c_sum.matmul(r_sum)/all_sum\n",
    "            print(all_sum,c_sum,r_sum,Expectation)\n",
    "            Chi2 += torch.sum((submatrix-Expectation)**2/Expectation)\n",
    "            \n",
    "        \n",
    "print(Chi2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "# Get a batch of data\n",
    "for batch, labels in test_loader:\n",
    "    break\n",
    "    \n",
    "#get best model,easrly stopping\n",
    "viz = Visualizer(model)\n",
    "\n",
    "# Reconstruct data using Joint-VAE model\n",
    "recon = viz.reconstructions(torch.unsqueeze(batch,1).cuda().to(dtype=torch.float32))\n",
    "\n",
    "# face\n",
    "recon=np.rollaxis(recon.numpy(), 0, 3)  \n",
    "print(recon[265:,:,:].max())\n",
    "recon[:,:,:]=(recon[:,:,:]+1)/2\n",
    "plt.imshow(recon[:,:,:].astype(float),cmap=\"gray\")\n",
    "\n",
    "#MNIST\n",
    "# plt.imshow(recon.numpy()[0, :, :].astype(float), cmap='gray')\n",
    "#plt.savefig(path+\"/recon.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_batch = iter(test_loader)\n",
    "test_batch = next(test_batch)\n",
    "latent_dist,mask,_ = model.encode(torch.unsqueeze(test_batch[0],1).cuda().to(dtype=torch.float32))\n",
    "\n",
    "print(mask,len(torch.nonzero(mask[0]==0)))\n",
    "\n",
    "# for latent in latent_dist['cont'][0]:\n",
    "#     count=torch.zeros((1,32))\n",
    "#     latent[latent<1e-7]=0\n",
    "    \n",
    "#     for i in range(128):\n",
    "    \n",
    "#     #print(latent[i].size(),torch.nonzero(latent[i]))#len(torch.nonzero(latent[0]==0))\n",
    "#         count[latent[i].reshape(1,32)!=0] += 1\n",
    "#     print(count)\n",
    "def show_idx(mask):\n",
    "    a = mask.cpu().detach().numpy().squeeze()\n",
    "    return np.array(np.where(a==1))+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCR():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist, mask, reg = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont'][0]\n",
    "        cov = covmatrix(mean)\n",
    "        cov[torch.abs(cov)<=1e-6]=0\n",
    "        cor = cov2cor(cov)\n",
    "        totalc += np.sum(cor) \n",
    "\n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "def TCV():\n",
    "    totalc = 0\n",
    "    for batch, labels in test_loader:\n",
    "        latent_dist,mask, reg = model.encode(torch.tensor(batch).cuda())\n",
    "        mean, var = latent_dist['cont'][0]\n",
    "        cov = covmatrix(mean).cpu().detach().numpy()\n",
    "        cov = cov-np.diag(np.diag(cov))\n",
    "        totalc += np.sum(cov**2) \n",
    "        \n",
    "    return totalc/len(test_loader)\n",
    "\n",
    "\n",
    "def covmatrix(mean):\n",
    "    exp_mu = torch.mean(mean, dim=0)  #####mean through batch\n",
    "\n",
    "    # expectation of mu mu.tranpose\n",
    "    mu_expand1 = mean.unsqueeze(1)  #####(batch_size, 1, number of mean of latent variables)\n",
    "    mu_expand2 = mean.unsqueeze(2)  #####(batch_size, number of mean of latent variables, 1) ignore batch_size, only transpose the means\n",
    "    exp_mu_mu_t = torch.mean(mu_expand1 * mu_expand2, dim=0)\n",
    "\n",
    "    # covariance of model mean\n",
    "    cov = exp_mu_mu_t - exp_mu.unsqueeze(0) * exp_mu.unsqueeze(1) \n",
    "    return cov\n",
    "def cov2cor(c):\n",
    "    #input batch * n_cont\n",
    "    c = c.cpu().detach()\n",
    "    d=np.zeros_like(c)\n",
    "    for i in range(c.shape[0]):\n",
    "        for j in range(c.shape[1]):\n",
    "            d[i,j]=c[i,j]/(np.sqrt(c[i,i]*c[j,j]+1e-10))\n",
    "    return d\n",
    "tcor=TCR()\n",
    "tcov=TCV()\n",
    "print(tcor,tcov)\n",
    "trainer.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###latent space T-SNE visualization\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "samples = torch.zeros(1)\n",
    "labels = torch.zeros(1)\n",
    "for i in range(10):\n",
    "    test_batch = iter(test_loader)\n",
    "    test_batch = next(test_batch)\n",
    "    new_labels =torch.tensor(test_batch[1])\n",
    "    latent_dist,_ ,_= model.encode(torch.tensor(test_batch[0]).cuda())\n",
    "    new_samples = model.reparameterize(latent_dist)\n",
    "    if torch.sum(samples) == 0:\n",
    "        samples =new_samples\n",
    "        labels = new_labels\n",
    "    else:\n",
    "        samples = torch.cat((samples,new_samples),0)\n",
    "        labels = torch.cat((labels, new_labels),0)\n",
    "    #print(samples.shape)\n",
    "    \n",
    "##latent_varibales should be N,D--->N,2\n",
    "\n",
    "\n",
    "# latent_variables = samples.reshape(samples[0],-1)\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "tsne.fit_transform(samples.detach().cpu().numpy())\n",
    "\n",
    "plt.scatter(tsne.embedding_[:,0],tsne.embedding_[:,1])\n",
    "#plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 10 # Number of labels\n",
    "\n",
    "# setup the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "# define the data\n",
    "x = tsne.embedding_[:,0]\n",
    "y = tsne.embedding_[:,1]\n",
    "tag = labels# Tag each point with a corresponding label    \n",
    "\n",
    "# define the colormap\n",
    "cmap = plt.cm.jet\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# create the new map\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0,N,N+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# make the scatter\n",
    "scat = ax.scatter(x,y,c=tag,s=np.random.randint(100,110,N),cmap=cmap,     norm=norm)\n",
    "# create the colorbar\n",
    "cb = plt.colorbar(scat, spacing='proportional',ticks=bounds)\n",
    "cb.set_label('Custom cbar')\n",
    "ax.set_title('Discrete color mappings')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "plt.savefig(path+\"/scatter.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t-SNE demo\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "X = np.arange(40).reshape(5,4,2)\n",
    "\n",
    "X_new = X.reshape(5,-1)\n",
    "#X = np.array([[[0,0], [0,0], [0,0]], [[0,0], [0,1], [1,1]], [[1,1], [1,0], [0,1]], [[1,1], [1,1], [1,1]]])\n",
    "print(X.shape,X)\n",
    "print(\"--------\")\n",
    "print(X_new)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne.fit_transform(X)\n",
    "print(tsne.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot samples\n",
    "\n",
    "samples = viz.samples()\n",
    "plt.imshow(samples.numpy()[0, :174, :], cmap='gray')\n",
    "print(np.sum(samples.numpy()[0, :174, :]))\n",
    "print(samples.numpy()[0, :, :].shape)\n",
    "####origin\n",
    "4*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot samples\n",
    "import matplotlib as mpl\n",
    "\n",
    "#MNIST\n",
    "# samples = viz.samples()\n",
    "# sample=samples.numpy()[0, :, :]/2+0.5\n",
    "# plt.imshow(sample, cmap='gray')\n",
    "# plt.imsave(path+\"/samples\",samples.numpy()[0, :, :]/2+0.5, cmap='gray')\n",
    "\n",
    "# face\n",
    "fig = plt.figure(figsize=(50, 50)) \n",
    "samples = viz.samples()\n",
    "samples = np.rollaxis(samples.numpy(), 0, 3)  \n",
    "print(samples[:,:,0].max())\n",
    "samples=(samples+1)/2\n",
    "plt.imshow(samples.astype(float),norm = norm)\n",
    "plt.imsave(path+\"/samples\",samples)\n",
    "###DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot all traversals\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "traversals = viz.all_latent_traversals(size=10)\n",
    "\n",
    "#MNIST\n",
    "# plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "# plt.imsave(path+\"/all_traversals\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "#face\n",
    "traversals = np.rollaxis(traversals.numpy(), 0, 3)  \n",
    "traversals=(traversals+1)/2\n",
    "plt.imshow(traversals)\n",
    "plt.imsave(path+\"/all_traversals\",traversals)\n",
    "###dip[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
    "#         0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=5, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "#MNIST\n",
    "# plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "# plt.imsave(path+\"/contVSdisc\",traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "traversals.numpy()[0, :, :].max()\n",
    "show_idx(mask)\n",
    "#face\n",
    "traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "traversals=(traversals+1)/2\n",
    "plt.imshow(traversals)\n",
    "plt.imsave(path+\"/contVSdisc\",traversals)\n",
    "##origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_t = viz.all_latent_traversals()\n",
    "print(all_t.shape)\n",
    "plt.imshow(all_t.numpy()[0, :, :], cmap='gray')\n",
    "plt.imsave(\"figures/beta/all_\",traversals.numpy()[0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "# Plot a grid of some traversals\n",
    "\n",
    "fig = plt.figure(figsize=(70, 70))  # width, height in inches\n",
    "print(\"continuous\")\n",
    "for i in range(n_cont):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=i, disc_idx=None,size=12)\n",
    "    \n",
    "    #MNIST\n",
    "#     sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "#     plt.savefig(path+\"/cont{}.png\".format(i))\n",
    "#     plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "    \n",
    "    #FACE\n",
    "    traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "    sub = fig.add_subplot(n_cont, 1, i + 1)\n",
    "    traversals=(traversals+1)/2\n",
    "    plt.imshow(traversals)   \n",
    "plt.savefig(path+\"/cont.png\")\n",
    "\n",
    "show_idx(mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"discrete\")\n",
    "for i in range(n_disc):\n",
    "    traversals = viz.latent_traversal_line(cont_idx=None, disc_idx=i,size=10)\n",
    "    ##MNIST\n",
    "#     sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "#     plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "#     plt.imshow(traversals.numpy()[0, :, :], cmap='gray')\n",
    "\n",
    "    #FACE\n",
    "    traversals = np.rollaxis(traversals.numpy(), 0, 3)\n",
    "    sub = fig.add_subplot(n_disc, 1, i + 1)\n",
    "traversals=(traversals+1)/2\n",
    "plt.savefig(path+\"/disc{}.png\".format(i))\n",
    "plt.imshow(traversals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import numpy as np\n",
    "import torch\n",
    "from latent_traversals import LatentTraverser\n",
    "from scipy import stats\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "    \n",
    "# face    \n",
    "def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "        # Generate latent traversal\n",
    "#         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "#                                                              disc_idx=disc_idx,\n",
    "#                                                              size=size)\n",
    "        dim = n_cont + sum(disc)\n",
    "        if prior:\n",
    "            latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "        else:\n",
    "            latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "        latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "        latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "        # Map samples through decoder\n",
    "        generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "        generated  = np.rollaxis(generated.detach().numpy(), 0, 3)\n",
    "        generated = (generated +1)/2\n",
    "        print(generated.min(),generated.max())\n",
    "        plt.imshow(generated)\n",
    "\n",
    "        \n",
    "def decode_latents(model, latent_samples):\n",
    "\n",
    "        latent_samples = Variable(latent_samples)\n",
    "        if model.use_cuda:\n",
    "            latent_samples = latent_samples.cuda()\n",
    "            result = model.decode(latent_samples).cpu()\n",
    "        return result\n",
    "\n",
    "#MNIST\n",
    "# def single_traversal(model,n_cont,cont_idx,cont_v,disc,disc_idx,prior):\n",
    "\n",
    "#        # Generate latent traversal\n",
    "#         latent_samples = latent_traverser.traverse_line(cont_idx=cont_idx,\n",
    "#                                                              disc_idx=disc_idx,\n",
    "#                                                              size=size)\n",
    "#         dim = n_cont + sum(disc)\n",
    "#         if prior:\n",
    "#             latent_samples = torch.tensor(np.random.normal(size=(1, dim)))\n",
    "#         else:\n",
    "#             latent_samples= torch.zeros((1,dim))\n",
    "            \n",
    "#         latent_samples[:,disc_idx+n_cont-1] = 1.0\n",
    "#         latent_samples[:,cont_idx]=cont_v\n",
    "        \n",
    "\n",
    "#         # Map samples through decoder\n",
    "#         generated = decode_latents(model, latent_samples.float()).squeeze()\n",
    "#         plt.imshow(generated.detach().numpy(),cmap=\"gray\")\n",
    "\n",
    "        \n",
    "# def decode_latents(model, latent_samples):\n",
    "\n",
    "#         latent_samples = Variable(latent_samples)\n",
    "#         if model.use_cuda:\n",
    "#             latent_samples = latent_samples.cuda()\n",
    "#         return model.decode(latent_samples).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "def interactive_view(model,n_cont,disc):\n",
    "    \n",
    "    \n",
    "    interact(single_traversal,model=fixed(model),\n",
    "             n_cont=fixed(n_cont), cont_idx=(0,n_cont,1), cont_v=(-2.5,2.5,0.5),\n",
    "             disc=fixed(disc),disc_idx=(0,9,1),\n",
    "             prior=True);\n",
    "             \n",
    "interactive_view(model,n_cont,disc)\n",
    "show_idx(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
